{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":12959761,"sourceType":"datasetVersion","datasetId":8201933},{"sourceId":13043543,"sourceType":"datasetVersion","datasetId":8259391}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-09-10T15:40:01.219118Z","iopub.execute_input":"2025-09-10T15:40:01.219405Z","execution_failed":"2025-09-10T18:55:18.453Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/sales-prediction/products.csv\n/kaggle/input/sales-prediction/bills (1).csv\n/kaggle/input/sales-prediction/products.csv\n/kaggle/input/sales-prediction/bills (1).csv\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"!pip install openpyxl xlsxwriter\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-12T16:37:25.778754Z","iopub.execute_input":"2025-09-12T16:37:25.779006Z","iopub.status.idle":"2025-09-12T16:37:30.185055Z","shell.execute_reply.started":"2025-09-12T16:37:25.778988Z","shell.execute_reply":"2025-09-12T16:37:30.184135Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: openpyxl in /usr/local/lib/python3.11/dist-packages (3.1.5)\nCollecting xlsxwriter\n  Downloading xlsxwriter-3.2.5-py3-none-any.whl.metadata (2.7 kB)\nRequirement already satisfied: et-xmlfile in /usr/local/lib/python3.11/dist-packages (from openpyxl) (2.0.0)\nDownloading xlsxwriter-3.2.5-py3-none-any.whl (172 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m172.3/172.3 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: xlsxwriter\nSuccessfully installed xlsxwriter-3.2.5\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import pandas as pd\nfrom datetime import datetime, timedelta\nimport ast\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.preprocessing import MinMaxScaler\nfrom joblib import Parallel, delayed\nimport numpy as np\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# -----------------------------\n# 1️⃣ Load data\n# -----------------------------\nbills_path = \"/kaggle/input/sales-prediction/bills (1).csv\"\nproducts_path = \"/kaggle/input/sales-prediction/products.csv\"\n\nbills_df = pd.read_csv(bills_path)\nbills_df['Date_Time'] = pd.to_datetime(bills_df['Date_Time'])\n\nproducts_df = pd.read_csv(products_path)\nproducts_df = products_df.rename(columns={\"ID\": \"ProductID\"})\nproducts_df[\"ProductID\"] = products_df[\"ProductID\"].astype(str)\n\n# -----------------------------\n# 2️⃣ Helper function to extract items\n# -----------------------------\ndef extract_items(row):\n    try:\n        product_ids = ast.literal_eval(row['Product_IDs'])\n        quantities = ast.literal_eval(row['Quantities'])\n        if len(product_ids) != len(quantities):\n            return []\n        return [{\"product_id\": str(pid), \"quantity_sold\": q} for pid, q in zip(product_ids, quantities)]\n    except:\n        return []\n\n# -----------------------------\n# 3️⃣ Summaries: Daily, Weekly, Monthly\n# -----------------------------\n# Daily\ndaily_records = []\nfor _, row in bills_df.iterrows():\n    items = extract_items(row)\n    for item in items:\n        daily_records.append({\n            \"Date\": row['Date_Time'].date(),\n            \"ProductID\": item['product_id'],\n            \"Quantity_Sold\": item['quantity_sold']\n        })\ndaily_sales_df = pd.DataFrame(daily_records)\nsummary_daily_df = daily_sales_df.groupby([\"Date\", \"ProductID\"], as_index=False)[\"Quantity_Sold\"].sum()\nsummary_daily_df = summary_daily_df.rename(columns={\"Quantity_Sold\": \"Total_Quantity_Sold\"})\n\n# Weekly\nbills_df['Week_Start'] = bills_df['Date_Time'] - pd.to_timedelta(bills_df['Date_Time'].dt.weekday, unit='d')\nweekly_records = []\nfor _, row in bills_df.iterrows():\n    items = extract_items(row)\n    week_start = row['Week_Start'].date()\n    for item in items:\n        weekly_records.append({\n            \"Week_Start\": week_start,\n            \"ProductID\": item['product_id'],\n            \"Quantity_Sold\": item['quantity_sold']\n        })\nweekly_sales_df = pd.DataFrame(weekly_records)\nsummary_weekly_df = weekly_sales_df.groupby([\"Week_Start\", \"ProductID\"], as_index=False)[\"Quantity_Sold\"].sum()\nsummary_weekly_df = summary_weekly_df.rename(columns={\"Quantity_Sold\": \"Total_Quantity_Sold\"})\n\n# Monthly\nbills_df['Month'] = bills_df['Date_Time'].dt.to_period('M')\nmonthly_records = []\nfor _, row in bills_df.iterrows():\n    items = extract_items(row)\n    month_str = str(row['Month'])\n    for item in items:\n        monthly_records.append({\n            \"Month\": month_str,\n            \"ProductID\": item['product_id'],\n            \"Quantity_Sold\": item['quantity_sold']\n        })\nmonthly_sales_df = pd.DataFrame(monthly_records)\nsummary_monthly_df = monthly_sales_df.groupby([\"Month\", \"ProductID\"], as_index=False)[\"Quantity_Sold\"].sum()\nsummary_monthly_df = summary_monthly_df.rename(columns={\"Quantity_Sold\": \"Total_Quantity_Sold\"})\n\n# -----------------------------\n# 4️⃣ Merge with products.csv\n# -----------------------------\nsummary_daily_df[\"ProductID\"] = summary_daily_df[\"ProductID\"].astype(str)\nsummary_weekly_df[\"ProductID\"] = summary_weekly_df[\"ProductID\"].astype(str)\nsummary_monthly_df[\"ProductID\"] = summary_monthly_df[\"ProductID\"].astype(str)\n\nsummary_daily_df = summary_daily_df.merge(products_df, on=\"ProductID\", how=\"left\")\nsummary_weekly_df = summary_weekly_df.merge(products_df, on=\"ProductID\", how=\"left\")\nsummary_monthly_df = summary_monthly_df.merge(products_df, on=\"ProductID\", how=\"left\")\n\n# -----------------------------\n# 5️⃣ Brand-level summaries\n# -----------------------------\nbrand_daily = summary_daily_df.groupby([\"Date\", \"Brand Name\"], as_index=False)[\"Total_Quantity_Sold\"].sum()\nbrand_daily.to_csv(\"brand_daily_sales.csv\", index=False)\n\nbrand_weekly = summary_weekly_df.groupby([\"Week_Start\", \"Brand Name\"], as_index=False)[\"Total_Quantity_Sold\"].sum()\nbrand_weekly.to_csv(\"brand_weekly_sales.csv\", index=False)\n\nbrand_monthly = summary_monthly_df.groupby([\"Month\", \"Brand Name\"], as_index=False)[\"Total_Quantity_Sold\"].sum()\nbrand_monthly.to_csv(\"brand_monthly_sales.csv\", index=False)\n\n# -----------------------------\n# 6️⃣ Identify no-sales products\n# -----------------------------\nall_sold_products = set(summary_daily_df[\"ProductID\"].unique())\nno_sales_products = products_df[~products_df[\"ProductID\"].isin(all_sold_products)]\nno_sales_products.to_csv(\"no_sales_products.csv\", index=False)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-10T15:40:04.360332Z","iopub.execute_input":"2025-09-10T15:40:04.360588Z","iopub.status.idle":"2025-09-10T15:41:26.274831Z","shell.execute_reply.started":"2025-09-10T15:40:04.360565Z","shell.execute_reply":"2025-09-10T15:41:26.274216Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.preprocessing import MinMaxScaler\nfrom joblib import Parallel, delayed\nimport numpy as np\nimport pandas as pd\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# -----------------------------\n# Custom Dataset\n# -----------------------------\nclass TimeSeriesDataset(Dataset):\n    def __init__(self, series, seq_len):\n        self.series = series\n        self.seq_len = seq_len\n\n    def __len__(self):\n        return len(self.series) - self.seq_len\n\n    def __getitem__(self, idx):\n        return (\n            torch.tensor(self.series[idx:idx+self.seq_len], dtype=torch.float32).unsqueeze(-1),\n            torch.tensor(self.series[idx+self.seq_len], dtype=torch.float32)\n        )\n\n# -----------------------------\n# GRU with Attention Model\n# -----------------------------\nclass GRUAttention(nn.Module):\n    def __init__(self, input_dim=1, hidden_dim=32, num_layers=1):\n        super().__init__()\n        self.gru = nn.GRU(input_dim, hidden_dim, num_layers, batch_first=True)\n        self.attn_fc = nn.Linear(hidden_dim, 1)\n        self.fc = nn.Linear(hidden_dim, 1)\n\n    def forward(self, x):\n        gru_out, _ = self.gru(x)  # (batch, seq_len, hidden_dim)\n        attn_weights = torch.softmax(self.attn_fc(gru_out), dim=1)  # (batch, seq_len, 1)\n        context_vector = torch.sum(attn_weights * gru_out, dim=1)   # (batch, hidden_dim)\n        output = self.fc(context_vector)  # (batch, 1)\n        return output\n\n# -----------------------------\n# Training + Forecasting Function\n# -----------------------------\ndef train_and_forecast(series, future=30, seq_len=30, epochs=50, batch_size=64):\n    scaler = MinMaxScaler()\n    series = scaler.fit_transform(series.reshape(-1,1)).flatten()\n\n    dataset = TimeSeriesDataset(series, seq_len)\n    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n\n    model = GRUAttention().to(device)\n    loss_fn = nn.MSELoss()\n    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n\n    # Early stopping\n    best_loss = float('inf')\n    patience = 5\n    no_improve = 0\n\n    for epoch in range(epochs):\n        epoch_loss = 0\n        for xb, yb in dataloader:\n            xb, yb = xb.to(device), yb.to(device)\n            pred = model(xb).squeeze(-1)\n            loss = loss_fn(pred, yb)\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n            epoch_loss += loss.item()\n\n        avg_loss = epoch_loss / len(dataloader)\n        if avg_loss < best_loss:\n            best_loss = avg_loss\n            no_improve = 0\n        else:\n            no_improve += 1\n\n        if no_improve >= patience:\n            break\n\n    # Forecast loop\n    input_seq = torch.tensor(series[-seq_len:], dtype=torch.float32).unsqueeze(0).unsqueeze(-1).to(device)\n    preds = []\n    model.eval()\n    with torch.no_grad():\n        for _ in range(future):\n            out = model(input_seq).item()\n            preds.append(out)\n            new_input = torch.cat([input_seq[:,1:,:], torch.tensor([[[out]]], dtype=torch.float32).to(device)], dim=1)\n            input_seq = new_input\n\n    preds = scaler.inverse_transform(np.array(preds).reshape(-1,1)).flatten()\n    return preds\n\n# -----------------------------\n# Forecast per Product\n# -----------------------------\ndef process_product_all(product_id, df, future_daily=30, future_week=7, future_month=30, seq_len=30):\n    group = df[df[\"ProductID\"] == product_id].sort_values(\"Date\")\n    series = group[\"Total_Quantity_Sold\"].values\n    if len(series) < seq_len:\n        return None\n\n    daily_forecast = train_and_forecast(series, future=future_daily, seq_len=seq_len)\n    week_forecast = train_and_forecast(series, future=future_week, seq_len=seq_len)\n    month_forecast = train_and_forecast(series, future=future_month, seq_len=seq_len)\n\n    return {\n        \"ProductID\": product_id,\n        \"Daily_Forecast\": daily_forecast.tolist(),\n        \"Week_Forecast\": week_forecast.tolist(),\n        \"Month_Forecast\": month_forecast.tolist()\n    }\n\n# -----------------------------\n# Run Forecasting in Parallel\n# -----------------------------\nproduct_ids = summary_daily_df[\"ProductID\"].unique()\nresults = Parallel(n_jobs=-1)(\n    delayed(process_product_all)(pid, summary_daily_df) for pid in product_ids\n)\nresults = [r for r in results if r is not None]\n\nforecast_df = pd.DataFrame(results)\nforecast_df = forecast_df.merge(products_df[['ProductID', 'Brand Name']], on='ProductID', how='left')\nforecast_df.to_csv(\"product_forecasts_all.csv\", index=False)\nprint(\"✅ Product-level forecasts saved (daily, weekly, monthly)\")\n\n# -----------------------------\n# BRAND-LEVEL FORECASTS\n# -----------------------------\ndef aggregate_brand_forecast(forecast_df, col_name, horizon_name):\n    expanded = []\n    for _, row in forecast_df.iterrows():\n        for i, val in enumerate(row[col_name]):\n            expanded.append({\"Brand\": row['Brand Name'], horizon_name: i+1, \"Forecast_Qty\": val})\n    expanded_df = pd.DataFrame(expanded)\n    brand_forecast = expanded_df.groupby([\"Brand\", horizon_name], as_index=False)[\"Forecast_Qty\"].sum()\n    return brand_forecast\n\n# Daily\nbrand_daily_forecast = aggregate_brand_forecast(forecast_df, \"Daily_Forecast\", \"Day\")\nbrand_daily_forecast.to_csv(\"brand_daily_forecast.csv\", index=False)\n\n# Weekly\nbrand_week_forecast = aggregate_brand_forecast(forecast_df, \"Week_Forecast\", \"Week\")\nbrand_week_forecast.to_csv(\"brand_week_forecast.csv\", index=False)\n\n# Monthly\nbrand_month_forecast = aggregate_brand_forecast(forecast_df, \"Month_Forecast\", \"Month\")\nbrand_month_forecast.to_csv(\"brand_month_forecast.csv\", index=False)\n\nprint(\"✅ Brand-level forecasts saved (daily, weekly, monthly)\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-10T16:14:45.767144Z","iopub.execute_input":"2025-09-10T16:14:45.767672Z","iopub.status.idle":"2025-09-10T16:35:59.854386Z","shell.execute_reply.started":"2025-09-10T16:14:45.767649Z","shell.execute_reply":"2025-09-10T16:35:59.853700Z"}},"outputs":[{"name":"stdout","text":"✅ Product-level forecasts saved (daily, weekly, monthly)\n✅ Brand-level forecasts saved (daily, weekly, monthly)\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"import pandas as pd\nimport ast\nfrom datetime import datetime\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.preprocessing import MinMaxScaler\nfrom joblib import Parallel, delayed\nimport numpy as np\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# -----------------------------\n# 1️⃣ Load Data\n# -----------------------------\nbills_path = \"/kaggle/input/sales-prediction/bills (1).csv\"\nproducts_path = \"/kaggle/input/sales-prediction/products.csv\"\n\nbills_df = pd.read_csv(bills_path)\nproducts_df = pd.read_csv(products_path)\nproducts_df[\"ProductID\"] = products_df[\"ID\"].astype(str)\n\n# -----------------------------\n# 2️⃣ Parse and Explode Data\n# -----------------------------\nbills_df['Product_IDs'] = bills_df['Product_IDs'].apply(ast.literal_eval)\nbills_df['Quantities'] = bills_df['Quantities'].apply(ast.literal_eval)\nbills_df['Date'] = pd.to_datetime(bills_df['Date_Time']).dt.date\n\nexploded_df = bills_df.explode(['Product_IDs', 'Quantities'])\nexploded_df['ProductID'] = exploded_df['Product_IDs'].astype(str)\nexploded_df['Quantity_Sold'] = exploded_df['Quantities'].astype(int)\n\n# -----------------------------\n# 3️⃣ Aggregate Daily Sales\n# -----------------------------\nsummary_daily_df = (\n    exploded_df.groupby(['Date', 'ProductID'], as_index=False)['Quantity_Sold']\n    .sum()\n    .rename(columns={'Quantity_Sold': 'Total_Quantity_Sold'})\n)\n\n# -----------------------------\n# 4️⃣ Identify No-Sales Products\n# -----------------------------\nall_sold_products = set(summary_daily_df[\"ProductID\"].unique())\nno_sales_products_df = products_df[~products_df[\"ProductID\"].isin(all_sold_products)]\nno_sales_products_df.to_csv(\"no_sales_products.csv\", index=False)\n\n# Save daily sales summary\nsummary_daily_df.to_csv(\"summary_daily_sales.csv\", index=False)\n\n# -----------------------------\n# 5️⃣ Brand-Level Summaries\n# -----------------------------\nsummary_daily_df = summary_daily_df.merge(products_df[['ProductID', 'Brand Name']], on='ProductID', how='left')\n\nbrand_daily = summary_daily_df.groupby([\"Date\", \"Brand Name\"], as_index=False)[\"Total_Quantity_Sold\"].sum()\nbrand_daily.to_csv(\"brand_daily_sales.csv\", index=False)\n\n# -----------------------------\n# 6️⃣ Forecasting Model Definition\n# -----------------------------\nclass TimeSeriesDataset(Dataset):\n    def __init__(self, series, seq_len):\n        self.series = series\n        self.seq_len = seq_len\n\n    def __len__(self):\n        return len(self.series) - self.seq_len\n\n    def __getitem__(self, idx):\n        return (\n            torch.tensor(self.series[idx:idx+self.seq_len], dtype=torch.float32).unsqueeze(-1),\n            torch.tensor(self.series[idx+self.seq_len], dtype=torch.float32)\n        )\n\nclass GRUAttention(nn.Module):\n    def __init__(self, input_dim=1, hidden_dim=32, num_layers=1):\n        super().__init__()\n        self.gru = nn.GRU(input_dim, hidden_dim, num_layers, batch_first=True)\n        self.attn_fc = nn.Linear(hidden_dim, 1)\n        self.fc = nn.Linear(hidden_dim, 1)\n\n    def forward(self, x):\n        gru_out, _ = self.gru(x)\n        attn_weights = torch.softmax(self.attn_fc(gru_out), dim=1)\n        context_vector = torch.sum(attn_weights * gru_out, dim=1)\n        output = self.fc(context_vector)\n        return output\n\ndef train_and_forecast(series, future=30, seq_len=30, epochs=50, batch_size=64):\n    scaler = MinMaxScaler()\n    series = scaler.fit_transform(series.reshape(-1,1)).flatten()\n\n    dataset = TimeSeriesDataset(series, seq_len)\n    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n\n    model = GRUAttention().to(device)\n    loss_fn = nn.MSELoss()\n    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n\n    best_loss = float('inf')\n    patience = 5\n    no_improve = 0\n\n    for epoch in range(epochs):\n        epoch_loss = 0\n        for xb, yb in dataloader:\n            xb, yb = xb.to(device), yb.to(device)\n            pred = model(xb).squeeze(-1)\n            loss = loss_fn(pred, yb)\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n            epoch_loss += loss.item()\n\n        avg_loss = epoch_loss / len(dataloader)\n        if avg_loss < best_loss:\n            best_loss = avg_loss\n            no_improve = 0\n        else:\n            no_improve += 1\n        if no_improve >= patience:\n            break\n\n    input_seq = torch.tensor(series[-seq_len:], dtype=torch.float32).unsqueeze(0).unsqueeze(-1).to(device)\n    preds = []\n    model.eval()\n    with torch.no_grad():\n        for _ in range(future):\n            out = model(input_seq).item()\n            preds.append(out)\n            new_input = torch.cat([input_seq[:,1:,:], torch.tensor([[[out]]], dtype=torch.float32).to(device)], dim=1)\n            input_seq = new_input\n\n    preds = scaler.inverse_transform(np.array(preds).reshape(-1,1)).flatten()\n    return preds\n\n# -----------------------------\n# 7️⃣ Forecast Per Product\n# -----------------------------\ndef process_product_all(product_id, df, future_daily=30, future_week=7, future_month=30, seq_len=30):\n    group = df[df[\"ProductID\"] == product_id].sort_values(\"Date\")\n    series = group[\"Total_Quantity_Sold\"].values\n    if len(series) < seq_len:\n        return None\n\n    daily_forecast = train_and_forecast(series, future=future_daily, seq_len=seq_len)\n    week_forecast = train_and_forecast(series, future=future_week, seq_len=seq_len)\n    month_forecast = train_and_forecast(series, future=future_month, seq_len=seq_len)\n\n    return {\n        \"ProductID\": product_id,\n        \"Daily_Forecast\": daily_forecast.tolist(),\n        \"Week_Forecast\": week_forecast.tolist(),\n        \"Month_Forecast\": month_forecast.tolist()\n    }\n\nproduct_ids = summary_daily_df[\"ProductID\"].unique()\nresults = Parallel(n_jobs=-1)(\n    delayed(process_product_all)(pid, summary_daily_df) for pid in product_ids\n)\nresults = [r for r in results if r is not None]\n\nforecast_df = pd.DataFrame(results)\nforecast_df = forecast_df.merge(products_df[['ProductID', 'Brand Name']], on='ProductID', how='left')\nforecast_df.to_csv(\"product_forecasts_all.csv\", index=False)\n\n# -----------------------------\n# 8️⃣ Aggregate Brand Forecasts\n# -----------------------------\ndef aggregate_brand_forecast(forecast_df, col_name, horizon_name):\n    expanded = []\n    for _, row in forecast_df.iterrows():\n        for i, val in enumerate(row[col_name]):\n            expanded.append({\"Brand\": row['Brand Name'], horizon_name: i+1, \"Forecast_Qty\": val})\n    expanded_df = pd.DataFrame(expanded)\n    brand_forecast = expanded_df.groupby([\"Brand\", horizon_name], as_index=False)[\"Forecast_Qty\"].sum()\n    return brand_forecast\n\nbrand_daily_forecast = aggregate_brand_forecast(forecast_df, \"Daily_Forecast\", \"Day\")\nbrand_daily_forecast.to_csv(\"brand_daily_forecast.csv\", index=False)\n\nbrand_week_forecast = aggregate_brand_forecast(forecast_df, \"Week_Forecast\", \"Week\")\nbrand_week_forecast.to_csv(\"brand_week_forecast.csv\", index=False)\n\nbrand_month_forecast = aggregate_brand_forecast(forecast_df, \"Month_Forecast\", \"Month\")\nbrand_month_forecast.to_csv(\"brand_month_forecast.csv\", index=False)\n\nprint(\"✅ All steps completed successfully.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-12T17:34:03.460694Z","iopub.execute_input":"2025-09-12T17:34:03.461288Z","iopub.status.idle":"2025-09-12T17:47:39.043523Z","shell.execute_reply.started":"2025-09-12T17:34:03.461262Z","shell.execute_reply":"2025-09-12T17:47:39.042723Z"}},"outputs":[{"name":"stdout","text":"✅ All steps completed successfully.\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.preprocessing import MinMaxScaler\nfrom joblib import Parallel, delayed\nimport ast\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# -----------------------------\n# Load Data\n# -----------------------------\nbills_df = pd.read_csv(\"/kaggle/input/sales-prediction/bills (1).csv\")\nproducts_df = pd.read_csv(\"/kaggle/input/sales-prediction/products.csv\")\n\n# -----------------------------\n# Safe Preprocessing & Merge\n# -----------------------------\nbills_df.columns = bills_df.columns.str.strip()\n\n# Expand multiple Product IDs\nbills_df[\"Product_IDs\"] = bills_df[\"Product_IDs\"].astype(str)\nbills_df = bills_df.assign(ProductID=bills_df[\"Product_IDs\"].str.split(\",\")).explode(\"ProductID\")\nbills_df[\"ProductID\"] = bills_df[\"ProductID\"].str.strip()\n\n# Convert stringified lists to Python lists for quantities\nbills_df[\"Product_IDs\"] = bills_df[\"Product_IDs\"].apply(ast.literal_eval)\nbills_df[\"Quantities\"] = bills_df[\"Quantities\"].apply(ast.literal_eval)\n\n# Explode each product and quantity\nbills_df = bills_df.explode([\"Product_IDs\", \"Quantities\"])\nbills_df[\"ProductID\"] = bills_df[\"Product_IDs\"].astype(str).str.strip()\nbills_df[\"Quantity_Sold\"] = bills_df[\"Quantities\"].astype(int)\n\n# -----------------------------\n# Ensure products_df columns\n# -----------------------------\nproducts_df.rename(columns={\n    'Product Code': 'ProductID',\n    'Product Name': 'Product_Name',\n    'Brand Name': 'Brand Name',\n    'Price (INR)': 'Price'\n}, inplace=True)\n\nproducts_df[\"ProductID\"] = products_df[\"ProductID\"].astype(str).str.strip()\n\n# Merge bills with products\nmerged_df = bills_df.merge(products_df[['ProductID', 'Product_Name', 'Brand Name']], on=\"ProductID\", how=\"left\")\nmerged_df[\"Date\"] = pd.to_datetime(merged_df[\"Date_Time\"]).dt.date\n\nprint(\"✅ Merged shape:\", merged_df.shape)\n\n# -----------------------------\n# Prepare Daily Summary\n# -----------------------------\nsummary_daily_df = (\n    merged_df.groupby([\"Date\", \"ProductID\", \"Brand Name\"])[\"Quantity_Sold\"]\n    .sum()\n    .reset_index()\n    .rename(columns={\"Quantity_Sold\": \"Total_Quantity_Sold\"})\n)\n\n# -----------------------------\n# TimeSeries Dataset\n# -----------------------------\nclass TimeSeriesDataset(Dataset):\n    def __init__(self, series, seq_len):\n        series = np.array(series, dtype=float)\n        if len(series) <= seq_len:\n            series = np.pad(series, (0, seq_len + 1 - len(series)), mode=\"constant\", constant_values=0.1)\n        self.series = series\n        self.seq_len = seq_len\n\n    def __len__(self):\n        return max(len(self.series) - self.seq_len, 1)\n\n    def __getitem__(self, idx):\n        idx = min(idx, len(self.series) - self.seq_len - 1)\n        return (\n            torch.tensor(self.series[idx:idx+self.seq_len], dtype=torch.float32).unsqueeze(-1),\n            torch.tensor(self.series[idx+self.seq_len], dtype=torch.float32)\n        )\n\n# -----------------------------\n# GRU + Attention Model\n# -----------------------------\nclass GRUAttention(nn.Module):\n    def __init__(self, input_dim=1, hidden_dim=32, num_layers=1):\n        super().__init__()\n        self.gru = nn.GRU(input_dim, hidden_dim, num_layers, batch_first=True)\n        self.attn_fc = nn.Linear(hidden_dim, 1)\n        self.fc = nn.Linear(hidden_dim, 1)\n\n    def forward(self, x):\n        gru_out, _ = self.gru(x)\n        attn_weights = torch.softmax(self.attn_fc(gru_out), dim=1)\n        context_vector = torch.sum(attn_weights * gru_out, dim=1)\n        output = self.fc(context_vector)\n        return output\n\n# -----------------------------\n# Train and Forecast\n# -----------------------------\ndef train_and_forecast(series, future=30, seq_len=30, epochs=50, batch_size=64):\n    if np.sum(series) == 0:\n        return np.zeros(future)\n\n    scaler = MinMaxScaler()\n    series_scaled = scaler.fit_transform(series.reshape(-1,1)).flatten()\n\n    dataset = TimeSeriesDataset(series_scaled, seq_len)\n    batch_size = min(batch_size, len(dataset))\n    shuffle = len(dataset) > 1\n    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle)\n\n    model = GRUAttention().to(device)\n    loss_fn = nn.MSELoss()\n    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n\n    best_loss = float('inf')\n    patience, no_improve = 5, 0\n\n    for epoch in range(epochs):\n        epoch_loss = 0\n        for xb, yb in dataloader:\n            xb, yb = xb.to(device), yb.to(device)\n            pred = model(xb).squeeze(-1)\n            loss = loss_fn(pred, yb)\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n            epoch_loss += loss.item()\n        avg_loss = epoch_loss / len(dataloader)\n        if avg_loss < best_loss:\n            best_loss = avg_loss\n            no_improve = 0\n        else:\n            no_improve += 1\n        if no_improve >= patience:\n            break\n\n    # Forecast\n    model.eval()\n    input_seq = torch.tensor(series_scaled[-seq_len:], dtype=torch.float32).unsqueeze(0).unsqueeze(-1).to(device)\n    preds = []\n    with torch.no_grad():\n        for _ in range(future):\n            out = model(input_seq).item()\n            preds.append(out)\n            new_input = torch.cat([input_seq[:,1:,:], torch.tensor([[[out]]], dtype=torch.float32).to(device)], dim=1)\n            input_seq = new_input\n\n    preds = scaler.inverse_transform(np.array(preds).reshape(-1,1)).flatten()\n    preds = np.maximum(preds, 0)\n    return np.round(preds)\n\n# -----------------------------\n# Product-level Forecasts\n# -----------------------------\ndef process_product_all(product_id, df, seq_len=30):\n    group = df[df[\"ProductID\"] == product_id].sort_values(\"Date\")\n    series = group[\"Total_Quantity_Sold\"].values\n    if len(series) < 2:\n        return None\n    return {\n        \"ProductID\": product_id,\n        \"Brand Name\": group[\"Brand Name\"].iloc[0],\n        \"Daily_Forecast\": train_and_forecast(series, future=30, seq_len=seq_len).tolist(),\n        \"Weekly_Forecast\": train_and_forecast(series, future=7, seq_len=seq_len).tolist(),\n        \"Monthly_Forecast\": train_and_forecast(series, future=30, seq_len=seq_len).tolist()\n    }\n\nproduct_ids = summary_daily_df[\"ProductID\"].unique()\nresults = Parallel(n_jobs=-1)(\n    delayed(process_product_all)(pid, summary_daily_df) for pid in product_ids\n)\nresults = [r for r in results if r is not None]\n\nforecast_df = pd.DataFrame(results)\nforecast_df.to_csv(\"product_forecasts_all.csv\", index=False)\nprint(\"✅ Product-level forecasts saved\")\n\n# -----------------------------\n# Brand-level Forecasts (aggregate across products)\n# -----------------------------\ndef aggregate_brand_forecast(forecast_df, col_name, horizon_name):\n    expanded = []\n    for _, row in forecast_df.iterrows():\n        brand_col = row.get('Brand Name', 'Unknown')\n        for i, val in enumerate(row[col_name]):\n            expanded.append({\"Brand\": brand_col, horizon_name: i+1, \"Forecast_Qty\": val})\n    expanded_df = pd.DataFrame(expanded)\n    brand_forecast = expanded_df.groupby([\"Brand\", horizon_name], as_index=False)[\"Forecast_Qty\"].sum()\n    brand_forecast[\"Forecast_Qty\"] = brand_forecast[\"Forecast_Qty\"].round()\n    return brand_forecast\n\nbrand_daily_forecast = aggregate_brand_forecast(forecast_df, \"Daily_Forecast\", \"Day\")\nbrand_weekly_forecast = aggregate_brand_forecast(forecast_df, \"Weekly_Forecast\", \"Week\")\nbrand_monthly_forecast = aggregate_brand_forecast(forecast_df, \"Monthly_Forecast\", \"Month\")\n\nbrand_daily_forecast.to_csv(\"brand_daily_forecast.csv\", index=False)\nbrand_weekly_forecast.to_csv(\"brand_weekly_forecast.csv\", index=False)\nbrand_monthly_forecast.to_csv(\"brand_monthly_forecast.csv\", index=False)\nprint(\"✅ Brand-level forecasts saved (aggregated across products)\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-13T05:30:07.243216Z","iopub.execute_input":"2025-09-13T05:30:07.243503Z","iopub.status.idle":"2025-09-13T05:30:55.087965Z","shell.execute_reply.started":"2025-09-13T05:30:07.243482Z","shell.execute_reply":"2025-09-13T05:30:55.087036Z"}},"outputs":[{"name":"stdout","text":"✅ Merged shape: (5398837, 16)\n✅ Product-level forecasts saved\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_36/3409059029.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    197\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mbrand_forecast\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m \u001b[0mbrand_daily_forecast\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maggregate_brand_forecast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mforecast_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Daily_Forecast\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Day\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m \u001b[0mbrand_weekly_forecast\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maggregate_brand_forecast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mforecast_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Weekly_Forecast\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Week\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m \u001b[0mbrand_monthly_forecast\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maggregate_brand_forecast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mforecast_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Monthly_Forecast\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Month\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_36/3409059029.py\u001b[0m in \u001b[0;36maggregate_brand_forecast\u001b[0;34m(forecast_df, col_name, horizon_name)\u001b[0m\n\u001b[1;32m    193\u001b[0m             \u001b[0mexpanded\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"Brand\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbrand_col\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhorizon_name\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Forecast_Qty\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mval\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m     \u001b[0mexpanded_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexpanded\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 195\u001b[0;31m     \u001b[0mbrand_forecast\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexpanded_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Brand\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhorizon_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mas_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Forecast_Qty\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m     \u001b[0mbrand_forecast\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Forecast_Qty\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbrand_forecast\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Forecast_Qty\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mbrand_forecast\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36mgroupby\u001b[0;34m(self, by, axis, level, as_index, sort, group_keys, observed, dropna)\u001b[0m\n\u001b[1;32m   9181\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"You have to supply one of 'by' and 'level'\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   9182\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 9183\u001b[0;31m         return DataFrameGroupBy(\n\u001b[0m\u001b[1;32m   9184\u001b[0m             \u001b[0mobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   9185\u001b[0m             \u001b[0mkeys\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mby\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/groupby/groupby.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, obj, keys, axis, level, grouper, exclusions, selection, as_index, sort, group_keys, observed, dropna)\u001b[0m\n\u001b[1;32m   1327\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1328\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mgrouper\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1329\u001b[0;31m             grouper, exclusions, obj = get_grouper(\n\u001b[0m\u001b[1;32m   1330\u001b[0m                 \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1331\u001b[0m                 \u001b[0mkeys\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/groupby/grouper.py\u001b[0m in \u001b[0;36mget_grouper\u001b[0;34m(obj, key, axis, level, sort, observed, validate, dropna)\u001b[0m\n\u001b[1;32m   1041\u001b[0m                 \u001b[0min_axis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgpr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgpr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1042\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1043\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgpr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1044\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgpr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mGrouper\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mgpr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkey\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1045\u001b[0m             \u001b[0;31m# Add key to exclusions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyError\u001b[0m: 'Brand'"],"ename":"KeyError","evalue":"'Brand'","output_type":"error"}],"execution_count":36},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error\n\n# -------------------------\n# 1. Load Data\n# -------------------------\nbills_df = pd.read_csv(\"/kaggle/input/trail-bills/10000.csv\")\nproducts_df = pd.read_csv(\"/kaggle/input/sales-prediction/products.csv\")\n\n# Clean column names\nbills_df.columns = bills_df.columns.str.strip()\nproducts_df.columns = products_df.columns.str.strip()\n\n# Force rename the last column in products_df → \"ProductID\"\nproducts_df.rename(columns={products_df.columns[-1]: \"ProductID\"}, inplace=True)\n\n# -------------------------\n# Process Bills Data\n# -------------------------\n# Expand multiple product IDs in one bill\nbills_df[\"Product_IDs\"] = bills_df[\"Product_IDs\"].astype(str)\nbills_df = bills_df.assign(ProductID=bills_df[\"Product_IDs\"].str.split(\",\")).explode(\"ProductID\")\nbills_df[\"ProductID\"] = bills_df[\"ProductID\"].str.strip()\n\n# -------------------------\n# Merge with Products\n# -------------------------\nproducts_df[\"ProductID\"] = products_df[\"ProductID\"].astype(str).str.strip()\nmerged_df = bills_df.merge(products_df, on=\"ProductID\", how=\"left\")\nmerged_df[\"Date\"] = pd.to_datetime(merged_df[\"Date_Time\"]).dt.date\n\nprint(\"✅ Merged shape:\", merged_df.shape)\nprint(merged_df.head())\n\n# -------------------------\n# 3. Aggregate Daily Sales\n# -------------------------\ndaily_sales = (\n    merged_df.groupby([\"Date\", \"ProductID\", \"Product Name\", \"Brand Name\"])\n    .agg(Total_Sales=(\"Total_Amount\", \"sum\"))\n    .reset_index()\n)\n\n# -------------------------\n# 4. PyTorch Dataset\n# -------------------------\nclass SalesDataset(Dataset):\n    def __init__(self, series, seq_len=7):\n        self.scaler = MinMaxScaler()\n        self.series = self.scaler.fit_transform(series.reshape(-1, 1))\n        self.seq_len = seq_len\n\n    def __len__(self):\n        return len(self.series) - self.seq_len\n\n    def __getitem__(self, idx):\n        X = self.series[idx:idx+self.seq_len]\n        y = self.series[idx+self.seq_len]\n        return torch.FloatTensor(X), torch.FloatTensor(y)\n\n# -------------------------\n# 5. Simple LSTM Model\n# -------------------------\nclass LSTMModel(nn.Module):\n    def __init__(self, input_size=1, hidden_size=64, num_layers=2):\n        super().__init__()\n        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n        self.fc = nn.Linear(hidden_size, 1)\n\n    def forward(self, x):\n        out, _ = self.lstm(x)\n        return self.fc(out[:, -1, :])\n\n# -------------------------\n# 6. Train & Forecast\n# -------------------------\ndef train_and_forecast(series, days=30, seq_len=7, epochs=20):\n    if len(series) < seq_len + 1:\n        return [0] * days  # not enough data\n\n    dataset = SalesDataset(series, seq_len)\n    loader = DataLoader(dataset, batch_size=16, shuffle=True)\n\n    model = LSTMModel()\n    criterion = nn.MSELoss()\n    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n\n    # Training\n    for epoch in range(epochs):\n        for X, y in loader:\n            X = X.unsqueeze(-1)  # [batch, seq_len, 1]\n            y = y.unsqueeze(-1)\n            optimizer.zero_grad()\n            output = model(X)\n            loss = criterion(output, y)\n            loss.backward()\n            optimizer.step()\n\n    # Forecasting\n    scaler = dataset.scaler\n    last_seq = dataset.series[-seq_len:]\n    preds = []\n\n    for _ in range(days):\n        inp = torch.FloatTensor(last_seq).unsqueeze(0).unsqueeze(-1)\n        pred = model(inp).item()\n        preds.append(pred)\n        last_seq = np.append(last_seq[1:], pred)\n\n    preds = scaler.inverse_transform(np.array(preds).reshape(-1, 1)).flatten()\n    return preds.tolist()\n\n# -------------------------\n# 7. Forecast Products\n# -------------------------\nproduct_forecasts = []\nfor pid, group in daily_sales.groupby(\"ProductID\"):\n    series = group.sort_values(\"Date\")[\"Total_Sales\"].values\n    preds = train_and_forecast(series)\n\n    weekly_avg = round(np.mean(preds[:7]), 2)\n    monthly_avg = round(np.mean(preds[:30]), 2)\n\n    product_forecasts.append({\n        \"ProductID\": pid,\n        \"Product Name\": group[\"Product Name\"].iloc[0],\n        \"Predictions\": preds,\n        \"Weekly_Avg\": weekly_avg,\n        \"Monthly_Avg\": monthly_avg\n    })\n\nproduct_forecasts_df = pd.DataFrame(product_forecasts)\nproduct_forecasts_df.to_csv(\"product_forecasts.csv\", index=False)\n\n# -------------------------\n# 8. Forecast Brands\n# -------------------------\nbrand_forecasts = []\nfor (pid, brand), group in daily_sales.groupby([\"ProductID\", \"Brand Name\"]):\n    series = group.sort_values(\"Date\")[\"Total_Sales\"].values\n    preds = train_and_forecast(series)\n\n    weekly_avg = round(np.mean(preds[:7]), 2)\n    monthly_avg = round(np.mean(preds[:30]), 2)\n\n    brand_forecasts.append({\n        \"ProductID\": pid,\n        \"Brand Name\": brand,\n        \"Predictions\": preds,\n        \"Weekly_Avg\": weekly_avg,\n        \"Monthly_Avg\": monthly_avg\n    })\n\nbrand_forecasts_df = pd.DataFrame(brand_forecasts)\nbrand_forecasts_df.to_csv(\"brand_forecasts.csv\", index=False)\n\nprint(\"✅ Forecasting complete. Results saved to product_forecasts.csv and brand_forecasts.csv\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-13T04:19:34.839963Z","iopub.execute_input":"2025-09-13T04:19:34.840508Z","iopub.status.idle":"2025-09-13T04:19:35.019612Z","shell.execute_reply.started":"2025-09-13T04:19:34.840486Z","shell.execute_reply":"2025-09-13T04:19:35.018815Z"}},"outputs":[{"name":"stdout","text":"✅ Merged shape: (39704, 22)\n     Bill_ID  Mobile_Number            Date_Time  \\\n0  BILL00001     9503299454  2024-07-06 01:09:51   \n1  BILL00001     9503299454  2024-07-06 01:09:51   \n2  BILL00001     9503299454  2024-07-06 01:09:51   \n3  BILL00002     9518832283  2022-11-25 05:51:28   \n4  BILL00002     9518832283  2022-11-25 05:51:28   \n\n                                         Product_IDs       Quantities  \\\n0  ['PVSL0504031125', 'PVSL0502021025', 'PVSL0516...        [9, 2, 4]   \n1  ['PVSL0504031125', 'PVSL0502021025', 'PVSL0516...        [9, 2, 4]   \n2  ['PVSL0504031125', 'PVSL0502021025', 'PVSL0516...        [9, 2, 4]   \n3  ['PVSL0905031125', 'PVSL0306030126', 'PVSL0114...  [5, 5, 3, 5, 3]   \n4  ['PVSL0905031125', 'PVSL0306030126', 'PVSL0114...  [5, 5, 3, 5, 3]   \n\n                         Original_Prices                    Offers_Applied  \\\n0         [36319.38, 19292.36, 35040.03]              ['Yes', 'No', 'Yes']   \n1         [36319.38, 19292.36, 35040.03]              ['Yes', 'No', 'Yes']   \n2         [36319.38, 19292.36, 35040.03]              ['Yes', 'No', 'Yes']   \n3  [85.3, 992.05, 51.7, 2565.65, 190.56]  ['No', 'No', 'Yes', 'Yes', 'No']   \n4  [85.3, 992.05, 51.7, 2565.65, 190.56]  ['No', 'No', 'Yes', 'Yes', 'No']   \n\n                        Discounted_Prices  Total_Amount Payment_Method  ...  \\\n0          [32687.44, 19292.36, 29784.03]     451907.80            UPI  ...   \n1          [32687.44, 19292.36, 29784.03]     451907.80            UPI  ...   \n2          [32687.44, 19292.36, 29784.03]     451907.80            UPI  ...   \n3  [85.3, 992.05, 41.36, 2437.37, 190.56]      18269.36           Cash  ...   \n4  [85.3, 992.05, 41.36, 2437.37, 190.56]      18269.36           Cash  ...   \n\n    ID Department Code Department  Product Code Product Name  Brand Code  \\\n0  NaN             NaN        NaN           NaN          NaN         NaN   \n1  NaN             NaN        NaN           NaN          NaN         NaN   \n2  NaN             NaN        NaN           NaN          NaN         NaN   \n3  NaN             NaN        NaN           NaN          NaN         NaN   \n4  NaN             NaN        NaN           NaN          NaN         NaN   \n\n  Brand Name  Expiry Month Expiry Year        Date  \n0        NaN           NaN         NaN  2024-07-06  \n1        NaN           NaN         NaN  2024-07-06  \n2        NaN           NaN         NaN  2024-07-06  \n3        NaN           NaN         NaN  2022-11-25  \n4        NaN           NaN         NaN  2022-11-25  \n\n[5 rows x 22 columns]\n✅ Forecasting complete. Results saved to product_forecasts.csv and brand_forecasts.csv\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/pandas/io/formats/format.py:1458: RuntimeWarning: invalid value encountered in greater\n  has_large_values = (abs_vals > 1e6).any()\n/usr/local/lib/python3.11/dist-packages/pandas/io/formats/format.py:1459: RuntimeWarning: invalid value encountered in less\n  has_small_values = ((abs_vals < 10 ** (-self.digits)) & (abs_vals > 0)).any()\n/usr/local/lib/python3.11/dist-packages/pandas/io/formats/format.py:1459: RuntimeWarning: invalid value encountered in greater\n  has_small_values = ((abs_vals < 10 ** (-self.digits)) & (abs_vals > 0)).any()\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.preprocessing import MinMaxScaler\nfrom joblib import Parallel, delayed\nimport ast\n\n# Use a GPU if available\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# ----------------------------\n# 1. Load Data\n# ----------------------------\n# Load the bills and products datasets\nbills_df = pd.read_csv(\"/kaggle/input/sales-prediction/bills (1).csv\")\nproducts_df = pd.read_csv(\"/kaggle/input/sales-prediction/products.csv\")\n\n# ----------------------------\n# 2. Preprocess Bills Data\n# ----------------------------\n# Clean column names by stripping whitespace\nbills_df.columns = bills_df.columns.str.strip()\n\n# Convert Product_IDs and Quantities from string representations to lists\nbills_df[\"Product_IDs\"] = bills_df[\"Product_IDs\"].astype(str)\nbills_df[\"Quantities\"] = bills_df[\"Quantities\"].apply(ast.literal_eval)\n\n# Explode the DataFrame to have one row per product in each bill\nbills_df = bills_df.assign(ProductID=bills_df[\"Product_IDs\"].str.split(\",\")).explode(\"ProductID\")\nbills_df = bills_df.explode([\"Quantities\"])\n\n# Clean up ProductID and set data types\nbills_df[\"ProductID\"] = bills_df[\"ProductID\"].astype(str).str.strip()\nbills_df[\"Quantity_Sold\"] = bills_df[\"Quantities\"].astype(int)\nbills_df[\"Date\"] = pd.to_datetime(bills_df[\"Date_Time\"]).dt.date\n\n# ----------------------------\n# 3. Preprocess Products Data\n# ----------------------------\n# Rename columns for consistency\nproducts_df.rename(columns={\n    'Product Code': 'ProductID',\n    'Product Name': 'Product_Name',\n    'Brand Name': 'Brand Name',\n    'Price (INR)': 'Price'\n}, inplace=True)\nproducts_df[\"ProductID\"] = products_df[\"ProductID\"].astype(str).str.strip()\n\n# ----------------------------\n# 4. Merge DataFrames\n# ----------------------------\n# Merge the bills and products dataframes\nmerged_df = bills_df.merge(products_df[['ProductID', 'Product_Name', 'Brand Name']], on='ProductID', how='left')\n# Fill any missing brand names with 'Unknown'\nmerged_df['Brand Name'] = merged_df['Brand Name'].fillna('Unknown')\n\n# ----------------------------\n# 5. Create Daily Sales Summary\n# ----------------------------\n# Group by Date, ProductID, and Brand Name to get total quantity sold each day\nsummary_daily_df = (\n    merged_df.groupby([\"Date\", \"ProductID\", \"Brand Name\"])[\"Quantity_Sold\"]\n    .sum()\n    .reset_index()\n    .rename(columns={\"Quantity_Sold\": \"Total_Quantity_Sold\"})\n)\n\n# ----------------------------\n# 6. PyTorch Dataset for Time Series\n# ----------------------------\nclass TimeSeriesDataset(Dataset):\n    def __init__(self, series, seq_len):\n        series = np.array(series, dtype=float)\n        # Pad the series if it's shorter than the sequence length\n        if len(series) <= seq_len:\n            series = np.pad(series, (0, seq_len + 1 - len(series)), 'constant', constant_values=0.1)\n        self.series = series\n        self.seq_len = seq_len\n\n    def __len__(self):\n        return max(len(self.series) - self.seq_len, 1)\n\n    def __getitem__(self, idx):\n        # Ensure the index is within bounds\n        idx = min(idx, len(self.series) - self.seq_len - 1)\n        return (\n            torch.tensor(self.series[idx:idx+self.seq_len], dtype=torch.float32).unsqueeze(-1),\n            torch.tensor(self.series[idx+self.seq_len], dtype=torch.float32)\n        )\n\n# ----------------------------\n# 7. GRU with Attention Model\n# ----------------------------\nclass GRUAttention(nn.Module):\n    def __init__(self, input_dim=1, hidden_dim=32, num_layers=1):\n        super().__init__()\n        self.gru = nn.GRU(input_dim, hidden_dim, num_layers, batch_first=True)\n        self.attn_fc = nn.Linear(hidden_dim, 1)\n        self.fc = nn.Linear(hidden_dim, 1)\n\n    def forward(self, x):\n        gru_out, _ = self.gru(x)\n        attn_weights = torch.softmax(self.attn_fc(gru_out), dim=1)\n        context_vector = torch.sum(attn_weights * gru_out, dim=1)\n        output = self.fc(context_vector)\n        return output\n\n# ----------------------------\n# 8. Training and Forecasting Function\n# ----------------------------\ndef train_and_forecast(series, future=30, seq_len=30, epochs=50, batch_size=64):\n    # If there's no sales data, predict zeros\n    if np.sum(series) == 0 or len(series) == 0:\n        return np.zeros(future).tolist()\n\n    # Scale the data\n    scaler = MinMaxScaler()\n    series_scaled = scaler.fit_transform(series.reshape(-1, 1)).flatten()\n\n    # Create dataset and dataloader\n    dataset = TimeSeriesDataset(series_scaled, seq_len)\n    batch_size = min(batch_size, len(dataset))\n    shuffle = len(dataset) > 1\n    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle)\n\n    # Initialize model, loss function, and optimizer\n    model = GRUAttention().to(device)\n    loss_fn = nn.MSELoss()\n    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n\n    # Training loop with early stopping\n    best_loss = float('inf')\n    patience, no_improve = 5, 0\n    for epoch in range(epochs):\n        epoch_loss = 0\n        for xb, yb in dataloader:\n            xb, yb = xb.to(device), yb.to(device)\n            pred = model(xb).squeeze(-1)\n            loss = loss_fn(pred, yb)\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n            epoch_loss += loss.item()\n\n        avg_loss = epoch_loss / len(dataloader)\n        if avg_loss < best_loss:\n            best_loss = avg_loss\n            no_improve = 0\n        else:\n            no_improve += 1\n        if no_improve >= patience:\n            break\n\n    # Forecasting\n    model.eval()\n    input_seq = torch.tensor(series_scaled[-seq_len:], dtype=torch.float32).unsqueeze(0).unsqueeze(-1).to(device)\n    preds = []\n    with torch.no_grad():\n        for _ in range(future):\n            out = model(input_seq).item()\n            preds.append(out)\n            new_input = torch.cat([input_seq[:, 1:, :], torch.tensor([[[out]]], dtype=torch.float32).to(device)], dim=1)\n            input_seq = new_input\n\n    # Inverse transform the predictions to get the actual scale\n    preds = scaler.inverse_transform(np.array(preds).reshape(-1, 1)).flatten()\n    preds = np.maximum(preds, 0)\n    return np.round(preds).tolist()\n\n# ----------------------------\n# 9. Generate Forecasts per Product\n# ----------------------------\ndef process_product_all(product_id, df, seq_len=30):\n    group = df[df[\"ProductID\"] == product_id].sort_values(\"Date\")\n    series = group[\"Total_Quantity_Sold\"].values\n    # Pad the series if it's too short\n    if len(series) < 2:\n        series = np.pad(series, (0, 2 - len(series)), 'constant', constant_values=0.1)\n\n    # Generate daily, weekly, and monthly forecasts\n    return {\n        \"ProductID\": product_id,\n        \"Brand Name\": group[\"Brand Name\"].iloc[0],\n        \"Daily_Forecast\": train_and_forecast(series, future=30, seq_len=seq_len),\n        \"Weekly_Forecast\": train_and_forecast(series, future=7, seq_len=seq_len),\n        \"Monthly_Forecast\": train_and_forecast(series, future=30, seq_len=seq_len)\n    }\n\n# Run the forecasting in parallel for speed\nproduct_ids = summary_daily_df[\"ProductID\"].unique()\nresults = Parallel(n_jobs=-1)(delayed(process_product_all)(pid, summary_daily_df) for pid in product_ids)\nresults = [r for r in results if r is not None]\n\n# Save the product-level forecasts\nforecast_df = pd.DataFrame(results)\nforecast_df.to_csv(\"product_forecasts_all.csv\", index=False)\nprint(\"✅ Product-level forecasts saved to product_forecasts_all.csv\")\n\n# ----------------------------\n# 10. Aggregate Forecasts by Brand\n# ----------------------------\ndef aggregate_brand_forecast(forecast_df, col_name, horizon_name):\n    expanded = []\n    for _, row in forecast_df.iterrows():\n        # Handle cases where brand name might be null\n        brand_col = row['Brand Name'] if pd.notnull(row['Brand Name']) else 'Unknown'\n        for i, val in enumerate(row[col_name]):\n            expanded.append({\"Brand\": brand_col, horizon_name: i + 1, \"Forecast_Qty\": val})\n\n    expanded_df = pd.DataFrame(expanded)\n    # Group by brand and the time horizon (Day, Week, Month)\n    brand_forecast = expanded_df.groupby([\"Brand\", horizon_name], as_index=False)[\"Forecast_Qty\"].sum()\n    brand_forecast[\"Forecast_Qty\"] = brand_forecast[\"Forecast_Qty\"].round()\n    return brand_forecast\n\n# Generate and save brand-level forecasts\nbrand_daily_forecast = aggregate_brand_forecast(forecast_df, \"Daily_Forecast\", \"Day\")\nbrand_weekly_forecast = aggregate_brand_forecast(forecast_df, \"Weekly_Forecast\", \"Week\")\nbrand_monthly_forecast = aggregate_brand_forecast(forecast_df, \"Monthly_Forecast\", \"Month\")\n\nbrand_daily_forecast.to_csv(\"brand_daily_forecast.csv\", index=False)\nbrand_weekly_forecast.to_csv(\"brand_weekly_forecast.csv\", index=False)\nbrand_monthly_forecast.to_csv(\"brand_monthly_forecast.csv\", index=False)\nprint(\"✅ Brand-level forecasts saved to brand_daily_forecast.csv, brand_weekly_forecast.csv, and brand_monthly_forecast.csv\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-13T06:49:40.009621Z","iopub.execute_input":"2025-09-13T06:49:40.009905Z","iopub.status.idle":"2025-09-13T07:14:27.042926Z","shell.execute_reply.started":"2025-09-13T06:49:40.009884Z","shell.execute_reply":"2025-09-13T07:14:27.042254Z"}},"outputs":[{"name":"stdout","text":"✅ Product-level forecasts saved to product_forecasts_all.csv\n✅ Brand-level forecasts saved to brand_daily_forecast.csv, brand_weekly_forecast.csv, and brand_monthly_forecast.csv\n","output_type":"stream"}],"execution_count":42}]}