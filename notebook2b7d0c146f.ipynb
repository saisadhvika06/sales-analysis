{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":12959761,"sourceType":"datasetVersion","datasetId":8201933},{"sourceId":13043543,"sourceType":"datasetVersion","datasetId":8259391},{"sourceId":13056181,"sourceType":"datasetVersion","datasetId":8267791}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-09-10T15:40:01.219118Z","iopub.execute_input":"2025-09-10T15:40:01.219405Z","execution_failed":"2025-09-10T18:55:18.453Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/sales-prediction/products.csv\n/kaggle/input/sales-prediction/bills (1).csv\n/kaggle/input/sales-prediction/products.csv\n/kaggle/input/sales-prediction/bills (1).csv\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"!pip install openpyxl xlsxwriter\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-12T16:37:25.778754Z","iopub.execute_input":"2025-09-12T16:37:25.779006Z","iopub.status.idle":"2025-09-12T16:37:30.185055Z","shell.execute_reply.started":"2025-09-12T16:37:25.778988Z","shell.execute_reply":"2025-09-12T16:37:30.184135Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: openpyxl in /usr/local/lib/python3.11/dist-packages (3.1.5)\nCollecting xlsxwriter\n  Downloading xlsxwriter-3.2.5-py3-none-any.whl.metadata (2.7 kB)\nRequirement already satisfied: et-xmlfile in /usr/local/lib/python3.11/dist-packages (from openpyxl) (2.0.0)\nDownloading xlsxwriter-3.2.5-py3-none-any.whl (172 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m172.3/172.3 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: xlsxwriter\nSuccessfully installed xlsxwriter-3.2.5\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import pandas as pd\nimport ast\n\ndef find_column(df, possible_names):\n    \"\"\"\n    Finds the correct column name from a list of possibilities,\n    handling case and spacing.\n    \"\"\"\n    for name in possible_names:\n        if name in df.columns.str.strip().tolist():\n            return name\n    return None\n\n# -----------------------------\n# 1️⃣ Load data\n# -----------------------------\nbills_path = \"/kaggle/input/sales-prediction/bills (1).csv\"\nproducts_path = \"/kaggle/input/sales-prediction/products.csv\"\n\ntry:\n    bills_df = pd.read_csv(bills_path)\n    products_df = pd.read_csv(products_path)\nexcept FileNotFoundError as e:\n    print(f\"Error: {e.filename} not found. Please ensure the CSV files are in the same directory.\")\n    exit()\n\n# Clean column names by removing leading/trailing spaces\nbills_df.columns = bills_df.columns.str.strip()\nproducts_df.columns = products_df.columns.str.strip()\n\n# -----------------------------\n# 2️⃣ Prepare Data for Analysis\n# -----------------------------\n# Find the correct column names using a flexible approach\nproduct_id_col = find_column(bills_df, ['Product_ID', 'Product_IDs', 'Product ID', 'Product IDs'])\nquantity_col = find_column(bills_df, ['Quantity', 'Quantities'])\ndiscounted_prices_col = find_column(bills_df, ['Discounted_Prices', 'Discounted Prices'])\nproduct_id_products_col = find_column(products_df, ['ID', 'Product_ID', 'ProductID', 'Product ID'])\n\nif not all([product_id_col, quantity_col, discounted_prices_col, product_id_products_col]):\n    print(\"Error: Could not find one or more required columns.\")\n    print(f\"Bills columns found: {bills_df.columns.tolist()}\")\n    print(f\"Products columns found: {products_df.columns.tolist()}\")\n    exit()\n\n# Convert the 'Date_Time' column to datetime objects\nbills_df['Date_Time'] = pd.to_datetime(bills_df['Date_Time'])\n\n# Convert string-lists to actual lists\nbills_df[product_id_col] = bills_df[product_id_col].apply(ast.literal_eval)\nbills_df[quantity_col] = bills_df[quantity_col].apply(ast.literal_eval)\nbills_df[discounted_prices_col] = bills_df[discounted_prices_col].apply(ast.literal_eval)\n\n# Explode the DataFrame to have one row per product sold\nexploded_df = bills_df.explode([product_id_col, quantity_col, discounted_prices_col])\n\n# Rename ProductID column for merging\nproducts_df = products_df.rename(columns={product_id_products_col: 'ProductID'})\nexploded_df = exploded_df.rename(columns={product_id_col: 'ProductID', quantity_col: 'Quantity'})\n\n# Merge with the products DataFrame\nmerged_df = pd.merge(exploded_df, products_df, on='ProductID', how='left')\n\n# Convert relevant columns to numeric type\nmerged_df['Quantity'] = pd.to_numeric(merged_df['Quantity'])\nmerged_df['Discounted_Prices'] = pd.to_numeric(merged_df['Discounted_Prices'])\n\n# Calculate total revenue per product in each bill\nmerged_df['Total_Revenue'] = merged_df['Quantity'] * merged_df['Discounted_Prices']\n\n# -----------------------------\n# 3️⃣ Create Summaries and save to CSV\n# -----------------------------\n# Create time-based columns\nmerged_df['Date'] = merged_df['Date_Time'].dt.date\nmerged_df['Week_Start'] = merged_df['Date_Time'].dt.to_period('W').dt.start_time.dt.date\nmerged_df['Month'] = merged_df['Date_Time'].dt.to_period('M')\n\n# Daily summary\ndaily_sales_summary = merged_df.groupby(['Date', 'ProductID'], as_index=False)['Quantity'].sum()\ndaily_sales_summary.rename(columns={'Quantity': 'Total_Quantity_Sold'}, inplace=True)\ndaily_sales_summary.to_csv(\"daily_sales_summary.csv\", index=False)\n\n# Weekly summary\nweekly_sales_summary = merged_df.groupby(['Week_Start', 'ProductID'], as_index=False)['Quantity'].sum()\nweekly_sales_summary.rename(columns={'Quantity': 'Total_Quantity_Sold'}, inplace=True)\nweekly_sales_summary.to_csv(\"weekly_sales_summary.csv\", index=False)\n\n# Monthly summary\nmonthly_sales_summary = merged_df.groupby(['Month', 'ProductID'], as_index=False)['Quantity'].sum()\nmonthly_sales_summary.rename(columns={'Quantity': 'Total_Quantity_Sold'}, inplace=True)\nmonthly_sales_summary.to_csv(\"monthly_sales_summary.csv\", index=False)\n\n# -----------------------------\n# 4️⃣ Brand-level summaries\n# -----------------------------\nbrand_daily = merged_df.groupby([\"Date\", \"Brand Name\"], as_index=False)[\"Total_Revenue\"].sum()\nbrand_daily.to_csv(\"brand_daily_sales.csv\", index=False)\n\nbrand_weekly = merged_df.groupby([\"Week_Start\", \"Brand Name\"], as_index=False)[\"Total_Revenue\"].sum()\nbrand_weekly.to_csv(\"brand_weekly_sales.csv\", index=False)\n\nbrand_monthly = merged_df.groupby([\"Month\", \"Brand Name\"], as_index=False)[\"Total_Revenue\"].sum()\nbrand_monthly.to_csv(\"brand_monthly_sales.csv\", index=False)\n\n# -----------------------------\n# 5️⃣ Identify no-sales products\n# -----------------------------\nall_sold_products = set(merged_df[\"ProductID\"].unique())\nno_sales_products = products_df[~products_df[\"ProductID\"].isin(all_sold_products)]\nno_sales_products.to_csv(\"no_sales_products.csv\", index=False)\n\n# -----------------------------\n# Final Output\n# -----------------------------\nprint(\"Sales analysis has been successfully completed and the summaries have been saved to CSV files.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-13T17:37:19.497718Z","iopub.execute_input":"2025-09-13T17:37:19.497972Z","iopub.status.idle":"2025-09-13T17:37:40.121613Z","shell.execute_reply.started":"2025-09-13T17:37:19.497950Z","shell.execute_reply":"2025-09-13T17:37:40.120842Z"}},"outputs":[{"name":"stdout","text":"Sales analysis has been successfully completed and the summaries have been saved to CSV files.\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import pandas as pd\nimport ast\nbills_path = \"/kaggle/input/sales-prediction/bills (1).csv\"\nproducts_path = \"/kaggle/input/sales-prediction/products.csv\"\n\ntry:\n    bills_df = pd.read_csv(bills_path)\n    products_df = pd.read_csv(products_path)\nexcept FileNotFoundError as e:\n    print(f\"Error: {e.filename} not found. Please ensure the CSV files are in the same directory.\")\n    exit()\n\ndef find_column(df, possible_names):\n    \"\"\"\n    Finds the correct column name from a list of possibilities,\n    handling case and spacing.\n    \"\"\"\n    for name in possible_names:\n        if name in df.columns.str.strip().tolist():\n            return name\n    return None\n\ndef generate_correct_monthly_summary(bills_df, products_df):\n    \"\"\"\n    Generates a correct monthly sales summary from the raw bills and products data.\n    \"\"\"\n    # Find the correct column names\n    product_id_col = find_column(bills_df, ['Product_ID', 'Product_IDs', 'Product ID', 'Product IDs'])\n    quantity_col = find_column(bills_df, ['Quantity', 'Quantities'])\n    \n    if not all([product_id_col, quantity_col]):\n        print(\"Error: Could not find required columns in bills_df for generating a correct summary.\")\n        return None\n\n    bills_df['Date_Time'] = pd.to_datetime(bills_df['Date_Time'])\n    bills_df[product_id_col] = bills_df[product_id_col].apply(ast.literal_eval)\n    bills_df[quantity_col] = bills_df[quantity_col].apply(ast.literal_eval)\n\n    exploded_df = bills_df.explode([product_id_col, quantity_col])\n    exploded_df = exploded_df.rename(columns={product_id_col: 'ProductID', quantity_col: 'Total_Quantity_Sold'})\n    \n    products_df.columns = products_df.columns.str.strip()\n    products_df = products_df.rename(columns={'ID': 'ProductID'})\n\n    merged_df = pd.merge(exploded_df, products_df, on='ProductID', how='left')\n    merged_df['Total_Quantity_Sold'] = pd.to_numeric(merged_df['Total_Quantity_Sold'])\n    \n    merged_df['Month'] = merged_df['Date_Time'].dt.to_period('M')\n    \n    correct_summary = merged_df.groupby(['Month', 'ProductID'], as_index=False)['Total_Quantity_Sold'].sum()\n    correct_summary['Month'] = correct_summary['Month'].astype(str)\n    \n    return correct_summary\n\n# -----------------------------\n# 1️⃣ Load data\n# -----------------------------\ntry:\n    bills_df = pd.read_csv('bills (1).csv')\n    products_df = pd.read_csv('products.csv')\n    user_summary_df = pd.read_csv('monthly_sales_summary (4).csv')\nexcept FileNotFoundError as e:\n    print(f\"Error: {e.filename} not found. Please ensure all three CSV files are in the same folder as this script.\")\n    exit()\n\n# Clean column names in user's file for consistency\nuser_summary_df.columns = user_summary_df.columns.str.strip()\n\n# -----------------------------\n# 2️⃣ Generate a correct summary for comparison\n# -----------------------------\ncorrect_summary = generate_correct_monthly_summary(bills_df, products_df)\n\nif correct_summary is None:\n    exit()\n    \n# -----------------------------\n# 3️⃣ Compare the two summaries\n# -----------------------------\n# Merge the correct summary with the user's summary\ncomparison_df = pd.merge(\n    user_summary_df,\n    correct_summary,\n    on=['Month', 'ProductID'],\n    suffixes=('_User', '_Correct')\n)\n\n# Identify discrepancies\ncomparison_df['Match'] = comparison_df['Total_Quantity_Sold_User'] == comparison_df['Total_Quantity_Sold_Correct']\n\n# -----------------------------\n# 4️⃣ Print the Accuracy Report\n# -----------------------------\ntotal_records = len(comparison_df)\naccurate_records = comparison_df['Match'].sum()\ninaccurate_records = total_records - accurate_records\n\nprint(\"\\n### Monthly Sales Summary Accuracy Report ###\")\nprint(\"---------------------------------------------\")\nprint(f\"Total records in both summaries: {total_records}\")\nprint(f\"Accurate records (exact matches): {accurate_records}\")\nprint(f\"Inaccurate records (discrepancies): {inaccurate_records}\")\n\nif inaccurate_records > 0:\n    print(\"\\n### Sample of Incorrect Records ###\")\n    print(comparison_df[~comparison_df['Match']].head(10).to_string(index=False))\nelse:\n    print(\"\\nAll records match perfectly!\")\n\nprint(\"\\n---------------------------------------------\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-13T18:35:59.166167Z","iopub.execute_input":"2025-09-13T18:35:59.166456Z","iopub.status.idle":"2025-09-13T18:36:02.475279Z","shell.execute_reply.started":"2025-09-13T18:35:59.166436Z","shell.execute_reply":"2025-09-13T18:36:02.474369Z"}},"outputs":[{"name":"stdout","text":"Error: bills (1).csv not found. Please ensure all three CSV files are in the same folder as this script.\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_36/1040260800.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;31m# Clean column names in user's file for consistency\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m \u001b[0muser_summary_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0muser_summary_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;31m# -----------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'user_summary_df' is not defined"],"ename":"NameError","evalue":"name 'user_summary_df' is not defined","output_type":"error"}],"execution_count":1},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.preprocessing import MinMaxScaler\nfrom joblib import Parallel, delayed\nimport numpy as np\nimport pandas as pd\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# -----------------------------\n# Custom Dataset\n# -----------------------------\nclass TimeSeriesDataset(Dataset):\n    def __init__(self, series, seq_len):\n        self.series = series\n        self.seq_len = seq_len\n\n    def __len__(self):\n        return len(self.series) - self.seq_len\n\n    def __getitem__(self, idx):\n        return (\n            torch.tensor(self.series[idx:idx+self.seq_len], dtype=torch.float32).unsqueeze(-1),\n            torch.tensor(self.series[idx+self.seq_len], dtype=torch.float32)\n        )\n\n# -----------------------------\n# GRU with Attention Model\n# -----------------------------\nclass GRUAttention(nn.Module):\n    def __init__(self, input_dim=1, hidden_dim=32, num_layers=1):\n        super().__init__()\n        self.gru = nn.GRU(input_dim, hidden_dim, num_layers, batch_first=True)\n        self.attn_fc = nn.Linear(hidden_dim, 1)\n        self.fc = nn.Linear(hidden_dim, 1)\n\n    def forward(self, x):\n        gru_out, _ = self.gru(x)  # (batch, seq_len, hidden_dim)\n        attn_weights = torch.softmax(self.attn_fc(gru_out), dim=1)  # (batch, seq_len, 1)\n        context_vector = torch.sum(attn_weights * gru_out, dim=1)   # (batch, hidden_dim)\n        output = self.fc(context_vector)  # (batch, 1)\n        return output\n\n# -----------------------------\n# Training + Forecasting Function\n# -----------------------------\ndef train_and_forecast(series, future=30, seq_len=30, epochs=50, batch_size=64):\n    scaler = MinMaxScaler()\n    series = scaler.fit_transform(series.reshape(-1,1)).flatten()\n\n    dataset = TimeSeriesDataset(series, seq_len)\n    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n\n    model = GRUAttention().to(device)\n    loss_fn = nn.MSELoss()\n    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n\n    # Early stopping\n    best_loss = float('inf')\n    patience = 5\n    no_improve = 0\n\n    for epoch in range(epochs):\n        epoch_loss = 0\n        for xb, yb in dataloader:\n            xb, yb = xb.to(device), yb.to(device)\n            pred = model(xb).squeeze(-1)\n            loss = loss_fn(pred, yb)\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n            epoch_loss += loss.item()\n\n        avg_loss = epoch_loss / len(dataloader)\n        if avg_loss < best_loss:\n            best_loss = avg_loss\n            no_improve = 0\n        else:\n            no_improve += 1\n\n        if no_improve >= patience:\n            break\n\n    # Forecast loop\n    input_seq = torch.tensor(series[-seq_len:], dtype=torch.float32).unsqueeze(0).unsqueeze(-1).to(device)\n    preds = []\n    model.eval()\n    with torch.no_grad():\n        for _ in range(future):\n            out = model(input_seq).item()\n            preds.append(out)\n            new_input = torch.cat([input_seq[:,1:,:], torch.tensor([[[out]]], dtype=torch.float32).to(device)], dim=1)\n            input_seq = new_input\n\n    preds = scaler.inverse_transform(np.array(preds).reshape(-1,1)).flatten()\n    return preds\n\n# -----------------------------\n# Forecast per Product\n# -----------------------------\ndef process_product_all(product_id, df, future_daily=30, future_week=7, future_month=30, seq_len=30):\n    group = df[df[\"ProductID\"] == product_id].sort_values(\"Date\")\n    series = group[\"Total_Quantity_Sold\"].values\n    if len(series) < seq_len:\n        return None\n\n    daily_forecast = train_and_forecast(series, future=future_daily, seq_len=seq_len)\n    week_forecast = train_and_forecast(series, future=future_week, seq_len=seq_len)\n    month_forecast = train_and_forecast(series, future=future_month, seq_len=seq_len)\n\n    return {\n        \"ProductID\": product_id,\n        \"Daily_Forecast\": daily_forecast.tolist(),\n        \"Week_Forecast\": week_forecast.tolist(),\n        \"Month_Forecast\": month_forecast.tolist()\n    }\n\n# -----------------------------\n# Run Forecasting in Parallel\n# -----------------------------\nproduct_ids = summary_daily_df[\"ProductID\"].unique()\nresults = Parallel(n_jobs=-1)(\n    delayed(process_product_all)(pid, summary_daily_df) for pid in product_ids\n)\nresults = [r for r in results if r is not None]\n\nforecast_df = pd.DataFrame(results)\nforecast_df = forecast_df.merge(products_df[['ProductID', 'Brand Name']], on='ProductID', how='left')\nforecast_df.to_csv(\"product_forecasts_all.csv\", index=False)\nprint(\"✅ Product-level forecasts saved (daily, weekly, monthly)\")\n\n# -----------------------------\n# BRAND-LEVEL FORECASTS\n# -----------------------------\ndef aggregate_brand_forecast(forecast_df, col_name, horizon_name):\n    expanded = []\n    for _, row in forecast_df.iterrows():\n        for i, val in enumerate(row[col_name]):\n            expanded.append({\"Brand\": row['Brand Name'], horizon_name: i+1, \"Forecast_Qty\": val})\n    expanded_df = pd.DataFrame(expanded)\n    brand_forecast = expanded_df.groupby([\"Brand\", horizon_name], as_index=False)[\"Forecast_Qty\"].sum()\n    return brand_forecast\n\n# Daily\nbrand_daily_forecast = aggregate_brand_forecast(forecast_df, \"Daily_Forecast\", \"Day\")\nbrand_daily_forecast.to_csv(\"brand_daily_forecast.csv\", index=False)\n\n# Weekly\nbrand_week_forecast = aggregate_brand_forecast(forecast_df, \"Week_Forecast\", \"Week\")\nbrand_week_forecast.to_csv(\"brand_week_forecast.csv\", index=False)\n\n# Monthly\nbrand_month_forecast = aggregate_brand_forecast(forecast_df, \"Month_Forecast\", \"Month\")\nbrand_month_forecast.to_csv(\"brand_month_forecast.csv\", index=False)\n\nprint(\"✅ Brand-level forecasts saved (daily, weekly, monthly)\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-10T16:14:45.767144Z","iopub.execute_input":"2025-09-10T16:14:45.767672Z","iopub.status.idle":"2025-09-10T16:35:59.854386Z","shell.execute_reply.started":"2025-09-10T16:14:45.767649Z","shell.execute_reply":"2025-09-10T16:35:59.853700Z"}},"outputs":[{"name":"stdout","text":"✅ Product-level forecasts saved (daily, weekly, monthly)\n✅ Brand-level forecasts saved (daily, weekly, monthly)\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"import pandas as pd\nimport ast\nfrom datetime import datetime\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.preprocessing import MinMaxScaler\nfrom joblib import Parallel, delayed\nimport numpy as np\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# -----------------------------\n# 1️⃣ Load Data\n# -----------------------------\nbills_path = \"/kaggle/input/sales-prediction/bills (1).csv\"\nproducts_path = \"/kaggle/input/sales-prediction/products.csv\"\n\nbills_df = pd.read_csv(bills_path)\nproducts_df = pd.read_csv(products_path)\nproducts_df[\"ProductID\"] = products_df[\"ID\"].astype(str)\n\n# -----------------------------\n# 2️⃣ Parse and Explode Data\n# -----------------------------\nbills_df['Product_IDs'] = bills_df['Product_IDs'].apply(ast.literal_eval)\nbills_df['Quantities'] = bills_df['Quantities'].apply(ast.literal_eval)\nbills_df['Date'] = pd.to_datetime(bills_df['Date_Time']).dt.date\n\nexploded_df = bills_df.explode(['Product_IDs', 'Quantities'])\nexploded_df['ProductID'] = exploded_df['Product_IDs'].astype(str)\nexploded_df['Quantity_Sold'] = exploded_df['Quantities'].astype(int)\n\n# -----------------------------\n# 3️⃣ Aggregate Daily Sales\n# -----------------------------\nsummary_daily_df = (\n    exploded_df.groupby(['Date', 'ProductID'], as_index=False)['Quantity_Sold']\n    .sum()\n    .rename(columns={'Quantity_Sold': 'Total_Quantity_Sold'})\n)\n\n# -----------------------------\n# 4️⃣ Identify No-Sales Products\n# -----------------------------\nall_sold_products = set(summary_daily_df[\"ProductID\"].unique())\nno_sales_products_df = products_df[~products_df[\"ProductID\"].isin(all_sold_products)]\nno_sales_products_df.to_csv(\"no_sales_products.csv\", index=False)\n\n# Save daily sales summary\nsummary_daily_df.to_csv(\"summary_daily_sales.csv\", index=False)\n\n# -----------------------------\n# 5️⃣ Brand-Level Summaries\n# -----------------------------\nsummary_daily_df = summary_daily_df.merge(products_df[['ProductID', 'Brand Name']], on='ProductID', how='left')\n\nbrand_daily = summary_daily_df.groupby([\"Date\", \"Brand Name\"], as_index=False)[\"Total_Quantity_Sold\"].sum()\nbrand_daily.to_csv(\"brand_daily_sales.csv\", index=False)\n\n# -----------------------------\n# 6️⃣ Forecasting Model Definition\n# -----------------------------\nclass TimeSeriesDataset(Dataset):\n    def __init__(self, series, seq_len):\n        self.series = series\n        self.seq_len = seq_len\n\n    def __len__(self):\n        return len(self.series) - self.seq_len\n\n    def __getitem__(self, idx):\n        return (\n            torch.tensor(self.series[idx:idx+self.seq_len], dtype=torch.float32).unsqueeze(-1),\n            torch.tensor(self.series[idx+self.seq_len], dtype=torch.float32)\n        )\n\nclass GRUAttention(nn.Module):\n    def __init__(self, input_dim=1, hidden_dim=32, num_layers=1):\n        super().__init__()\n        self.gru = nn.GRU(input_dim, hidden_dim, num_layers, batch_first=True)\n        self.attn_fc = nn.Linear(hidden_dim, 1)\n        self.fc = nn.Linear(hidden_dim, 1)\n\n    def forward(self, x):\n        gru_out, _ = self.gru(x)\n        attn_weights = torch.softmax(self.attn_fc(gru_out), dim=1)\n        context_vector = torch.sum(attn_weights * gru_out, dim=1)\n        output = self.fc(context_vector)\n        return output\n\ndef train_and_forecast(series, future=30, seq_len=30, epochs=50, batch_size=64):\n    scaler = MinMaxScaler()\n    series = scaler.fit_transform(series.reshape(-1,1)).flatten()\n\n    dataset = TimeSeriesDataset(series, seq_len)\n    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n\n    model = GRUAttention().to(device)\n    loss_fn = nn.MSELoss()\n    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n\n    best_loss = float('inf')\n    patience = 5\n    no_improve = 0\n\n    for epoch in range(epochs):\n        epoch_loss = 0\n        for xb, yb in dataloader:\n            xb, yb = xb.to(device), yb.to(device)\n            pred = model(xb).squeeze(-1)\n            loss = loss_fn(pred, yb)\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n            epoch_loss += loss.item()\n\n        avg_loss = epoch_loss / len(dataloader)\n        if avg_loss < best_loss:\n            best_loss = avg_loss\n            no_improve = 0\n        else:\n            no_improve += 1\n        if no_improve >= patience:\n            break\n\n    input_seq = torch.tensor(series[-seq_len:], dtype=torch.float32).unsqueeze(0).unsqueeze(-1).to(device)\n    preds = []\n    model.eval()\n    with torch.no_grad():\n        for _ in range(future):\n            out = model(input_seq).item()\n            preds.append(out)\n            new_input = torch.cat([input_seq[:,1:,:], torch.tensor([[[out]]], dtype=torch.float32).to(device)], dim=1)\n            input_seq = new_input\n\n    preds = scaler.inverse_transform(np.array(preds).reshape(-1,1)).flatten()\n    return preds\n\n# -----------------------------\n# 7️⃣ Forecast Per Product\n# -----------------------------\ndef process_product_all(product_id, df, future_daily=30, future_week=7, future_month=30, seq_len=30):\n    group = df[df[\"ProductID\"] == product_id].sort_values(\"Date\")\n    series = group[\"Total_Quantity_Sold\"].values\n    if len(series) < seq_len:\n        return None\n\n    daily_forecast = train_and_forecast(series, future=future_daily, seq_len=seq_len)\n    week_forecast = train_and_forecast(series, future=future_week, seq_len=seq_len)\n    month_forecast = train_and_forecast(series, future=future_month, seq_len=seq_len)\n\n    return {\n        \"ProductID\": product_id,\n        \"Daily_Forecast\": daily_forecast.tolist(),\n        \"Week_Forecast\": week_forecast.tolist(),\n        \"Month_Forecast\": month_forecast.tolist()\n    }\n\nproduct_ids = summary_daily_df[\"ProductID\"].unique()\nresults = Parallel(n_jobs=-1)(\n    delayed(process_product_all)(pid, summary_daily_df) for pid in product_ids\n)\nresults = [r for r in results if r is not None]\n\nforecast_df = pd.DataFrame(results)\nforecast_df = forecast_df.merge(products_df[['ProductID', 'Brand Name']], on='ProductID', how='left')\nforecast_df.to_csv(\"product_forecasts_all.csv\", index=False)\n\n# -----------------------------\n# 8️⃣ Aggregate Brand Forecasts\n# -----------------------------\ndef aggregate_brand_forecast(forecast_df, col_name, horizon_name):\n    expanded = []\n    for _, row in forecast_df.iterrows():\n        for i, val in enumerate(row[col_name]):\n            expanded.append({\"Brand\": row['Brand Name'], horizon_name: i+1, \"Forecast_Qty\": val})\n    expanded_df = pd.DataFrame(expanded)\n    brand_forecast = expanded_df.groupby([\"Brand\", horizon_name], as_index=False)[\"Forecast_Qty\"].sum()\n    return brand_forecast\n\nbrand_daily_forecast = aggregate_brand_forecast(forecast_df, \"Daily_Forecast\", \"Day\")\nbrand_daily_forecast.to_csv(\"brand_daily_forecast.csv\", index=False)\n\nbrand_week_forecast = aggregate_brand_forecast(forecast_df, \"Week_Forecast\", \"Week\")\nbrand_week_forecast.to_csv(\"brand_week_forecast.csv\", index=False)\n\nbrand_month_forecast = aggregate_brand_forecast(forecast_df, \"Month_Forecast\", \"Month\")\nbrand_month_forecast.to_csv(\"brand_month_forecast.csv\", index=False)\n\nprint(\"✅ All steps completed successfully.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-12T17:34:03.460694Z","iopub.execute_input":"2025-09-12T17:34:03.461288Z","iopub.status.idle":"2025-09-12T17:47:39.043523Z","shell.execute_reply.started":"2025-09-12T17:34:03.461262Z","shell.execute_reply":"2025-09-12T17:47:39.042723Z"}},"outputs":[{"name":"stdout","text":"✅ All steps completed successfully.\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.preprocessing import MinMaxScaler\nfrom joblib import Parallel, delayed\nimport ast\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# -----------------------------\n# Load Data\n# -----------------------------\nbills_df = pd.read_csv(\"/kaggle/input/sales-prediction/bills (1).csv\")\nproducts_df = pd.read_csv(\"/kaggle/input/sales-prediction/products.csv\")\n\n# -----------------------------\n# Safe Preprocessing & Merge\n# -----------------------------\nbills_df.columns = bills_df.columns.str.strip()\n\n# Expand multiple Product IDs\nbills_df[\"Product_IDs\"] = bills_df[\"Product_IDs\"].astype(str)\nbills_df = bills_df.assign(ProductID=bills_df[\"Product_IDs\"].str.split(\",\")).explode(\"ProductID\")\nbills_df[\"ProductID\"] = bills_df[\"ProductID\"].str.strip()\n\n# Convert stringified lists to Python lists for quantities\nbills_df[\"Product_IDs\"] = bills_df[\"Product_IDs\"].apply(ast.literal_eval)\nbills_df[\"Quantities\"] = bills_df[\"Quantities\"].apply(ast.literal_eval)\n\n# Explode each product and quantity\nbills_df = bills_df.explode([\"Product_IDs\", \"Quantities\"])\nbills_df[\"ProductID\"] = bills_df[\"Product_IDs\"].astype(str).str.strip()\nbills_df[\"Quantity_Sold\"] = bills_df[\"Quantities\"].astype(int)\n\n# -----------------------------\n# Ensure products_df columns\n# -----------------------------\nproducts_df.rename(columns={\n    'Product Code': 'ProductID',\n    'Product Name': 'Product_Name',\n    'Brand Name': 'Brand Name',\n    'Price (INR)': 'Price'\n}, inplace=True)\n\nproducts_df[\"ProductID\"] = products_df[\"ProductID\"].astype(str).str.strip()\n\n# Merge bills with products\nmerged_df = bills_df.merge(products_df[['ProductID', 'Product_Name', 'Brand Name']], on=\"ProductID\", how=\"left\")\nmerged_df[\"Date\"] = pd.to_datetime(merged_df[\"Date_Time\"]).dt.date\n\nprint(\"✅ Merged shape:\", merged_df.shape)\n\n# -----------------------------\n# Prepare Daily Summary\n# -----------------------------\nsummary_daily_df = (\n    merged_df.groupby([\"Date\", \"ProductID\", \"Brand Name\"])[\"Quantity_Sold\"]\n    .sum()\n    .reset_index()\n    .rename(columns={\"Quantity_Sold\": \"Total_Quantity_Sold\"})\n)\n\n# -----------------------------\n# TimeSeries Dataset\n# -----------------------------\nclass TimeSeriesDataset(Dataset):\n    def __init__(self, series, seq_len):\n        series = np.array(series, dtype=float)\n        if len(series) <= seq_len:\n            series = np.pad(series, (0, seq_len + 1 - len(series)), mode=\"constant\", constant_values=0.1)\n        self.series = series\n        self.seq_len = seq_len\n\n    def __len__(self):\n        return max(len(self.series) - self.seq_len, 1)\n\n    def __getitem__(self, idx):\n        idx = min(idx, len(self.series) - self.seq_len - 1)\n        return (\n            torch.tensor(self.series[idx:idx+self.seq_len], dtype=torch.float32).unsqueeze(-1),\n            torch.tensor(self.series[idx+self.seq_len], dtype=torch.float32)\n        )\n\n# -----------------------------\n# GRU + Attention Model\n# -----------------------------\nclass GRUAttention(nn.Module):\n    def __init__(self, input_dim=1, hidden_dim=32, num_layers=1):\n        super().__init__()\n        self.gru = nn.GRU(input_dim, hidden_dim, num_layers, batch_first=True)\n        self.attn_fc = nn.Linear(hidden_dim, 1)\n        self.fc = nn.Linear(hidden_dim, 1)\n\n    def forward(self, x):\n        gru_out, _ = self.gru(x)\n        attn_weights = torch.softmax(self.attn_fc(gru_out), dim=1)\n        context_vector = torch.sum(attn_weights * gru_out, dim=1)\n        output = self.fc(context_vector)\n        return output\n\n# -----------------------------\n# Train and Forecast\n# -----------------------------\ndef train_and_forecast(series, future=30, seq_len=30, epochs=50, batch_size=64):\n    if np.sum(series) == 0:\n        return np.zeros(future)\n\n    scaler = MinMaxScaler()\n    series_scaled = scaler.fit_transform(series.reshape(-1,1)).flatten()\n\n    dataset = TimeSeriesDataset(series_scaled, seq_len)\n    batch_size = min(batch_size, len(dataset))\n    shuffle = len(dataset) > 1\n    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle)\n\n    model = GRUAttention().to(device)\n    loss_fn = nn.MSELoss()\n    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n\n    best_loss = float('inf')\n    patience, no_improve = 5, 0\n\n    for epoch in range(epochs):\n        epoch_loss = 0\n        for xb, yb in dataloader:\n            xb, yb = xb.to(device), yb.to(device)\n            pred = model(xb).squeeze(-1)\n            loss = loss_fn(pred, yb)\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n            epoch_loss += loss.item()\n        avg_loss = epoch_loss / len(dataloader)\n        if avg_loss < best_loss:\n            best_loss = avg_loss\n            no_improve = 0\n        else:\n            no_improve += 1\n        if no_improve >= patience:\n            break\n\n    # Forecast\n    model.eval()\n    input_seq = torch.tensor(series_scaled[-seq_len:], dtype=torch.float32).unsqueeze(0).unsqueeze(-1).to(device)\n    preds = []\n    with torch.no_grad():\n        for _ in range(future):\n            out = model(input_seq).item()\n            preds.append(out)\n            new_input = torch.cat([input_seq[:,1:,:], torch.tensor([[[out]]], dtype=torch.float32).to(device)], dim=1)\n            input_seq = new_input\n\n    preds = scaler.inverse_transform(np.array(preds).reshape(-1,1)).flatten()\n    preds = np.maximum(preds, 0)\n    return np.round(preds)\n\n# -----------------------------\n# Product-level Forecasts\n# -----------------------------\ndef process_product_all(product_id, df, seq_len=30):\n    group = df[df[\"ProductID\"] == product_id].sort_values(\"Date\")\n    series = group[\"Total_Quantity_Sold\"].values\n    if len(series) < 2:\n        return None\n    return {\n        \"ProductID\": product_id,\n        \"Brand Name\": group[\"Brand Name\"].iloc[0],\n        \"Daily_Forecast\": train_and_forecast(series, future=30, seq_len=seq_len).tolist(),\n        \"Weekly_Forecast\": train_and_forecast(series, future=7, seq_len=seq_len).tolist(),\n        \"Monthly_Forecast\": train_and_forecast(series, future=30, seq_len=seq_len).tolist()\n    }\n\nproduct_ids = summary_daily_df[\"ProductID\"].unique()\nresults = Parallel(n_jobs=-1)(\n    delayed(process_product_all)(pid, summary_daily_df) for pid in product_ids\n)\nresults = [r for r in results if r is not None]\n\nforecast_df = pd.DataFrame(results)\nforecast_df.to_csv(\"product_forecasts_all.csv\", index=False)\nprint(\"✅ Product-level forecasts saved\")\n\n# -----------------------------\n# Brand-level Forecasts (aggregate across products)\n# -----------------------------\ndef aggregate_brand_forecast(forecast_df, col_name, horizon_name):\n    expanded = []\n    for _, row in forecast_df.iterrows():\n        brand_col = row.get('Brand Name', 'Unknown')\n        for i, val in enumerate(row[col_name]):\n            expanded.append({\"Brand\": brand_col, horizon_name: i+1, \"Forecast_Qty\": val})\n    expanded_df = pd.DataFrame(expanded)\n    brand_forecast = expanded_df.groupby([\"Brand\", horizon_name], as_index=False)[\"Forecast_Qty\"].sum()\n    brand_forecast[\"Forecast_Qty\"] = brand_forecast[\"Forecast_Qty\"].round()\n    return brand_forecast\n\nbrand_daily_forecast = aggregate_brand_forecast(forecast_df, \"Daily_Forecast\", \"Day\")\nbrand_weekly_forecast = aggregate_brand_forecast(forecast_df, \"Weekly_Forecast\", \"Week\")\nbrand_monthly_forecast = aggregate_brand_forecast(forecast_df, \"Monthly_Forecast\", \"Month\")\n\nbrand_daily_forecast.to_csv(\"brand_daily_forecast.csv\", index=False)\nbrand_weekly_forecast.to_csv(\"brand_weekly_forecast.csv\", index=False)\nbrand_monthly_forecast.to_csv(\"brand_monthly_forecast.csv\", index=False)\nprint(\"✅ Brand-level forecasts saved (aggregated across products)\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-13T05:30:07.243216Z","iopub.execute_input":"2025-09-13T05:30:07.243503Z","iopub.status.idle":"2025-09-13T05:30:55.087965Z","shell.execute_reply.started":"2025-09-13T05:30:07.243482Z","shell.execute_reply":"2025-09-13T05:30:55.087036Z"}},"outputs":[{"name":"stdout","text":"✅ Merged shape: (5398837, 16)\n✅ Product-level forecasts saved\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_36/3409059029.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    197\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mbrand_forecast\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m \u001b[0mbrand_daily_forecast\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maggregate_brand_forecast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mforecast_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Daily_Forecast\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Day\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m \u001b[0mbrand_weekly_forecast\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maggregate_brand_forecast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mforecast_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Weekly_Forecast\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Week\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m \u001b[0mbrand_monthly_forecast\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maggregate_brand_forecast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mforecast_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Monthly_Forecast\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Month\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_36/3409059029.py\u001b[0m in \u001b[0;36maggregate_brand_forecast\u001b[0;34m(forecast_df, col_name, horizon_name)\u001b[0m\n\u001b[1;32m    193\u001b[0m             \u001b[0mexpanded\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"Brand\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbrand_col\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhorizon_name\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Forecast_Qty\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mval\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m     \u001b[0mexpanded_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexpanded\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 195\u001b[0;31m     \u001b[0mbrand_forecast\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexpanded_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Brand\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhorizon_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mas_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Forecast_Qty\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m     \u001b[0mbrand_forecast\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Forecast_Qty\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbrand_forecast\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Forecast_Qty\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mbrand_forecast\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36mgroupby\u001b[0;34m(self, by, axis, level, as_index, sort, group_keys, observed, dropna)\u001b[0m\n\u001b[1;32m   9181\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"You have to supply one of 'by' and 'level'\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   9182\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 9183\u001b[0;31m         return DataFrameGroupBy(\n\u001b[0m\u001b[1;32m   9184\u001b[0m             \u001b[0mobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   9185\u001b[0m             \u001b[0mkeys\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mby\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/groupby/groupby.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, obj, keys, axis, level, grouper, exclusions, selection, as_index, sort, group_keys, observed, dropna)\u001b[0m\n\u001b[1;32m   1327\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1328\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mgrouper\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1329\u001b[0;31m             grouper, exclusions, obj = get_grouper(\n\u001b[0m\u001b[1;32m   1330\u001b[0m                 \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1331\u001b[0m                 \u001b[0mkeys\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/groupby/grouper.py\u001b[0m in \u001b[0;36mget_grouper\u001b[0;34m(obj, key, axis, level, sort, observed, validate, dropna)\u001b[0m\n\u001b[1;32m   1041\u001b[0m                 \u001b[0min_axis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgpr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgpr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1042\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1043\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgpr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1044\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgpr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mGrouper\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mgpr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkey\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1045\u001b[0m             \u001b[0;31m# Add key to exclusions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyError\u001b[0m: 'Brand'"],"ename":"KeyError","evalue":"'Brand'","output_type":"error"}],"execution_count":36},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error\n\n# -------------------------\n# 1. Load Data\n# -------------------------\nbills_df = pd.read_csv(\"/kaggle/input/trail-bills/10000.csv\")\nproducts_df = pd.read_csv(\"/kaggle/input/sales-prediction/products.csv\")\n\n# Clean column names\nbills_df.columns = bills_df.columns.str.strip()\nproducts_df.columns = products_df.columns.str.strip()\n\n# Force rename the last column in products_df → \"ProductID\"\nproducts_df.rename(columns={products_df.columns[-1]: \"ProductID\"}, inplace=True)\n\n# -------------------------\n# Process Bills Data\n# -------------------------\n# Expand multiple product IDs in one bill\nbills_df[\"Product_IDs\"] = bills_df[\"Product_IDs\"].astype(str)\nbills_df = bills_df.assign(ProductID=bills_df[\"Product_IDs\"].str.split(\",\")).explode(\"ProductID\")\nbills_df[\"ProductID\"] = bills_df[\"ProductID\"].str.strip()\n\n# -------------------------\n# Merge with Products\n# -------------------------\nproducts_df[\"ProductID\"] = products_df[\"ProductID\"].astype(str).str.strip()\nmerged_df = bills_df.merge(products_df, on=\"ProductID\", how=\"left\")\nmerged_df[\"Date\"] = pd.to_datetime(merged_df[\"Date_Time\"]).dt.date\n\nprint(\"✅ Merged shape:\", merged_df.shape)\nprint(merged_df.head())\n\n# -------------------------\n# 3. Aggregate Daily Sales\n# -------------------------\ndaily_sales = (\n    merged_df.groupby([\"Date\", \"ProductID\", \"Product Name\", \"Brand Name\"])\n    .agg(Total_Sales=(\"Total_Amount\", \"sum\"))\n    .reset_index()\n)\n\n# -------------------------\n# 4. PyTorch Dataset\n# -------------------------\nclass SalesDataset(Dataset):\n    def __init__(self, series, seq_len=7):\n        self.scaler = MinMaxScaler()\n        self.series = self.scaler.fit_transform(series.reshape(-1, 1))\n        self.seq_len = seq_len\n\n    def __len__(self):\n        return len(self.series) - self.seq_len\n\n    def __getitem__(self, idx):\n        X = self.series[idx:idx+self.seq_len]\n        y = self.series[idx+self.seq_len]\n        return torch.FloatTensor(X), torch.FloatTensor(y)\n\n# -------------------------\n# 5. Simple LSTM Model\n# -------------------------\nclass LSTMModel(nn.Module):\n    def __init__(self, input_size=1, hidden_size=64, num_layers=2):\n        super().__init__()\n        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n        self.fc = nn.Linear(hidden_size, 1)\n\n    def forward(self, x):\n        out, _ = self.lstm(x)\n        return self.fc(out[:, -1, :])\n\n# -------------------------\n# 6. Train & Forecast\n# -------------------------\ndef train_and_forecast(series, days=30, seq_len=7, epochs=20):\n    if len(series) < seq_len + 1:\n        return [0] * days  # not enough data\n\n    dataset = SalesDataset(series, seq_len)\n    loader = DataLoader(dataset, batch_size=16, shuffle=True)\n\n    model = LSTMModel()\n    criterion = nn.MSELoss()\n    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n\n    # Training\n    for epoch in range(epochs):\n        for X, y in loader:\n            X = X.unsqueeze(-1)  # [batch, seq_len, 1]\n            y = y.unsqueeze(-1)\n            optimizer.zero_grad()\n            output = model(X)\n            loss = criterion(output, y)\n            loss.backward()\n            optimizer.step()\n\n    # Forecasting\n    scaler = dataset.scaler\n    last_seq = dataset.series[-seq_len:]\n    preds = []\n\n    for _ in range(days):\n        inp = torch.FloatTensor(last_seq).unsqueeze(0).unsqueeze(-1)\n        pred = model(inp).item()\n        preds.append(pred)\n        last_seq = np.append(last_seq[1:], pred)\n\n    preds = scaler.inverse_transform(np.array(preds).reshape(-1, 1)).flatten()\n    return preds.tolist()\n\n# -------------------------\n# 7. Forecast Products\n# -------------------------\nproduct_forecasts = []\nfor pid, group in daily_sales.groupby(\"ProductID\"):\n    series = group.sort_values(\"Date\")[\"Total_Sales\"].values\n    preds = train_and_forecast(series)\n\n    weekly_avg = round(np.mean(preds[:7]), 2)\n    monthly_avg = round(np.mean(preds[:30]), 2)\n\n    product_forecasts.append({\n        \"ProductID\": pid,\n        \"Product Name\": group[\"Product Name\"].iloc[0],\n        \"Predictions\": preds,\n        \"Weekly_Avg\": weekly_avg,\n        \"Monthly_Avg\": monthly_avg\n    })\n\nproduct_forecasts_df = pd.DataFrame(product_forecasts)\nproduct_forecasts_df.to_csv(\"product_forecasts.csv\", index=False)\n\n# -------------------------\n# 8. Forecast Brands\n# -------------------------\nbrand_forecasts = []\nfor (pid, brand), group in daily_sales.groupby([\"ProductID\", \"Brand Name\"]):\n    series = group.sort_values(\"Date\")[\"Total_Sales\"].values\n    preds = train_and_forecast(series)\n\n    weekly_avg = round(np.mean(preds[:7]), 2)\n    monthly_avg = round(np.mean(preds[:30]), 2)\n\n    brand_forecasts.append({\n        \"ProductID\": pid,\n        \"Brand Name\": brand,\n        \"Predictions\": preds,\n        \"Weekly_Avg\": weekly_avg,\n        \"Monthly_Avg\": monthly_avg\n    })\n\nbrand_forecasts_df = pd.DataFrame(brand_forecasts)\nbrand_forecasts_df.to_csv(\"brand_forecasts.csv\", index=False)\n\nprint(\"✅ Forecasting complete. Results saved to product_forecasts.csv and brand_forecasts.csv\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-13T04:19:34.839963Z","iopub.execute_input":"2025-09-13T04:19:34.840508Z","iopub.status.idle":"2025-09-13T04:19:35.019612Z","shell.execute_reply.started":"2025-09-13T04:19:34.840486Z","shell.execute_reply":"2025-09-13T04:19:35.018815Z"}},"outputs":[{"name":"stdout","text":"✅ Merged shape: (39704, 22)\n     Bill_ID  Mobile_Number            Date_Time  \\\n0  BILL00001     9503299454  2024-07-06 01:09:51   \n1  BILL00001     9503299454  2024-07-06 01:09:51   \n2  BILL00001     9503299454  2024-07-06 01:09:51   \n3  BILL00002     9518832283  2022-11-25 05:51:28   \n4  BILL00002     9518832283  2022-11-25 05:51:28   \n\n                                         Product_IDs       Quantities  \\\n0  ['PVSL0504031125', 'PVSL0502021025', 'PVSL0516...        [9, 2, 4]   \n1  ['PVSL0504031125', 'PVSL0502021025', 'PVSL0516...        [9, 2, 4]   \n2  ['PVSL0504031125', 'PVSL0502021025', 'PVSL0516...        [9, 2, 4]   \n3  ['PVSL0905031125', 'PVSL0306030126', 'PVSL0114...  [5, 5, 3, 5, 3]   \n4  ['PVSL0905031125', 'PVSL0306030126', 'PVSL0114...  [5, 5, 3, 5, 3]   \n\n                         Original_Prices                    Offers_Applied  \\\n0         [36319.38, 19292.36, 35040.03]              ['Yes', 'No', 'Yes']   \n1         [36319.38, 19292.36, 35040.03]              ['Yes', 'No', 'Yes']   \n2         [36319.38, 19292.36, 35040.03]              ['Yes', 'No', 'Yes']   \n3  [85.3, 992.05, 51.7, 2565.65, 190.56]  ['No', 'No', 'Yes', 'Yes', 'No']   \n4  [85.3, 992.05, 51.7, 2565.65, 190.56]  ['No', 'No', 'Yes', 'Yes', 'No']   \n\n                        Discounted_Prices  Total_Amount Payment_Method  ...  \\\n0          [32687.44, 19292.36, 29784.03]     451907.80            UPI  ...   \n1          [32687.44, 19292.36, 29784.03]     451907.80            UPI  ...   \n2          [32687.44, 19292.36, 29784.03]     451907.80            UPI  ...   \n3  [85.3, 992.05, 41.36, 2437.37, 190.56]      18269.36           Cash  ...   \n4  [85.3, 992.05, 41.36, 2437.37, 190.56]      18269.36           Cash  ...   \n\n    ID Department Code Department  Product Code Product Name  Brand Code  \\\n0  NaN             NaN        NaN           NaN          NaN         NaN   \n1  NaN             NaN        NaN           NaN          NaN         NaN   \n2  NaN             NaN        NaN           NaN          NaN         NaN   \n3  NaN             NaN        NaN           NaN          NaN         NaN   \n4  NaN             NaN        NaN           NaN          NaN         NaN   \n\n  Brand Name  Expiry Month Expiry Year        Date  \n0        NaN           NaN         NaN  2024-07-06  \n1        NaN           NaN         NaN  2024-07-06  \n2        NaN           NaN         NaN  2024-07-06  \n3        NaN           NaN         NaN  2022-11-25  \n4        NaN           NaN         NaN  2022-11-25  \n\n[5 rows x 22 columns]\n✅ Forecasting complete. Results saved to product_forecasts.csv and brand_forecasts.csv\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/pandas/io/formats/format.py:1458: RuntimeWarning: invalid value encountered in greater\n  has_large_values = (abs_vals > 1e6).any()\n/usr/local/lib/python3.11/dist-packages/pandas/io/formats/format.py:1459: RuntimeWarning: invalid value encountered in less\n  has_small_values = ((abs_vals < 10 ** (-self.digits)) & (abs_vals > 0)).any()\n/usr/local/lib/python3.11/dist-packages/pandas/io/formats/format.py:1459: RuntimeWarning: invalid value encountered in greater\n  has_small_values = ((abs_vals < 10 ** (-self.digits)) & (abs_vals > 0)).any()\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.preprocessing import MinMaxScaler\nfrom joblib import Parallel, delayed\nimport ast\n\n# Use a GPU if available\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# ----------------------------\n# 1. Load Data\n# ----------------------------\n# Load the bills and products datasets\nbills_df = pd.read_csv(\"/kaggle/input/sales-prediction/bills (1).csv\")\nproducts_df = pd.read_csv(\"/kaggle/input/sales-prediction/products.csv\")\n\n# ----------------------------\n# 2. Preprocess Bills Data\n# ----------------------------\n# Clean column names by stripping whitespace\nbills_df.columns = bills_df.columns.str.strip()\n\n# Convert Product_IDs and Quantities from string representations to lists\nbills_df[\"Product_IDs\"] = bills_df[\"Product_IDs\"].astype(str)\nbills_df[\"Quantities\"] = bills_df[\"Quantities\"].apply(ast.literal_eval)\n\n# Explode the DataFrame to have one row per product in each bill\nbills_df = bills_df.assign(ProductID=bills_df[\"Product_IDs\"].str.split(\",\")).explode(\"ProductID\")\nbills_df = bills_df.explode([\"Quantities\"])\n\n# Clean up ProductID and set data types\nbills_df[\"ProductID\"] = bills_df[\"ProductID\"].astype(str).str.strip()\nbills_df[\"Quantity_Sold\"] = bills_df[\"Quantities\"].astype(int)\nbills_df[\"Date\"] = pd.to_datetime(bills_df[\"Date_Time\"]).dt.date\n\n# ----------------------------\n# 3. Preprocess Products Data\n# ----------------------------\n# Rename columns for consistency\nproducts_df.rename(columns={\n    'Product Code': 'ProductID',\n    'Product Name': 'Product_Name',\n    'Brand Name': 'Brand Name',\n    'Price (INR)': 'Price'\n}, inplace=True)\nproducts_df[\"ProductID\"] = products_df[\"ProductID\"].astype(str).str.strip()\n\n# ----------------------------\n# 4. Merge DataFrames\n# ----------------------------\n# Merge the bills and products dataframes\nmerged_df = bills_df.merge(products_df[['ProductID', 'Product_Name', 'Brand Name']], on='ProductID', how='left')\n# Fill any missing brand names with 'Unknown'\nmerged_df['Brand Name'] = merged_df['Brand Name'].fillna('Unknown')\n\n# ----------------------------\n# 5. Create Daily Sales Summary\n# ----------------------------\n# Group by Date, ProductID, and Brand Name to get total quantity sold each day\nsummary_daily_df = (\n    merged_df.groupby([\"Date\", \"ProductID\", \"Brand Name\"])[\"Quantity_Sold\"]\n    .sum()\n    .reset_index()\n    .rename(columns={\"Quantity_Sold\": \"Total_Quantity_Sold\"})\n)\n\n# ----------------------------\n# 6. PyTorch Dataset for Time Series\n# ----------------------------\nclass TimeSeriesDataset(Dataset):\n    def __init__(self, series, seq_len):\n        series = np.array(series, dtype=float)\n        # Pad the series if it's shorter than the sequence length\n        if len(series) <= seq_len:\n            series = np.pad(series, (0, seq_len + 1 - len(series)), 'constant', constant_values=0.1)\n        self.series = series\n        self.seq_len = seq_len\n\n    def __len__(self):\n        return max(len(self.series) - self.seq_len, 1)\n\n    def __getitem__(self, idx):\n        # Ensure the index is within bounds\n        idx = min(idx, len(self.series) - self.seq_len - 1)\n        return (\n            torch.tensor(self.series[idx:idx+self.seq_len], dtype=torch.float32).unsqueeze(-1),\n            torch.tensor(self.series[idx+self.seq_len], dtype=torch.float32)\n        )\n\n# ----------------------------\n# 7. GRU with Attention Model\n# ----------------------------\nclass GRUAttention(nn.Module):\n    def __init__(self, input_dim=1, hidden_dim=32, num_layers=1):\n        super().__init__()\n        self.gru = nn.GRU(input_dim, hidden_dim, num_layers, batch_first=True)\n        self.attn_fc = nn.Linear(hidden_dim, 1)\n        self.fc = nn.Linear(hidden_dim, 1)\n\n    def forward(self, x):\n        gru_out, _ = self.gru(x)\n        attn_weights = torch.softmax(self.attn_fc(gru_out), dim=1)\n        context_vector = torch.sum(attn_weights * gru_out, dim=1)\n        output = self.fc(context_vector)\n        return output\n\n# ----------------------------\n# 8. Training and Forecasting Function\n# ----------------------------\ndef train_and_forecast(series, future=30, seq_len=30, epochs=50, batch_size=64):\n    # If there's no sales data, predict zeros\n    if np.sum(series) == 0 or len(series) == 0:\n        return np.zeros(future).tolist()\n\n    # Scale the data\n    scaler = MinMaxScaler()\n    series_scaled = scaler.fit_transform(series.reshape(-1, 1)).flatten()\n\n    # Create dataset and dataloader\n    dataset = TimeSeriesDataset(series_scaled, seq_len)\n    batch_size = min(batch_size, len(dataset))\n    shuffle = len(dataset) > 1\n    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle)\n\n    # Initialize model, loss function, and optimizer\n    model = GRUAttention().to(device)\n    loss_fn = nn.MSELoss()\n    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n\n    # Training loop with early stopping\n    best_loss = float('inf')\n    patience, no_improve = 5, 0\n    for epoch in range(epochs):\n        epoch_loss = 0\n        for xb, yb in dataloader:\n            xb, yb = xb.to(device), yb.to(device)\n            pred = model(xb).squeeze(-1)\n            loss = loss_fn(pred, yb)\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n            epoch_loss += loss.item()\n\n        avg_loss = epoch_loss / len(dataloader)\n        if avg_loss < best_loss:\n            best_loss = avg_loss\n            no_improve = 0\n        else:\n            no_improve += 1\n        if no_improve >= patience:\n            break\n\n    # Forecasting\n    model.eval()\n    input_seq = torch.tensor(series_scaled[-seq_len:], dtype=torch.float32).unsqueeze(0).unsqueeze(-1).to(device)\n    preds = []\n    with torch.no_grad():\n        for _ in range(future):\n            out = model(input_seq).item()\n            preds.append(out)\n            new_input = torch.cat([input_seq[:, 1:, :], torch.tensor([[[out]]], dtype=torch.float32).to(device)], dim=1)\n            input_seq = new_input\n\n    # Inverse transform the predictions to get the actual scale\n    preds = scaler.inverse_transform(np.array(preds).reshape(-1, 1)).flatten()\n    preds = np.maximum(preds, 0)\n    return np.round(preds).tolist()\n\n# ----------------------------\n# 9. Generate Forecasts per Product\n# ----------------------------\ndef process_product_all(product_id, df, seq_len=30):\n    group = df[df[\"ProductID\"] == product_id].sort_values(\"Date\")\n    series = group[\"Total_Quantity_Sold\"].values\n    # Pad the series if it's too short\n    if len(series) < 2:\n        series = np.pad(series, (0, 2 - len(series)), 'constant', constant_values=0.1)\n\n    # Generate daily, weekly, and monthly forecasts\n    return {\n        \"ProductID\": product_id,\n        \"Brand Name\": group[\"Brand Name\"].iloc[0],\n        \"Daily_Forecast\": train_and_forecast(series, future=30, seq_len=seq_len),\n        \"Weekly_Forecast\": train_and_forecast(series, future=7, seq_len=seq_len),\n        \"Monthly_Forecast\": train_and_forecast(series, future=30, seq_len=seq_len)\n    }\n\n# Run the forecasting in parallel for speed\nproduct_ids = summary_daily_df[\"ProductID\"].unique()\nresults = Parallel(n_jobs=-1)(delayed(process_product_all)(pid, summary_daily_df) for pid in product_ids)\nresults = [r for r in results if r is not None]\n\n# Save the product-level forecasts\nforecast_df = pd.DataFrame(results)\nforecast_df.to_csv(\"product_forecasts_all.csv\", index=False)\nprint(\"✅ Product-level forecasts saved to product_forecasts_all.csv\")\n\n# ----------------------------\n# 10. Aggregate Forecasts by Brand\n# ----------------------------\ndef aggregate_brand_forecast(forecast_df, col_name, horizon_name):\n    expanded = []\n    for _, row in forecast_df.iterrows():\n        # Handle cases where brand name might be null\n        brand_col = row['Brand Name'] if pd.notnull(row['Brand Name']) else 'Unknown'\n        for i, val in enumerate(row[col_name]):\n            expanded.append({\"Brand\": brand_col, horizon_name: i + 1, \"Forecast_Qty\": val})\n\n    expanded_df = pd.DataFrame(expanded)\n    # Group by brand and the time horizon (Day, Week, Month)\n    brand_forecast = expanded_df.groupby([\"Brand\", horizon_name], as_index=False)[\"Forecast_Qty\"].sum()\n    brand_forecast[\"Forecast_Qty\"] = brand_forecast[\"Forecast_Qty\"].round()\n    return brand_forecast\n\n# Generate and save brand-level forecasts\nbrand_daily_forecast = aggregate_brand_forecast(forecast_df, \"Daily_Forecast\", \"Day\")\nbrand_weekly_forecast = aggregate_brand_forecast(forecast_df, \"Weekly_Forecast\", \"Week\")\nbrand_monthly_forecast = aggregate_brand_forecast(forecast_df, \"Monthly_Forecast\", \"Month\")\n\nbrand_daily_forecast.to_csv(\"brand_daily_forecast.csv\", index=False)\nbrand_weekly_forecast.to_csv(\"brand_weekly_forecast.csv\", index=False)\nbrand_monthly_forecast.to_csv(\"brand_monthly_forecast.csv\", index=False)\nprint(\"✅ Brand-level forecasts saved to brand_daily_forecast.csv, brand_weekly_forecast.csv, and brand_monthly_forecast.csv\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-13T06:49:40.009621Z","iopub.execute_input":"2025-09-13T06:49:40.009905Z","iopub.status.idle":"2025-09-13T07:14:27.042926Z","shell.execute_reply.started":"2025-09-13T06:49:40.009884Z","shell.execute_reply":"2025-09-13T07:14:27.042254Z"}},"outputs":[{"name":"stdout","text":"✅ Product-level forecasts saved to product_forecasts_all.csv\n✅ Brand-level forecasts saved to brand_daily_forecast.csv, brand_weekly_forecast.csv, and brand_monthly_forecast.csv\n","output_type":"stream"}],"execution_count":42},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.model_selection import train_test_split\nfrom joblib import Parallel, delayed\nimport ast\n\n# Use a GPU if available\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# ----------------------------\n# 1. Load Data\n# ----------------------------\n# Load the bills and products datasets\nbills_df = pd.read_csv(\"/kaggle/input/sales-prediction/bills (1).csv\")\nproducts_df = pd.read_csv(\"/kaggle/input/sales-prediction/products.csv\")\n\n# ----------------------------\n# 2. Preprocess Bills Data\n# ----------------------------\n# Clean column names by stripping whitespace\nbills_df.columns = bills_df.columns.str.strip()\n\n# Convert Product_IDs and Quantities from string representations to lists\nbills_df[\"Product_IDs\"] = bills_df[\"Product_IDs\"].apply(ast.literal_eval)\nbills_df[\"Quantities\"] = bills_df[\"Quantities\"].apply(ast.literal_eval)\n\n\n# Explode the DataFrame to have one row per product-quantity pair\nbills_df = bills_df.explode([\"Product_IDs\", \"Quantities\"])\nbills_df.rename(columns={\"Product_IDs\": \"ProductID\", \"Quantities\": \"Quantity_Sold\"}, inplace=True)\n\n\nbills_df[\"ProductID\"] = bills_df[\"ProductID\"].astype(str).str.strip()\nbills_df[\"Quantity_Sold\"] = bills_df[\"Quantity_Sold\"].astype(int)\n\nbills_df[\"Date\"] = pd.to_datetime(bills_df[\"Date_Time\"]).dt.date\n\n# ----------------------------\n# 3. Preprocess Products Data\n# ----------------------------\n# Clean up column names and ProductID\nproducts_df.columns = products_df.columns.str.strip()\nproducts_df.rename(columns={'Product Code': 'ProductID'}, inplace=True)\nproducts_df[\"ProductID\"] = products_df[\"ProductID\"].astype(str).str.strip()\n\n\n# ----------------------------\n# 4. Merge DataFrames\n# ----------------------------\n# Merge the bills and products dataframes\nmerged_df = bills_df.merge(products_df[['ProductID', 'Brand Name']], on='ProductID', how='left')\nmerged_df['Brand Name'] = merged_df['Brand Name'].fillna('Unknown')\n\n# ----------------------------\n# 5. Create Daily Sales Summary\n# ----------------------------\nsummary_daily_df = (\n    merged_df.groupby([\"Date\", \"ProductID\", \"Brand Name\"])[\"Quantity_Sold\"]\n    .sum()\n    .reset_index()\n    .rename(columns={\"Quantity_Sold\": \"Total_Quantity_Sold\"})\n)\n\n# ----------------------------\n# 6. PyTorch Dataset for Time Series\n# ----------------------------\nclass TimeSeriesDataset(Dataset):\n    def __init__(self, series, seq_len):\n        self.series = series\n        self.seq_len = seq_len\n\n    def __len__(self):\n        return len(self.series) - self.seq_len\n\n    def __getitem__(self, idx):\n        return (\n            torch.tensor(self.series[idx:idx+self.seq_len], dtype=torch.float32).unsqueeze(-1),\n            torch.tensor(self.series[idx+self.seq_len], dtype=torch.float32)\n        )\n\n# ----------------------------\n# 7. GRU with Attention Model\n# ----------------------------\nclass GRUAttention(nn.Module):\n    def __init__(self, input_dim=1, hidden_dim=32, num_layers=1):\n        super().__init__()\n        self.gru = nn.GRU(input_dim, hidden_dim, num_layers, batch_first=True)\n        self.attn_fc = nn.Linear(hidden_dim, 1)\n        self.fc = nn.Linear(hidden_dim, 1)\n\n    def forward(self, x):\n        gru_out, _ = self.gru(x)\n        attn_weights = torch.softmax(self.attn_fc(gru_out), dim=1)\n        context_vector = torch.sum(attn_weights * gru_out, dim=1)\n        output = self.fc(context_vector)\n        return output\n\n# ----------------------------\n# 8. Training, Forecasting, and Accuracy Check\n# ----------------------------\ndef train_and_forecast_with_accuracy(series, future=30, seq_len=30, epochs=50, batch_size=64):\n    if len(series) < seq_len + 10: # Ensure enough data for a meaningful split\n        return np.zeros(future).tolist(), 0.0\n\n    # Train-test split\n    train_series, test_series = train_test_split(series, test_size=0.2, shuffle=False)\n\n    scaler = MinMaxScaler()\n    train_scaled = scaler.fit_transform(train_series.reshape(-1, 1)).flatten()\n\n    train_dataset = TimeSeriesDataset(train_scaled, seq_len)\n    if len(train_dataset) == 0:\n        return np.zeros(future).tolist(), 0.0\n    \n    train_loader = DataLoader(train_dataset, batch_size=min(batch_size, len(train_dataset)), shuffle=True)\n    \n    model = GRUAttention().to(device)\n    loss_fn = nn.MSELoss()\n    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n\n    # Training\n    for epoch in range(epochs):\n        for xb, yb in train_loader:\n            xb, yb = xb.to(device), yb.to(device)\n            pred = model(xb).squeeze(-1)\n            loss = loss_fn(pred, yb)\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n\n    # Accuracy check on the test set\n    model.eval()\n    test_inputs = scaler.transform(series[-seq_len - len(test_series):-len(test_series)].reshape(-1, 1)).flatten()\n    test_actuals = test_series\n    \n    test_preds = []\n    with torch.no_grad():\n        current_seq = torch.tensor(test_inputs, dtype=torch.float32).unsqueeze(0).unsqueeze(-1).to(device)\n        for _ in range(len(test_actuals)):\n            out = model(current_seq).item()\n            test_preds.append(out)\n            # Update sequence for next prediction\n            new_seq_val = torch.tensor([[[out]]], dtype=torch.float32).to(device)\n            current_seq = torch.cat([current_seq[:, 1:, :], new_seq_val], dim=1)\n\n    test_preds = scaler.inverse_transform(np.array(test_preds).reshape(-1, 1)).flatten()\n    \n    # Calculate MAPE\n    mape = np.mean(np.abs((test_actuals - test_preds) / np.maximum(test_actuals, 1))) * 100\n    accuracy = 100 - mape\n\n    # Forecasting for the future\n    model.eval()\n    input_seq = torch.tensor(scaler.transform(series[-seq_len:].reshape(-1, 1)).flatten(), dtype=torch.float32).unsqueeze(0).unsqueeze(-1).to(device)\n    future_preds = []\n    with torch.no_grad():\n        for _ in range(future):\n            out = model(input_seq).item()\n            future_preds.append(out)\n            new_input = torch.cat([input_seq[:, 1:, :], torch.tensor([[[out]]], dtype=torch.float32).to(device)], dim=1)\n            input_seq = new_input\n            \n    future_preds = scaler.inverse_transform(np.array(future_preds).reshape(-1, 1)).flatten()\n    future_preds = np.maximum(future_preds, 0)\n    \n    return np.round(future_preds).tolist(), accuracy\n\n# ----------------------------\n# 9. Generate Forecasts per Product\n# ----------------------------\ndef process_product_all(product_id, df, seq_len=30):\n    group = df[df[\"ProductID\"] == product_id].sort_values(\"Date\")\n    series = group[\"Total_Quantity_Sold\"].values\n\n    if len(series) < 2:\n        return None\n\n    daily_forecast, accuracy = train_and_forecast_with_accuracy(series, future=30, seq_len=seq_len)\n    weekly_forecast, _ = train_and_forecast_with_accuracy(series, future=7, seq_len=seq_len)\n    monthly_forecast, _ = train_and_forecast_with_accuracy(series, future=30, seq_len=seq_len)\n    \n    print(f\"ProductID: {product_id}, Forecast Accuracy: {accuracy:.2f}%\")\n    \n    return {\n        \"ProductID\": product_id,\n        \"Brand Name\": group[\"Brand Name\"].iloc[0],\n        \"Daily_Forecast\": daily_forecast,\n        \"Weekly_Forecast\": weekly_forecast,\n        \"Monthly_Forecast\": monthly_forecast,\n        \"Accuracy (%)\": accuracy\n    }\n\n# Run the forecasting in parallel\nproduct_ids = summary_daily_df[\"ProductID\"].unique()\nresults = Parallel(n_jobs=-1)(delayed(process_product_all)(pid, summary_daily_df) for pid in product_ids)\nresults = [r for r in results if r is not None]\n\n# Save the product-level forecasts\nforecast_df = pd.DataFrame(results)\nforecast_df.to_csv(\"product_forecasts_with_accuracy.csv\", index=False)\nprint(\"✅ Product-level forecasts saved to product_forecasts_with_accuracy.csv\")\n\n# ----------------------------\n# 10. Aggregate Forecasts by Brand\n# ----------------------------\ndef aggregate_brand_forecast(forecast_df, col_name, horizon_name):\n    expanded = []\n    for _, row in forecast_df.iterrows():\n        brand_col = row['Brand Name'] if pd.notnull(row['Brand Name']) else 'Unknown'\n        for i, val in enumerate(row[col_name]):\n            expanded.append({\"Brand\": brand_col, horizon_name: i + 1, \"Forecast_Qty\": val})\n\n    expanded_df = pd.DataFrame(expanded)\n    brand_forecast = expanded_df.groupby([\"Brand\", horizon_name], as_index=False)[\"Forecast_Qty\"].sum()\n    brand_forecast[\"Forecast_Qty\"] = brand_forecast[\"Forecast_Qty\"].round()\n    return brand_forecast\n\n# Generate and save brand-level forecasts\nbrand_daily_forecast = aggregate_brand_forecast(forecast_df, \"Daily_Forecast\", \"Day\")\nbrand_weekly_forecast = aggregate_brand_forecast(forecast_df, \"Weekly_Forecast\", \"Week\")\nbrand_monthly_forecast = aggregate_brand_forecast(forecast_df, \"Monthly_Forecast\", \"Month\")\n\nbrand_daily_forecast.to_csv(\"brand_daily_forecast.csv\", index=False)\nbrand_weekly_forecast.to_csv(\"brand_weekly_forecast.csv\", index=False)\nbrand_monthly_forecast.to_csv(\"brand_monthly_forecast.csv\", index=False)\nprint(\"✅ Brand-level forecasts saved to brand_daily_forecast.csv, brand_weekly_forecast.csv, and brand_monthly_forecast.csv\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-13T07:27:30.888818Z","iopub.execute_input":"2025-09-13T07:27:30.889089Z","iopub.status.idle":"2025-09-13T08:02:32.803225Z","shell.execute_reply.started":"2025-09-13T07:27:30.889069Z","shell.execute_reply":"2025-09-13T08:02:32.802608Z"}},"outputs":[{"name":"stdout","text":"ProductID: PVSL0101031125, Forecast Accuracy: 16.96%\nProductID: PVSL0102020126, Forecast Accuracy: 8.40%\nProductID: PVSL0103011025, Forecast Accuracy: -18.14%\nProductID: PVSL0103031025, Forecast Accuracy: 3.73%\nProductID: PVSL0104050126, Forecast Accuracy: 1.92%\nProductID: PVSL0105051025, Forecast Accuracy: -32.72%\nProductID: PVSL0106050126, Forecast Accuracy: -19.32%\nProductID: PVSL0107051025, Forecast Accuracy: -11.33%\nProductID: PVSL0108051025, Forecast Accuracy: -15.43%\nProductID: PVSL0109050126, Forecast Accuracy: -2.23%\nProductID: PVSL0111031025, Forecast Accuracy: 2.43%\nProductID: PVSL0112020925, Forecast Accuracy: -17.69%\nProductID: PVSL0112060925, Forecast Accuracy: -53.94%\nProductID: PVSL0114021125, Forecast Accuracy: 2.47%\nProductID: PVSL0115011125, Forecast Accuracy: -9.41%\nProductID: PVSL0116021125, Forecast Accuracy: 8.76%\nProductID: PVSL0117050126, Forecast Accuracy: -13.58%\nProductID: PVSL0119010925, Forecast Accuracy: 16.87%\nProductID: PVSL0119050925, Forecast Accuracy: -3.33%\nProductID: PVSL0121020925, Forecast Accuracy: -3.35%\nProductID: PVSL0122021125, Forecast Accuracy: -5.95%\nProductID: PVSL0123030925, Forecast Accuracy: -3.51%\nProductID: PVSL0125021125, Forecast Accuracy: 18.42%\nProductID: PVSL0125061125, Forecast Accuracy: -53.65%\nProductID: PVSL0126051025, Forecast Accuracy: -3.04%\nProductID: PVSL0127071025, Forecast Accuracy: 7.58%\nProductID: PVSL0128051225, Forecast Accuracy: -11.09%\nProductID: PVSL0130031025, Forecast Accuracy: -6.69%\nProductID: PVSL0131041025, Forecast Accuracy: -26.64%\nProductID: PVSL0132030126, Forecast Accuracy: 5.03%\nProductID: PVSL0135010925, Forecast Accuracy: 8.23%\nProductID: PVSL0136040925, Forecast Accuracy: 2.25%\nProductID: PVSL0138031225, Forecast Accuracy: -12.30%\nProductID: PVSL0139040126, Forecast Accuracy: -23.35%\nProductID: PVSL0140050925, Forecast Accuracy: -17.94%\nProductID: PVSL0141061125, Forecast Accuracy: -27.69%\nProductID: PVSL0142060126, Forecast Accuracy: -7.67%\nProductID: PVSL0202041125, Forecast Accuracy: 13.11%\nProductID: PVSL0203020126, Forecast Accuracy: -8.43%\nProductID: PVSL0204051225, Forecast Accuracy: -12.93%\nProductID: PVSL0205031125, Forecast Accuracy: 7.26%\nProductID: PVSL0206031225, Forecast Accuracy: -22.86%\nProductID: PVSL0207041125, Forecast Accuracy: -22.80%\nProductID: PVSL0208041025, Forecast Accuracy: -1.01%\nProductID: PVSL0210021125, Forecast Accuracy: -6.13%\nProductID: PVSL0211021025, Forecast Accuracy: 5.70%\nProductID: PVSL0212020126, Forecast Accuracy: -19.95%\nProductID: PVSL0212060126, Forecast Accuracy: 4.60%\nProductID: PVSL0214031125, Forecast Accuracy: 3.87%\nProductID: PVSL0215020126, Forecast Accuracy: -18.53%\nProductID: PVSL0215060126, Forecast Accuracy: 2.93%\nProductID: PVSL0216060126, Forecast Accuracy: 7.02%\nProductID: PVSL0218011125, Forecast Accuracy: 33.35%\nProductID: PVSL0218061125, Forecast Accuracy: -22.99%\nProductID: PVSL0219041225, Forecast Accuracy: -2.38%\nProductID: PVSL0220061025, Forecast Accuracy: -3.31%\nProductID: PVSL0302041125, Forecast Accuracy: -6.03%\nProductID: PVSL0303041125, Forecast Accuracy: 15.94%\nProductID: PVSL0304021025, Forecast Accuracy: -11.97%\nProductID: PVSL0305030925, Forecast Accuracy: -14.86%\nProductID: PVSL0306011025, Forecast Accuracy: -2.38%\nProductID: PVSL0307021225, Forecast Accuracy: -0.27%\nProductID: PVSL0308030925, Forecast Accuracy: -19.92%\nProductID: PVSL0309021125, Forecast Accuracy: -5.13%\nProductID: PVSL0309061125, Forecast Accuracy: -31.28%\nProductID: PVSL0310060126, Forecast Accuracy: -18.58%\nProductID: PVSL0312010126, Forecast Accuracy: -7.54%\nProductID: PVSL0313011025, Forecast Accuracy: 5.84%\nProductID: PVSL0314060925, Forecast Accuracy: -26.12%\nProductID: PVSL0316010925, Forecast Accuracy: 2.98%\nProductID: PVSL0316060925, Forecast Accuracy: -0.09%\nProductID: PVSL0317061125, Forecast Accuracy: -4.37%\nProductID: PVSL0318040126, Forecast Accuracy: -25.90%\nProductID: PVSL0319041225, Forecast Accuracy: -12.41%\nProductID: PVSL0401040925, Forecast Accuracy: -12.86%\nProductID: PVSL0402021225, Forecast Accuracy: -3.62%\nProductID: PVSL0403010126, Forecast Accuracy: -23.29%\nProductID: PVSL0403060126, Forecast Accuracy: -19.50%\nProductID: PVSL0404061225, Forecast Accuracy: -16.34%\nProductID: PVSL0405060126, Forecast Accuracy: 19.05%\nProductID: PVSL0406041125, Forecast Accuracy: 20.03%\nProductID: PVSL0407030126, Forecast Accuracy: -6.31%\nProductID: PVSL0408010925, Forecast Accuracy: 16.13%\nProductID: PVSL0409020925, Forecast Accuracy: 2.21%\nProductID: PVSL0409050925, Forecast Accuracy: -16.76%\nProductID: PVSL0410061125, Forecast Accuracy: -12.25%\nProductID: PVSL0413051225, Forecast Accuracy: -5.26%\nProductID: PVSL0414061125, Forecast Accuracy: -3.07%\nProductID: PVSL0415050126, Forecast Accuracy: -24.39%\nProductID: PVSL0416031025, Forecast Accuracy: 19.75%\nProductID: PVSL0417021225, Forecast Accuracy: 3.40%\nProductID: PVSL0417061225, Forecast Accuracy: -4.59%\nProductID: PVSL0418061125, Forecast Accuracy: -26.57%\nProductID: PVSL0419050126, Forecast Accuracy: 9.28%\nProductID: PVSL0501040925, Forecast Accuracy: -7.92%\nProductID: PVSL0502040925, Forecast Accuracy: -27.53%\nProductID: PVSL0504011125, Forecast Accuracy: 4.17%\nProductID: PVSL0504051125, Forecast Accuracy: -25.97%\nProductID: PVSL0505051125, Forecast Accuracy: -7.35%\nProductID: PVSL0506051225, Forecast Accuracy: -31.44%\nProductID: PVSL0507040925, Forecast Accuracy: -25.78%\nProductID: PVSL0508020925, Forecast Accuracy: -16.99%\nProductID: PVSL0509041125, Forecast Accuracy: 9.60%\nProductID: PVSL0510061025, Forecast Accuracy: -19.14%\nProductID: PVSL0511050925, Forecast Accuracy: -41.81%\nProductID: PVSL0512031125, Forecast Accuracy: 0.55%\nProductID: PVSL0513021225, Forecast Accuracy: -8.50%\nProductID: PVSL0514020126, Forecast Accuracy: 3.33%\nProductID: PVSL0515011225, Forecast Accuracy: -7.41%\nProductID: PVSL0516021025, Forecast Accuracy: -15.88%\nProductID: PVSL0517021225, Forecast Accuracy: 9.87%\nProductID: PVSL0518031025, Forecast Accuracy: -26.21%\nProductID: PVSL0519061125, Forecast Accuracy: -25.40%\nProductID: PVSL0601031125, Forecast Accuracy: -12.61%\nProductID: PVSL0602041125, Forecast Accuracy: -3.42%\nProductID: PVSL0604011025, Forecast Accuracy: -2.14%\nProductID: PVSL0605031125, Forecast Accuracy: 3.72%\nProductID: PVSL0606051225, Forecast Accuracy: -8.86%\nProductID: PVSL0607031225, Forecast Accuracy: 19.04%\nProductID: PVSL0608021025, Forecast Accuracy: -19.92%\nProductID: PVSL0609041125, Forecast Accuracy: -8.63%\nProductID: PVSL0610030925, Forecast Accuracy: 21.28%\nProductID: PVSL0702011225, Forecast Accuracy: 9.75%\nProductID: PVSL0703041025, Forecast Accuracy: 3.34%\nProductID: PVSL0704061125, Forecast Accuracy: -12.24%\nProductID: PVSL0706041225, Forecast Accuracy: 12.03%\nProductID: PVSL0707061125, Forecast Accuracy: -4.20%\nProductID: PVSL0708061225, Forecast Accuracy: -14.28%\nProductID: PVSL0710011225, Forecast Accuracy: 1.37%\nProductID: PVSL0801010126, Forecast Accuracy: 9.52%\nProductID: PVSL0801060126, Forecast Accuracy: -11.11%\nProductID: PVSL0802060925, Forecast Accuracy: 1.02%\nProductID: PVSL0804041025, Forecast Accuracy: -13.79%\nProductID: PVSL0805041025, Forecast Accuracy: -17.03%\nProductID: PVSL0806021025, Forecast Accuracy: 9.48%\nProductID: PVSL0807020126, Forecast Accuracy: -43.41%\nProductID: PVSL0808011125, Forecast Accuracy: -31.19%\nProductID: PVSL0808061125, Forecast Accuracy: -15.20%\nProductID: PVSL0809040925, Forecast Accuracy: -4.20%\nProductID: PVSL0901011125, Forecast Accuracy: -5.08%\nProductID: PVSL0901030126, Forecast Accuracy: -5.39%\nProductID: PVSL0901060925, Forecast Accuracy: -4.13%\nProductID: PVSL0902021025, Forecast Accuracy: -24.75%\nProductID: PVSL0902041025, Forecast Accuracy: -14.90%\nProductID: PVSL0902050925, Forecast Accuracy: -38.36%\nProductID: PVSL0902060925, Forecast Accuracy: -11.89%\nProductID: PVSL0903021025, Forecast Accuracy: -34.71%\nProductID: PVSL0903040126, Forecast Accuracy: -32.83%\nProductID: PVSL0903050126, Forecast Accuracy: -24.92%\nProductID: PVSL0903061025, Forecast Accuracy: -0.94%\nProductID: PVSL0904021225, Forecast Accuracy: -27.02%\nProductID: PVSL0904041125, Forecast Accuracy: -2.72%\nProductID: PVSL0904060126, Forecast Accuracy: -14.13%\nProductID: PVSL0905010925, Forecast Accuracy: 15.65%ProductID: PVSL0101011125, Forecast Accuracy: 15.58%\nProductID: PVSL0101051125, Forecast Accuracy: 11.65%\nProductID: PVSL0102040126, Forecast Accuracy: 24.64%\nProductID: PVSL0103051025, Forecast Accuracy: -14.32%\nProductID: PVSL0105021025, Forecast Accuracy: 10.97%\nProductID: PVSL0106020126, Forecast Accuracy: -0.99%\nProductID: PVSL0107021025, Forecast Accuracy: 4.81%\nProductID: PVSL0108021025, Forecast Accuracy: -54.11%\nProductID: PVSL0109030126, Forecast Accuracy: 9.00%\nProductID: PVSL0110040126, Forecast Accuracy: 1.31%\nProductID: PVSL0111051025, Forecast Accuracy: 13.94%\nProductID: PVSL0112040925, Forecast Accuracy: 17.90%\nProductID: PVSL0113051125, Forecast Accuracy: 5.60%\nProductID: PVSL0114051125, Forecast Accuracy: -17.74%\nProductID: PVSL0115041125, Forecast Accuracy: 3.33%\nProductID: PVSL0116041125, Forecast Accuracy: -20.30%\nProductID: PVSL0118040126, Forecast Accuracy: -10.04%\nProductID: PVSL0119030925, Forecast Accuracy: -36.69%\nProductID: PVSL0120040126, Forecast Accuracy: -6.02%\nProductID: PVSL0121050925, Forecast Accuracy: -17.11%\nProductID: PVSL0122051125, Forecast Accuracy: -6.20%\nProductID: PVSL0125011125, Forecast Accuracy: 5.68%\nProductID: PVSL0125051125, Forecast Accuracy: -2.79%\nProductID: PVSL0126041025, Forecast Accuracy: -6.40%\nProductID: PVSL0127041025, Forecast Accuracy: -3.34%\nProductID: PVSL0128041225, Forecast Accuracy: -30.41%\nProductID: PVSL0130021025, Forecast Accuracy: -0.49%\nProductID: PVSL0131031025, Forecast Accuracy: -7.38%\nProductID: PVSL0132020126, Forecast Accuracy: -16.13%\nProductID: PVSL0133031025, Forecast Accuracy: -5.60%\nProductID: PVSL0135040925, Forecast Accuracy: -2.12%\nProductID: PVSL0138011225, Forecast Accuracy: 0.60%\nProductID: PVSL0138051225, Forecast Accuracy: -19.39%\nProductID: PVSL0140030925, Forecast Accuracy: -18.50%\nProductID: PVSL0141011125, Forecast Accuracy: -1.21%\nProductID: PVSL0142030126, Forecast Accuracy: -33.32%\nProductID: PVSL0201041025, Forecast Accuracy: -14.45%\nProductID: PVSL0202061125, Forecast Accuracy: 23.84%\nProductID: PVSL0203060126, Forecast Accuracy: -13.46%\nProductID: PVSL0205011125, Forecast Accuracy: -22.41%\nProductID: PVSL0205051125, Forecast Accuracy: -33.56%\nProductID: PVSL0206061225, Forecast Accuracy: -7.97%\nProductID: PVSL0208021025, Forecast Accuracy: -4.97%\nProductID: PVSL0209061225, Forecast Accuracy: -16.00%\nProductID: PVSL0210041125, Forecast Accuracy: 25.83%\nProductID: PVSL0211051025, Forecast Accuracy: -23.57%\nProductID: PVSL0212040126, Forecast Accuracy: -24.68%\nProductID: PVSL0213041225, Forecast Accuracy: -3.96%\nProductID: PVSL0214061125, Forecast Accuracy: 10.51%\nProductID: PVSL0215040126, Forecast Accuracy: -35.46%\nProductID: PVSL0216020126, Forecast Accuracy: 10.33%\nProductID: PVSL0217051225, Forecast Accuracy: -10.15%\nProductID: PVSL0218031125, Forecast Accuracy: -11.64%\nProductID: PVSL0219021225, Forecast Accuracy: -27.81%\nProductID: PVSL0219061225, Forecast Accuracy: 12.73%\nProductID: PVSL0302021125, Forecast Accuracy: 2.27%\nProductID: PVSL0303011125, Forecast Accuracy: 10.76%\nProductID: PVSL0303061125, Forecast Accuracy: -41.64%\nProductID: PVSL0304051025, Forecast Accuracy: -19.19%\nProductID: PVSL0305050925, Forecast Accuracy: 0.26%\nProductID: PVSL0306061025, Forecast Accuracy: -15.72%\nProductID: PVSL0308010925, Forecast Accuracy: -7.02%\nProductID: PVSL0308050925, Forecast Accuracy: 16.71%\nProductID: PVSL0309051125, Forecast Accuracy: -33.42%\nProductID: PVSL0310040126, Forecast Accuracy: 14.18%\nProductID: PVSL0311061025, Forecast Accuracy: -53.32%\nProductID: PVSL0312050126, Forecast Accuracy: -5.76%\nProductID: PVSL0314010925, Forecast Accuracy: 15.29%\nProductID: PVSL0315031025, Forecast Accuracy: -16.41%\nProductID: PVSL0316040925, Forecast Accuracy: -3.22%\nProductID: PVSL0317041125, Forecast Accuracy: 23.42%\nProductID: PVSL0318020126, Forecast Accuracy: -13.03%\nProductID: PVSL0319011225, Forecast Accuracy: 7.43%\nProductID: PVSL0320041225, Forecast Accuracy: -16.72%\nProductID: PVSL0401060925, Forecast Accuracy: 27.51%\nProductID: PVSL0402041225, Forecast Accuracy: -29.20%\nProductID: PVSL0403040126, Forecast Accuracy: 10.42%\nProductID: PVSL0404041225, Forecast Accuracy: -22.96%\nProductID: PVSL0405020126, Forecast Accuracy: -30.23%\nProductID: PVSL0406021125, Forecast Accuracy: -8.26%\nProductID: PVSL0407010126, Forecast Accuracy: 31.71%\nProductID: PVSL0407050126, Forecast Accuracy: 0.66%\nProductID: PVSL0408050925, Forecast Accuracy: -22.33%\nProductID: PVSL0409030925, Forecast Accuracy: -19.68%\nProductID: PVSL0410021125, Forecast Accuracy: -18.24%\nProductID: PVSL0411061225, Forecast Accuracy: -21.68%\nProductID: PVSL0414021125, Forecast Accuracy: -3.82%\nProductID: PVSL0415010126, Forecast Accuracy: -29.91%\nProductID: PVSL0415060126, Forecast Accuracy: -31.35%\nProductID: PVSL0416041025, Forecast Accuracy: 16.19%\nProductID: PVSL0417031225, Forecast Accuracy: -18.92%\nProductID: PVSL0418031125, Forecast Accuracy: -11.35%\nProductID: PVSL0419010126, Forecast Accuracy: -20.06%\nProductID: PVSL0419060126, Forecast Accuracy: 13.10%\nProductID: PVSL0501050925, Forecast Accuracy: -18.75%\nProductID: PVSL0503021225, Forecast Accuracy: 7.46%\nProductID: PVSL0504031125, Forecast Accuracy: -19.07%\nProductID: PVSL0505021125, Forecast Accuracy: -33.30%\nProductID: PVSL0506011225, Forecast Accuracy: -27.24%\nProductID: PVSL0507010925, Forecast Accuracy: -30.73%\nProductID: PVSL0507060925, Forecast Accuracy: -9.57%\nProductID: PVSL0509011125, Forecast Accuracy: -7.78%\nProductID: PVSL0510021025, Forecast Accuracy: -7.23%\nProductID: PVSL0511030925, Forecast Accuracy: 29.49%\nProductID: PVSL0512021125, Forecast Accuracy: -7.19%\nProductID: PVSL0512061125, Forecast Accuracy: -2.89%\nProductID: PVSL0514010126, Forecast Accuracy: 11.92%\nProductID: PVSL0514060126, Forecast Accuracy: -3.56%\nProductID: PVSL0516011025, Forecast Accuracy: 16.14%\nProductID: PVSL0516061025, Forecast Accuracy: -14.84%\nProductID: PVSL0517061225, Forecast Accuracy: 25.24%\nProductID: PVSL0519051125, Forecast Accuracy: -16.71%\nProductID: PVSL0601021125, Forecast Accuracy: -22.30%\nProductID: PVSL0602021125, Forecast Accuracy: -30.19%\nProductID: PVSL0603031025, Forecast Accuracy: -27.73%\nProductID: PVSL0605021125, Forecast Accuracy: -2.69%\nProductID: PVSL0606041225, Forecast Accuracy: -4.85%\nProductID: PVSL0607021225, Forecast Accuracy: -9.63%\nProductID: PVSL0608011025, Forecast Accuracy: -32.15%\nProductID: PVSL0609031125, Forecast Accuracy: 15.22%\nProductID: PVSL0610020925, Forecast Accuracy: -9.22%\nProductID: PVSL0701030925, Forecast Accuracy: 0.80%\nProductID: PVSL0702061225, Forecast Accuracy: -5.88%\nProductID: PVSL0704021125, Forecast Accuracy: -31.76%\nProductID: PVSL0705060925, Forecast Accuracy: 0.35%\nProductID: PVSL0707041125, Forecast Accuracy: -0.29%\nProductID: PVSL0708041225, Forecast Accuracy: 2.41%\nProductID: PVSL0709061125, Forecast Accuracy: -13.05%\nProductID: PVSL0710051225, Forecast Accuracy: -2.47%\nProductID: PVSL0801050126, Forecast Accuracy: 25.03%\nProductID: PVSL0802050925, Forecast Accuracy: 17.65%\nProductID: PVSL0803061025, Forecast Accuracy: 10.57%\nProductID: PVSL0804061025, Forecast Accuracy: -5.77%\nProductID: PVSL0805061025, Forecast Accuracy: -2.34%\nProductID: PVSL0806051025, Forecast Accuracy: -17.34%\nProductID: PVSL0807050126, Forecast Accuracy: -11.84%\nProductID: PVSL0808031125, Forecast Accuracy: -11.13%\nProductID: PVSL0809030925, Forecast Accuracy: -25.03%\nProductID: PVSL0901011025, Forecast Accuracy: -28.00%\nProductID: PVSL0901031025, Forecast Accuracy: -27.41%\nProductID: PVSL0901051225, Forecast Accuracy: -7.03%\nProductID: PVSL0902011025, Forecast Accuracy: 0.35%\nProductID: PVSL0902030126, Forecast Accuracy: -4.84%\nProductID: PVSL0902050126, Forecast Accuracy: -11.95%\nProductID: PVSL0902061225, Forecast Accuracy: -15.15%\nProductID: PVSL0903021225, Forecast Accuracy: 6.47%\nProductID: PVSL0903041125, Forecast Accuracy: -1.15%\nProductID: PVSL0903051225, Forecast Accuracy: -27.52%\nProductID: PVSL0904010925, Forecast Accuracy: -53.48%\nProductID: PVSL0904030925, Forecast Accuracy: -46.21%\nProductID: PVSL0904050925, Forecast Accuracy: -6.00%\nProductID: PVSL0905010126, Forecast Accuracy: -36.43%\nProductID: PVSL0905021225, Forecast Accuracy: 1.44%\nProductID: PVSL0905051025, Forecast Accuracy: 7.53%ProductID: PVSL0101021125, Forecast Accuracy: 13.21%\nProductID: PVSL0102010126, Forecast Accuracy: 20.22%\nProductID: PVSL0102050126, Forecast Accuracy: -19.36%\nProductID: PVSL0104020126, Forecast Accuracy: 2.25%\nProductID: PVSL0105031025, Forecast Accuracy: 20.28%\nProductID: PVSL0106030126, Forecast Accuracy: 10.20%\nProductID: PVSL0107031025, Forecast Accuracy: 11.35%\nProductID: PVSL0108031025, Forecast Accuracy: -10.49%\nProductID: PVSL0109040126, Forecast Accuracy: -33.88%\nProductID: PVSL0111021025, Forecast Accuracy: -12.34%\nProductID: PVSL0112010925, Forecast Accuracy: 14.96%\nProductID: PVSL0112050925, Forecast Accuracy: -20.65%\nProductID: PVSL0113061125, Forecast Accuracy: -12.35%\nProductID: PVSL0114061125, Forecast Accuracy: -3.47%\nProductID: PVSL0115051125, Forecast Accuracy: -9.00%\nProductID: PVSL0117030126, Forecast Accuracy: 2.13%\nProductID: PVSL0118050126, Forecast Accuracy: 3.50%\nProductID: PVSL0119040925, Forecast Accuracy: -24.47%\nProductID: PVSL0121010925, Forecast Accuracy: -0.26%\nProductID: PVSL0122011125, Forecast Accuracy: -13.86%\nProductID: PVSL0123010925, Forecast Accuracy: -15.29%\nProductID: PVSL0124040925, Forecast Accuracy: -22.12%\nProductID: PVSL0125041125, Forecast Accuracy: -12.17%\nProductID: PVSL0126021025, Forecast Accuracy: -4.81%\nProductID: PVSL0127031025, Forecast Accuracy: -11.96%\nProductID: PVSL0128031225, Forecast Accuracy: 19.25%\nProductID: PVSL0129031025, Forecast Accuracy: -5.24%\nProductID: PVSL0131021025, Forecast Accuracy: -10.10%\nProductID: PVSL0132010126, Forecast Accuracy: -17.79%\nProductID: PVSL0133021025, Forecast Accuracy: -10.06%\nProductID: PVSL0135030925, Forecast Accuracy: -5.42%\nProductID: PVSL0137031225, Forecast Accuracy: -25.64%\nProductID: PVSL0138061225, Forecast Accuracy: 4.87%\nProductID: PVSL0140040925, Forecast Accuracy: 8.65%\nProductID: PVSL0141041125, Forecast Accuracy: 11.34%\nProductID: PVSL0142040126, Forecast Accuracy: -12.53%\nProductID: PVSL0202031125, Forecast Accuracy: -8.92%\nProductID: PVSL0203010126, Forecast Accuracy: -12.15%\nProductID: PVSL0204041225, Forecast Accuracy: 6.23%\nProductID: PVSL0205021125, Forecast Accuracy: -29.30%\nProductID: PVSL0205061125, Forecast Accuracy: -57.49%\nProductID: PVSL0207031125, Forecast Accuracy: -28.10%\nProductID: PVSL0208031025, Forecast Accuracy: 26.74%\nProductID: PVSL0210011125, Forecast Accuracy: -19.06%\nProductID: PVSL0210061125, Forecast Accuracy: -3.73%\nProductID: PVSL0211061025, Forecast Accuracy: -5.10%\nProductID: PVSL0212050126, Forecast Accuracy: 8.96%\nProductID: PVSL0214011125, Forecast Accuracy: -22.12%\nProductID: PVSL0215010126, Forecast Accuracy: 19.96%\nProductID: PVSL0215050126, Forecast Accuracy: -6.13%\nProductID: PVSL0216050126, Forecast Accuracy: 19.09%\nProductID: PVSL0217061225, Forecast Accuracy: -19.59%\nProductID: PVSL0218051125, Forecast Accuracy: 3.47%\nProductID: PVSL0219031225, Forecast Accuracy: -5.17%\nProductID: PVSL0220021025, Forecast Accuracy: 1.84%\nProductID: PVSL0302031125, Forecast Accuracy: -1.85%\nProductID: PVSL0303021125, Forecast Accuracy: -18.27%\nProductID: PVSL0304011025, Forecast Accuracy: 3.55%\nProductID: PVSL0305020925, Forecast Accuracy: 5.56%\nProductID: PVSL0305060925, Forecast Accuracy: -13.02%\nProductID: PVSL0307011225, Forecast Accuracy: 2.54%\nProductID: PVSL0308020925, Forecast Accuracy: -11.01%\nProductID: PVSL0309011125, Forecast Accuracy: -6.91%\nProductID: PVSL0310020126, Forecast Accuracy: -25.71%\nProductID: PVSL0311011025, Forecast Accuracy: -28.51%\nProductID: PVSL0312020126, Forecast Accuracy: 18.51%\nProductID: PVSL0313041025, Forecast Accuracy: -9.52%\nProductID: PVSL0315011025, Forecast Accuracy: 8.34%\nProductID: PVSL0316030925, Forecast Accuracy: -20.20%\nProductID: PVSL0317011125, Forecast Accuracy: 11.01%\nProductID: PVSL0318010126, Forecast Accuracy: -22.68%\nProductID: PVSL0318060126, Forecast Accuracy: -5.00%\nProductID: PVSL0320031225, Forecast Accuracy: -18.22%\nProductID: PVSL0401050925, Forecast Accuracy: 3.12%\nProductID: PVSL0402031225, Forecast Accuracy: -10.41%\nProductID: PVSL0403020126, Forecast Accuracy: -9.13%\nProductID: PVSL0404021225, Forecast Accuracy: -17.05%\nProductID: PVSL0405010126, Forecast Accuracy: -5.06%\nProductID: PVSL0406011125, Forecast Accuracy: -13.12%\nProductID: PVSL0406051125, Forecast Accuracy: -2.70%\nProductID: PVSL0407040126, Forecast Accuracy: 11.19%\nProductID: PVSL0408040925, Forecast Accuracy: -9.35%\nProductID: PVSL0409010925, Forecast Accuracy: -12.30%\nProductID: PVSL0410011125, Forecast Accuracy: -16.70%\nProductID: PVSL0411031225, Forecast Accuracy: -26.88%\nProductID: PVSL0413011225, Forecast Accuracy: 6.10%\nProductID: PVSL0414051125, Forecast Accuracy: -22.31%\nProductID: PVSL0415040126, Forecast Accuracy: -9.86%\nProductID: PVSL0416021025, Forecast Accuracy: -20.93%\nProductID: PVSL0417011225, Forecast Accuracy: -43.88%\nProductID: PVSL0417051225, Forecast Accuracy: -3.24%\nProductID: PVSL0418051125, Forecast Accuracy: -3.52%\nProductID: PVSL0419030126, Forecast Accuracy: 11.70%\nProductID: PVSL0420040925, Forecast Accuracy: -17.50%\nProductID: PVSL0502020925, Forecast Accuracy: -1.43%\nProductID: PVSL0503041225, Forecast Accuracy: -37.76%\nProductID: PVSL0505011125, Forecast Accuracy: -13.45%\nProductID: PVSL0505061125, Forecast Accuracy: -1.75%\nProductID: PVSL0506061225, Forecast Accuracy: -16.55%\nProductID: PVSL0507050925, Forecast Accuracy: -31.68%\nProductID: PVSL0508050925, Forecast Accuracy: 0.47%\nProductID: PVSL0509061125, Forecast Accuracy: -3.74%\nProductID: PVSL0511020925, Forecast Accuracy: -5.50%\nProductID: PVSL0512011125, Forecast Accuracy: -12.42%\nProductID: PVSL0512051125, Forecast Accuracy: 9.43%\nProductID: PVSL0513051225, Forecast Accuracy: -7.46%\nProductID: PVSL0514040126, Forecast Accuracy: -14.48%\nProductID: PVSL0515051225, Forecast Accuracy: 9.52%\nProductID: PVSL0516031025, Forecast Accuracy: 5.73%\nProductID: PVSL0517031225, Forecast Accuracy: -21.24%\nProductID: PVSL0518041025, Forecast Accuracy: -6.96%\nProductID: PVSL0520011125, Forecast Accuracy: 7.32%\nProductID: PVSL0601041125, Forecast Accuracy: -17.49%\nProductID: PVSL0602051125, Forecast Accuracy: -1.97%\nProductID: PVSL0604031025, Forecast Accuracy: 1.62%\nProductID: PVSL0605041125, Forecast Accuracy: -12.24%\nProductID: PVSL0606061225, Forecast Accuracy: 18.76%\nProductID: PVSL0607051225, Forecast Accuracy: -16.56%\nProductID: PVSL0608041025, Forecast Accuracy: -38.79%\nProductID: PVSL0609061125, Forecast Accuracy: 12.73%\nProductID: PVSL0610040925, Forecast Accuracy: -2.69%\nProductID: PVSL0702031225, Forecast Accuracy: -15.49%\nProductID: PVSL0703051025, Forecast Accuracy: -23.42%\nProductID: PVSL0705010925, Forecast Accuracy: 21.40%\nProductID: PVSL0706051225, Forecast Accuracy: -0.64%\nProductID: PVSL0708011225, Forecast Accuracy: -11.70%\nProductID: PVSL0709021125, Forecast Accuracy: 0.83%\nProductID: PVSL0710021225, Forecast Accuracy: -13.13%\nProductID: PVSL0801020126, Forecast Accuracy: 11.08%\nProductID: PVSL0802010925, Forecast Accuracy: 19.01%\nProductID: PVSL0803011025, Forecast Accuracy: -18.61%\nProductID: PVSL0804031025, Forecast Accuracy: 24.88%\nProductID: PVSL0805021025, Forecast Accuracy: -5.72%\nProductID: PVSL0806011025, Forecast Accuracy: -37.67%\nProductID: PVSL0807010126, Forecast Accuracy: -5.83%\nProductID: PVSL0807060126, Forecast Accuracy: -10.23%\nProductID: PVSL0808051125, Forecast Accuracy: -3.63%\nProductID: PVSL0809050925, Forecast Accuracy: -20.88%\nProductID: PVSL0901020925, Forecast Accuracy: 6.52%\nProductID: PVSL0901041225, Forecast Accuracy: -25.43%\nProductID: PVSL0901061125, Forecast Accuracy: -2.19%\nProductID: PVSL0902021225, Forecast Accuracy: -5.02%\nProductID: PVSL0902041225, Forecast Accuracy: 10.75%\nProductID: PVSL0902060126, Forecast Accuracy: -29.37%\nProductID: PVSL0903011225, Forecast Accuracy: -37.99%\nProductID: PVSL0903030925, Forecast Accuracy: -7.59%\nProductID: PVSL0903051125, Forecast Accuracy: 3.93%\nProductID: PVSL0903060925, Forecast Accuracy: 9.28%\nProductID: PVSL0904020126, Forecast Accuracy: -19.61%\nProductID: PVSL0904031225, Forecast Accuracy: -54.24%\nProductID: PVSL0904051125, Forecast Accuracy: -20.58%\nProductID: PVSL0904061025, Forecast Accuracy: -28.14%\nProductID: PVSL0905020126, Forecast Accuracy: -42.20%\nProductID: PVSL0905041225, Forecast Accuracy: -19.80%ProductID: PVSL0101041125, Forecast Accuracy: 20.31%\nProductID: PVSL0102030126, Forecast Accuracy: 39.08%\nProductID: PVSL0103021025, Forecast Accuracy: -3.11%\nProductID: PVSL0103041025, Forecast Accuracy: -5.43%\nProductID: PVSL0105011025, Forecast Accuracy: 20.35%\nProductID: PVSL0106010126, Forecast Accuracy: 10.56%\nProductID: PVSL0106040126, Forecast Accuracy: 15.95%\nProductID: PVSL0107041025, Forecast Accuracy: -3.59%\nProductID: PVSL0109010126, Forecast Accuracy: 8.99%\nProductID: PVSL0110010126, Forecast Accuracy: 5.35%\nProductID: PVSL0111041025, Forecast Accuracy: -12.23%\nProductID: PVSL0112030925, Forecast Accuracy: -2.35%\nProductID: PVSL0113031125, Forecast Accuracy: 24.90%\nProductID: PVSL0114041125, Forecast Accuracy: 5.81%\nProductID: PVSL0115031125, Forecast Accuracy: -24.87%\nProductID: PVSL0116031125, Forecast Accuracy: 4.47%\nProductID: PVSL0118020126, Forecast Accuracy: 0.54%\nProductID: PVSL0119020925, Forecast Accuracy: -13.03%\nProductID: PVSL0120030126, Forecast Accuracy: -0.40%\nProductID: PVSL0121040925, Forecast Accuracy: -14.77%\nProductID: PVSL0122031125, Forecast Accuracy: 4.96%\nProductID: PVSL0123040925, Forecast Accuracy: -5.70%\nProductID: PVSL0125031125, Forecast Accuracy: -6.99%\nProductID: PVSL0126011025, Forecast Accuracy: 6.43%\nProductID: PVSL0127011025, Forecast Accuracy: 5.86%\nProductID: PVSL0128021225, Forecast Accuracy: -17.05%\nProductID: PVSL0129021025, Forecast Accuracy: -17.94%\nProductID: PVSL0130041025, Forecast Accuracy: 31.22%\nProductID: PVSL0131051025, Forecast Accuracy: 14.34%\nProductID: PVSL0133011025, Forecast Accuracy: -24.73%\nProductID: PVSL0135020925, Forecast Accuracy: 25.85%\nProductID: PVSL0137011225, Forecast Accuracy: 5.54%\nProductID: PVSL0138041225, Forecast Accuracy: -3.00%\nProductID: PVSL0139050126, Forecast Accuracy: -28.01%\nProductID: PVSL0140060925, Forecast Accuracy: 3.81%\nProductID: PVSL0142020126, Forecast Accuracy: -0.82%\nProductID: PVSL0201031025, Forecast Accuracy: -8.00%\nProductID: PVSL0202051125, Forecast Accuracy: 25.53%\nProductID: PVSL0203050126, Forecast Accuracy: -15.00%\nProductID: PVSL0204061225, Forecast Accuracy: -19.84%\nProductID: PVSL0205041125, Forecast Accuracy: -8.70%\nProductID: PVSL0206041225, Forecast Accuracy: -2.46%\nProductID: PVSL0207061125, Forecast Accuracy: -25.53%\nProductID: PVSL0208061025, Forecast Accuracy: -3.58%\nProductID: PVSL0210031125, Forecast Accuracy: -2.39%\nProductID: PVSL0211041025, Forecast Accuracy: -25.79%\nProductID: PVSL0212030126, Forecast Accuracy: -2.44%\nProductID: PVSL0213021225, Forecast Accuracy: 11.39%\nProductID: PVSL0214051125, Forecast Accuracy: -5.98%\nProductID: PVSL0215030126, Forecast Accuracy: 16.56%\nProductID: PVSL0216010126, Forecast Accuracy: -19.81%\nProductID: PVSL0217031225, Forecast Accuracy: -12.38%\nProductID: PVSL0218021125, Forecast Accuracy: 2.75%\nProductID: PVSL0219011225, Forecast Accuracy: -9.15%\nProductID: PVSL0219051225, Forecast Accuracy: 7.44%\nProductID: PVSL0302011125, Forecast Accuracy: -20.30%\nProductID: PVSL0302051125, Forecast Accuracy: -9.40%\nProductID: PVSL0303051125, Forecast Accuracy: -16.01%\nProductID: PVSL0304041025, Forecast Accuracy: -18.76%\nProductID: PVSL0305040925, Forecast Accuracy: 5.89%\nProductID: PVSL0306031025, Forecast Accuracy: 8.11%\nProductID: PVSL0307041225, Forecast Accuracy: -16.68%\nProductID: PVSL0308040925, Forecast Accuracy: 6.79%\nProductID: PVSL0309031125, Forecast Accuracy: -19.02%\nProductID: PVSL0310030126, Forecast Accuracy: -19.29%\nProductID: PVSL0311051025, Forecast Accuracy: -0.25%\nProductID: PVSL0312030126, Forecast Accuracy: -21.45%\nProductID: PVSL0314040925, Forecast Accuracy: -34.48%\nProductID: PVSL0315051025, Forecast Accuracy: 0.34%\nProductID: PVSL0316050925, Forecast Accuracy: -4.57%\nProductID: PVSL0317051125, Forecast Accuracy: 11.18%\nProductID: PVSL0318030126, Forecast Accuracy: -3.46%\nProductID: PVSL0319021225, Forecast Accuracy: 0.00%\nProductID: PVSL0401030925, Forecast Accuracy: -58.82%\nProductID: PVSL0402011225, Forecast Accuracy: -35.18%\nProductID: PVSL0402051225, Forecast Accuracy: 4.30%\nProductID: PVSL0403050126, Forecast Accuracy: -22.36%\nProductID: PVSL0404051225, Forecast Accuracy: -1.34%\nProductID: PVSL0405050126, Forecast Accuracy: -14.57%\nProductID: PVSL0406031125, Forecast Accuracy: 17.71%\nProductID: PVSL0407020126, Forecast Accuracy: -3.64%\nProductID: PVSL0407060126, Forecast Accuracy: -16.80%\nProductID: PVSL0408060925, Forecast Accuracy: 2.78%\nProductID: PVSL0409040925, Forecast Accuracy: -12.72%\nProductID: PVSL0410041125, Forecast Accuracy: 5.26%\nProductID: PVSL0412051025, Forecast Accuracy: 8.74%\nProductID: PVSL0414031125, Forecast Accuracy: -19.51%\nProductID: PVSL0415030126, Forecast Accuracy: -15.63%\nProductID: PVSL0416011025, Forecast Accuracy: -14.86%\nProductID: PVSL0416061025, Forecast Accuracy: -11.96%\nProductID: PVSL0417041225, Forecast Accuracy: -25.75%\nProductID: PVSL0418041125, Forecast Accuracy: -1.27%\nProductID: PVSL0419020126, Forecast Accuracy: 9.48%\nProductID: PVSL0420010925, Forecast Accuracy: -25.85%\nProductID: PVSL0501060925, Forecast Accuracy: 12.25%\nProductID: PVSL0503031225, Forecast Accuracy: -13.75%\nProductID: PVSL0504041125, Forecast Accuracy: -11.70%\nProductID: PVSL0505031125, Forecast Accuracy: -19.76%\nProductID: PVSL0506031225, Forecast Accuracy: -1.13%\nProductID: PVSL0507030925, Forecast Accuracy: 8.76%\nProductID: PVSL0508010925, Forecast Accuracy: -26.75%\nProductID: PVSL0509051125, Forecast Accuracy: -15.70%\nProductID: PVSL0511010925, Forecast Accuracy: -12.76%\nProductID: PVSL0511060925, Forecast Accuracy: 12.38%\nProductID: PVSL0512041125, Forecast Accuracy: -14.67%\nProductID: PVSL0513041225, Forecast Accuracy: 1.40%\nProductID: PVSL0514050126, Forecast Accuracy: -20.05%\nProductID: PVSL0515061225, Forecast Accuracy: -10.22%\nProductID: PVSL0516051025, Forecast Accuracy: -27.27%\nProductID: PVSL0517041225, Forecast Accuracy: -14.88%\nProductID: PVSL0519011125, Forecast Accuracy: -3.00%\nProductID: PVSL0601011125, Forecast Accuracy: 20.01%\nProductID: PVSL0602011125, Forecast Accuracy: -0.53%\nProductID: PVSL0603021025, Forecast Accuracy: -30.24%\nProductID: PVSL0604041025, Forecast Accuracy: -24.42%\nProductID: PVSL0606021225, Forecast Accuracy: -55.58%\nProductID: PVSL0607011225, Forecast Accuracy: -13.37%\nProductID: PVSL0607061225, Forecast Accuracy: 6.60%\nProductID: PVSL0608061025, Forecast Accuracy: -13.85%\nProductID: PVSL0610010925, Forecast Accuracy: -39.93%\nProductID: PVSL0701010925, Forecast Accuracy: -13.16%\nProductID: PVSL0702051225, Forecast Accuracy: 5.18%\nProductID: PVSL0703061025, Forecast Accuracy: -32.42%\nProductID: PVSL0705030925, Forecast Accuracy: 11.50%\nProductID: PVSL0707011125, Forecast Accuracy: -17.60%\nProductID: PVSL0708031225, Forecast Accuracy: -5.03%\nProductID: PVSL0709041125, Forecast Accuracy: -2.62%\nProductID: PVSL0710031225, Forecast Accuracy: -36.44%\nProductID: PVSL0801040126, Forecast Accuracy: 7.96%\nProductID: PVSL0802040925, Forecast Accuracy: 6.75%\nProductID: PVSL0803031025, Forecast Accuracy: 11.73%\nProductID: PVSL0804051025, Forecast Accuracy: -20.10%\nProductID: PVSL0805051025, Forecast Accuracy: 8.83%\nProductID: PVSL0806031025, Forecast Accuracy: -20.25%\nProductID: PVSL0807040126, Forecast Accuracy: -7.81%\nProductID: PVSL0808021125, Forecast Accuracy: -3.72%\nProductID: PVSL0809020925, Forecast Accuracy: -10.40%\nProductID: PVSL0809060925, Forecast Accuracy: 18.71%\nProductID: PVSL0901021125, Forecast Accuracy: -33.44%\nProductID: PVSL0901050126, Forecast Accuracy: -11.98%\nProductID: PVSL0902011125, Forecast Accuracy: -3.57%\nProductID: PVSL0902031125, Forecast Accuracy: -11.39%\nProductID: PVSL0902051025, Forecast Accuracy: -37.33%\nProductID: PVSL0903020126, Forecast Accuracy: -8.15%\nProductID: PVSL0903030126, Forecast Accuracy: -18.12%\nProductID: PVSL0903041225, Forecast Accuracy: 9.80%\nProductID: PVSL0903060126, Forecast Accuracy: -15.13%\nProductID: PVSL0904011125, Forecast Accuracy: -7.74%\nProductID: PVSL0904031125, Forecast Accuracy: -7.56%\nProductID: PVSL0904050126, Forecast Accuracy: 1.07%\nProductID: PVSL0904060925, Forecast Accuracy: 3.41%\nProductID: PVSL0905021125, Forecast Accuracy: 3.56%\nProductID: PVSL0905041125, Forecast Accuracy: 2.82%\nProductID: PVSL0905061125, Forecast Accuracy: -3.19%\n✅ Product-level forecasts saved to product_forecasts_with_accuracy.csv\n✅ Brand-level forecasts saved to brand_daily_forecast.csv, brand_weekly_forecast.csv, and brand_monthly_forecast.csv\n","output_type":"stream"}],"execution_count":44},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.model_selection import train_test_split\nfrom joblib import Parallel, delayed\nimport ast\n\n# Use a GPU if available\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# ----------------------------\n# 1. Load Data\n# ----------------------------\n# Load the bills and products datasets\nbills_df = pd.read_csv(\"/kaggle/input/sales-prediction/bills (1).csv\")\nproducts_df = pd.read_csv(\"/kaggle/input/sales-prediction/products.csv\")\n\n# ----------------------------\n# 2. Preprocess Bills Data\n# ----------------------------\n# Clean column names by stripping whitespace\nbills_df.columns = bills_df.columns.str.strip()\n\n# Convert Product_IDs and Quantities from string representations to lists\nbills_df[\"Product_IDs\"] = bills_df[\"Product_IDs\"].apply(ast.literal_eval)\nbills_df[\"Quantities\"] = bills_df[\"Quantities\"].apply(ast.literal_eval)\n\n# Explode the DataFrame to have one row per product-quantity pair\nbills_df = bills_df.explode([\"Product_IDs\", \"Quantities\"])\nbills_df.rename(columns={\"Product_IDs\": \"ProductID\", \"Quantities\": \"Quantity_Sold\"}, inplace=True)\n\nbills_df[\"ProductID\"] = bills_df[\"ProductID\"].astype(str).str.strip()\nbills_df[\"Quantity_Sold\"] = bills_df[\"Quantity_Sold\"].astype(int)\nbills_df[\"Date\"] = pd.to_datetime(bills_df[\"Date_Time\"]).dt.date\n\n# ----------------------------\n# 3. Preprocess Products Data\n# ----------------------------\n# Clean up column names and ProductID\nproducts_df.columns = products_df.columns.str.strip()\nproducts_df.rename(columns={'Product Code': 'ProductID'}, inplace=True)\nproducts_df[\"ProductID\"] = products_df[\"ProductID\"].astype(str).str.strip()\n\n# --- Find ProductIDs in bills_df that are not in products_df ---\nunmatched_ids = set(bills_df[\"ProductID\"]) - set(products_df[\"ProductID\"])\nprint(f\"Found {len(unmatched_ids)} ProductIDs in the bills data that are not in the product data.\")\n# ----------------------------------------------------------------\n\n# ----------------------------\n# 4. Merge DataFrames\n# ----------------------------\n# Merge the bills and products dataframes\nmerged_df = bills_df.merge(products_df[['ProductID', 'Brand Name']], on='ProductID', how='left')\nmerged_df['Brand Name'] = merged_df['Brand Name'].fillna('Unknown')\n\n# ----------------------------\n# 5. Create Daily Sales Summary\n# ----------------------------\nsummary_daily_df = (\n    merged_df.groupby([\"Date\", \"ProductID\", \"Brand Name\"])[\"Quantity_Sold\"]\n    .sum()\n    .reset_index()\n    .rename(columns={\"Quantity_Sold\": \"Total_Quantity_Sold\"})\n)\n\n# ----------------------------\n# 6. PyTorch Dataset for Time Series\n# ----------------------------\nclass TimeSeriesDataset(Dataset):\n    def __init__(self, series, seq_len):\n        self.series = series\n        self.seq_len = seq_len\n\n    def __len__(self):\n        return len(self.series) - self.seq_len\n\n    def __getitem__(self, idx):\n        return (\n            torch.tensor(self.series[idx:idx+self.seq_len], dtype=torch.float32).unsqueeze(-1),\n            torch.tensor(self.series[idx+self.seq_len], dtype=torch.float32)\n        )\n\n# ----------------------------\n# 7. GRU with Attention Model\n# ----------------------------\nclass GRUAttention(nn.Module):\n    def __init__(self, input_dim=1, hidden_dim=64, num_layers=2):\n        super().__init__()\n        self.gru = nn.GRU(input_dim, hidden_dim, num_layers, batch_first=True, dropout=0.2)\n        self.attn_fc = nn.Linear(hidden_dim, 1)\n        self.fc = nn.Linear(hidden_dim, 1)\n\n    def forward(self, x):\n        gru_out, _ = self.gru(x)\n        attn_weights = torch.softmax(self.attn_fc(gru_out), dim=1)\n        context_vector = torch.sum(attn_weights * gru_out, dim=1)\n        output = self.fc(context_vector)\n        return output\n\n# ----------------------------\n# 8. Training, Forecasting, and Accuracy Check\n# ----------------------------\ndef train_and_forecast_with_accuracy(series, future=30, seq_len=30, epochs=100, batch_size=64):\n    if len(series) < seq_len + 10: # Ensure enough data for a meaningful split\n        return np.zeros(future).tolist(), 0.0\n\n    # Train-test split\n    train_series, test_series = train_test_split(series, test_size=0.2, shuffle=False)\n\n    scaler = MinMaxScaler()\n    train_scaled = scaler.fit_transform(train_series.reshape(-1, 1)).flatten()\n\n    train_dataset = TimeSeriesDataset(train_scaled, seq_len)\n    if len(train_dataset) == 0:\n        return np.zeros(future).tolist(), 0.0\n    \n    train_loader = DataLoader(train_dataset, batch_size=min(batch_size, len(train_dataset)), shuffle=True)\n    \n    model = GRUAttention().to(device)\n    loss_fn = nn.MSELoss()\n    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n\n    # Training\n    for epoch in range(epochs):\n        for xb, yb in train_loader:\n            xb, yb = xb.to(device), yb.to(device)\n            pred = model(xb).squeeze(-1)\n            loss = loss_fn(pred, yb)\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n\n    # Accuracy check on the test set\n    model.eval()\n    test_inputs = scaler.transform(series[-seq_len - len(test_series):-len(test_series)].reshape(-1, 1)).flatten()\n    test_actuals = test_series\n    \n    test_preds = []\n    with torch.no_grad():\n        current_seq = torch.tensor(test_inputs, dtype=torch.float32).unsqueeze(0).unsqueeze(-1).to(device)\n        for _ in range(len(test_actuals)):\n            out = model(current_seq).item()\n            test_preds.append(out)\n            # Update sequence for next prediction\n            new_seq_val = torch.tensor([[[out]]], dtype=torch.float32).to(device)\n            current_seq = torch.cat([current_seq[:, 1:, :], new_seq_val], dim=1)\n\n    test_preds = scaler.inverse_transform(np.array(test_preds).reshape(-1, 1)).flatten()\n    \n    # Calculate MAPE\n    mape = np.mean(np.abs((test_actuals - test_preds) / np.maximum(test_actuals, 1))) * 100\n    accuracy = 100 - mape\n\n    # Forecasting for the future\n    model.eval()\n    input_seq = torch.tensor(scaler.transform(series[-seq_len:].reshape(-1, 1)).flatten(), dtype=torch.float32).unsqueeze(0).unsqueeze(-1).to(device)\n    future_preds = []\n    with torch.no_grad():\n        for _ in range(future):\n            out = model(input_seq).item()\n            future_preds.append(out)\n            new_input = torch.cat([input_seq[:, 1:, :], torch.tensor([[[out]]], dtype=torch.float32).to(device)], dim=1)\n            input_seq = new_input\n            \n    future_preds = scaler.inverse_transform(np.array(future_preds).reshape(-1, 1)).flatten()\n    future_preds = np.maximum(future_preds, 0)\n    \n    return np.round(future_preds).tolist(), accuracy\n\n# ----------------------------\n# 9. Generate Forecasts per Product\n# ----------------------------\ndef process_product_all(product_id, df, seq_len=30):\n    group = df[df[\"ProductID\"] == product_id].sort_values(\"Date\")\n    series = group[\"Total_Quantity_Sold\"].values\n\n    if len(series) < 2:\n        return None\n\n    daily_forecast, accuracy = train_and_forecast_with_accuracy(series, future=30, seq_len=seq_len)\n    weekly_forecast, _ = train_and_forecast_with_accuracy(series, future=7, seq_len=seq_len)\n    monthly_forecast, _ = train_and_forecast_with_accuracy(series, future=30, seq_len=seq_len)\n    \n    print(f\"ProductID: {product_id}, Forecast Accuracy: {accuracy:.2f}%\")\n    \n    return {\n        \"ProductID\": product_id,\n        \"Brand Name\": group[\"Brand Name\"].iloc[0],\n        \"Daily_Forecast\": daily_forecast,\n        \"Weekly_Forecast\": weekly_forecast,\n        \"Monthly_Forecast\": monthly_forecast,\n        \"Accuracy (%)\": accuracy\n    }\n\n# Run the forecasting in parallel\nproduct_ids = summary_daily_df[\"ProductID\"].unique()\nresults = Parallel(n_jobs=-1)(delayed(process_product_all)(pid, summary_daily_df) for pid in product_ids)\nresults = [r for r in results if r is not None]\n\n# Save the product-level forecasts\nforecast_df = pd.DataFrame(results)\nforecast_df.to_csv(\"product_forecasts_with_accuracy.csv\", index=False)\nprint(\"✅ Product-level forecasts saved to product_forecasts_with_accuracy.csv\")\n\n# ----------------------------\n# 10. Aggregate Forecasts by Brand\n# ----------------------------\ndef aggregate_brand_forecast(forecast_df, col_name, horizon_name):\n    expanded = []\n    for _, row in forecast_df.iterrows():\n        brand_col = row['Brand Name'] if pd.notnull(row['Brand Name']) else 'Unknown'\n        for i, val in enumerate(row[col_name]):\n            expanded.append({\"Brand\": brand_col, horizon_name: i + 1, \"Forecast_Qty\": val})\n\n    expanded_df = pd.DataFrame(expanded)\n    brand_forecast = expanded_df.groupby([\"Brand\", horizon_name], as_index=False)[\"Forecast_Qty\"].sum()\n    brand_forecast[\"Forecast_Qty\"] = brand_forecast[\"Forecast_Qty\"].round()\n    return brand_forecast\n\n# Generate and save brand-level forecasts\nbrand_daily_forecast = aggregate_brand_forecast(forecast_df, \"Daily_Forecast\", \"Day\")\nbrand_weekly_forecast = aggregate_brand_forecast(forecast_df, \"Weekly_Forecast\", \"Week\")\nbrand_monthly_forecast = aggregate_brand_forecast(forecast_df, \"Month_Forecast\", \"Month\")\n\nbrand_daily_forecast.to_csv(\"brand_daily_forecast.csv\", index=False)\nbrand_weekly_forecast.to_csv(\"brand_weekly_forecast.csv\", index=False)\nbrand_monthly_forecast.to_csv(\"brand_monthly_forecast.csv\", index=False)\nprint(\"✅ Brand-level forecasts saved to brand_daily_forecast.csv, brand_weekly_forecast.csv, and brand_monthly_forecast.csv\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-13T08:06:01.368904Z","iopub.execute_input":"2025-09-13T08:06:01.369168Z","iopub.status.idle":"2025-09-13T08:23:04.200606Z","shell.execute_reply.started":"2025-09-13T08:06:01.369148Z","shell.execute_reply":"2025-09-13T08:23:04.199584Z"}},"outputs":[{"name":"stdout","text":"Found 1078 ProductIDs in the bills data that are not in the product data.\n\nProductID: PVSL0905061225, Forecast Accuracy: 11.33%\nProductID: PVSL0906021125, Forecast Accuracy: 8.45%\nProductID: PVSL0906041025, Forecast Accuracy: 5.13%\nProductID: PVSL0906050925, Forecast Accuracy: -12.80%\nProductID: PVSL0907011025, Forecast Accuracy: 0.22%\nProductID: PVSL0907030126, Forecast Accuracy: -36.14%\nProductID: PVSL0907040126, Forecast Accuracy: -20.47%\nProductID: PVSL0907051025, Forecast Accuracy: -33.17%\nProductID: PVSL0908011125, Forecast Accuracy: -13.39%\nProductID: PVSL0908030126, Forecast Accuracy: 11.33%\nProductID: PVSL0908040925, Forecast Accuracy: -4.72%\nProductID: PVSL0908060925, Forecast Accuracy: -3.77%\nProductID: PVSL0909011125, Forecast Accuracy: -27.22%\nProductID: PVSL0909031125, Forecast Accuracy: 8.21%\nProductID: PVSL0909050925, Forecast Accuracy: 2.35%\nProductID: PVSL0909061125, Forecast Accuracy: -28.53%\nProductID: PVSL0910030126, Forecast Accuracy: -8.35%\nProductID: PVSL0910051125, Forecast Accuracy: -23.38%\nProductID: PVSL0910061125, Forecast Accuracy: -5.95%\nProductID: PVSL0108041025, Forecast Accuracy: 3.52%\nProductID: PVSL0113021125, Forecast Accuracy: -17.15%\nProductID: PVSL0115061125, Forecast Accuracy: -15.93%\nProductID: PVSL0117020126, Forecast Accuracy: -7.73%\nProductID: PVSL0123050925, Forecast Accuracy: 1.00%\nProductID: PVSL0126061025, Forecast Accuracy: 17.19%\nProductID: PVSL0129051025, Forecast Accuracy: -23.91%\nProductID: PVSL0134041125, Forecast Accuracy: -18.82%\nProductID: PVSL0139010126, Forecast Accuracy: 16.25%\nProductID: PVSL0141021125, Forecast Accuracy: -7.21%\nProductID: PVSL0202021125, Forecast Accuracy: -19.05%\nProductID: PVSL0204021225, Forecast Accuracy: -6.13%\nProductID: PVSL0207021125, Forecast Accuracy: -17.25%\nProductID: PVSL0211031025, Forecast Accuracy: -25.27%\nProductID: PVSL0214041125, Forecast Accuracy: -27.13%\nProductID: PVSL0217011225, Forecast Accuracy: 2.76%\nProductID: PVSL0220041025, Forecast Accuracy: -6.88%\nProductID: PVSL0301050925, Forecast Accuracy: -17.17%\nProductID: PVSL0306021025, Forecast Accuracy: 19.64%\nProductID: PVSL0311041025, Forecast Accuracy: -9.11%\nProductID: PVSL0314050925, Forecast Accuracy: -27.61%\nProductID: PVSL0319031225, Forecast Accuracy: -13.57%\nProductID: PVSL0320021225, Forecast Accuracy: -12.05%\nProductID: PVSL0404011225, Forecast Accuracy: 17.84%\nProductID: PVSL0408020925, Forecast Accuracy: -3.45%\nProductID: PVSL0411021225, Forecast Accuracy: -34.64%\nProductID: PVSL0412061025, Forecast Accuracy: 20.00%\nProductID: PVSL0416051025, Forecast Accuracy: 0.24%\nProductID: PVSL0501010925, Forecast Accuracy: -38.58%\nProductID: PVSL0503011225, Forecast Accuracy: -34.31%\nProductID: PVSL0504061125, Forecast Accuracy: 9.53%\nProductID: PVSL0507020925, Forecast Accuracy: -18.41%\nProductID: PVSL0510011025, Forecast Accuracy: -2.90%\nProductID: PVSL0513031225, Forecast Accuracy: -17.57%\nProductID: PVSL0517011225, Forecast Accuracy: -20.03%\nProductID: PVSL0518051025, Forecast Accuracy: -7.95%\nProductID: PVSL0520031125, Forecast Accuracy: -6.42%\nProductID: PVSL0603051025, Forecast Accuracy: -4.03%\nProductID: PVSL0605011125, Forecast Accuracy: -63.25%\nProductID: PVSL0608031025, Forecast Accuracy: 18.82%\nProductID: PVSL0610050925, Forecast Accuracy: 9.95%\nProductID: PVSL0701060925, Forecast Accuracy: 4.38%\nProductID: PVSL0704041125, Forecast Accuracy: 37.60%\nProductID: PVSL0706011225, Forecast Accuracy: -22.62%\nProductID: PVSL0707021125, Forecast Accuracy: -10.90%\nProductID: PVSL0710041225, Forecast Accuracy: -25.96%\nProductID: PVSL0805031025, Forecast Accuracy: 10.39%\nProductID: PVSL0810030126, Forecast Accuracy: 21.52%\nProductID: PVSL0901061025, Forecast Accuracy: 16.61%\nProductID: PVSL0903031125, Forecast Accuracy: 5.23%\nProductID: PVSL0904020925, Forecast Accuracy: -14.22%\nProductID: PVSL0904061125, Forecast Accuracy: -14.15%\nProductID: PVSL0905061025, Forecast Accuracy: -5.72%\nProductID: PVSL0906030925, Forecast Accuracy: -44.09%\nProductID: PVSL0907010925, Forecast Accuracy: -22.60%\nProductID: PVSL0907061125, Forecast Accuracy: -13.72%\nProductID: PVSL0908060126, Forecast Accuracy: -21.43%\nProductID: PVSL0910010126, Forecast Accuracy: -1.24%\nProductID: PVSL0910031025, Forecast Accuracy: 4.45%\nProductID: PVSL0111061025, Forecast Accuracy: -29.87%\nProductID: PVSL0118030126, Forecast Accuracy: 8.06%\nProductID: PVSL0127061025, Forecast Accuracy: 3.15%\nProductID: PVSL0131011025, Forecast Accuracy: -2.15%\nProductID: PVSL0140010925, Forecast Accuracy: -14.22%\nProductID: PVSL0201061025, Forecast Accuracy: -19.59%\nProductID: PVSL0211011025, Forecast Accuracy: -17.33%\nProductID: PVSL0303031125, Forecast Accuracy: -11.80%\nProductID: PVSL0310050126, Forecast Accuracy: -16.91%\nProductID: PVSL0315021025, Forecast Accuracy: -1.80%\nProductID: PVSL0320051225, Forecast Accuracy: -24.01%\nProductID: PVSL0411051225, Forecast Accuracy: -5.31%\nProductID: PVSL0420020925, Forecast Accuracy: -8.78%\nProductID: PVSL0508030925, Forecast Accuracy: -21.14%\nProductID: PVSL0519041125, Forecast Accuracy: -17.12%\nProductID: PVSL0601051125, Forecast Accuracy: 13.08%\nProductID: PVSL0703031025, Forecast Accuracy: -6.93%\nProductID: PVSL0709031125, Forecast Accuracy: -17.63%\nProductID: PVSL0804011025, Forecast Accuracy: -31.14%\nProductID: PVSL0810060126, Forecast Accuracy: -10.59%\nProductID: PVSL0902010126, Forecast Accuracy: -23.24%\nProductID: PVSL0907060126, Forecast Accuracy: 7.50%\nProductID: PVSL0104030126, Forecast Accuracy: -24.92%\nProductID: PVSL0124050925, Forecast Accuracy: -32.14%\nProductID: PVSL0142050126, Forecast Accuracy: -3.62%\nProductID: PVSL0220051025, Forecast Accuracy: -8.40%\nProductID: PVSL0312040126, Forecast Accuracy: 2.39%\nProductID: PVSL0413041225, Forecast Accuracy: -25.56%\nProductID: PVSL0510031025, Forecast Accuracy: -31.90%\nProductID: PVSL0609051125, Forecast Accuracy: -21.30%\nProductID: PVSL0803021025, Forecast Accuracy: -1.85%\nProductID: PVSL0905011125, Forecast Accuracy: -5.98%\nProductID: PVSL0137041225, Forecast Accuracy: 10.29%\nProductID: PVSL0304061025, Forecast Accuracy: 6.85%\nProductID: PVSL0704051125, Forecast Accuracy: 9.87%\nProductID: PVSL0903010126, Forecast Accuracy: -19.21%\nProductID: PVSL0807030126, Forecast Accuracy: 23.99%\nProductID: PVSL0803051025, Forecast Accuracy: -27.50%\nProductID: PVSL0101031125, Forecast Accuracy: 12.30%\nProductID: PVSL0102020126, Forecast Accuracy: 7.06%\nProductID: PVSL0103011025, Forecast Accuracy: -16.73%\nProductID: PVSL0103031025, Forecast Accuracy: 5.79%\nProductID: PVSL0104050126, Forecast Accuracy: -4.49%\nProductID: PVSL0105051025, Forecast Accuracy: -41.53%\nProductID: PVSL0106050126, Forecast Accuracy: -18.71%\nProductID: PVSL0107051025, Forecast Accuracy: -3.14%\nProductID: PVSL0109010126, Forecast Accuracy: 2.56%\nProductID: PVSL0110010126, Forecast Accuracy: 6.95%\nProductID: PVSL0111031025, Forecast Accuracy: 0.64%\nProductID: PVSL0112020925, Forecast Accuracy: -24.08%\nProductID: PVSL0112060925, Forecast Accuracy: -38.69%\nProductID: PVSL0114021125, Forecast Accuracy: 1.18%\nProductID: PVSL0115011125, Forecast Accuracy: -8.00%\nProductID: PVSL0116021125, Forecast Accuracy: 12.65%\nProductID: PVSL0117050126, Forecast Accuracy: -18.63%\nProductID: PVSL0119010925, Forecast Accuracy: 16.77%\nProductID: PVSL0119050925, Forecast Accuracy: 0.51%\nProductID: PVSL0121020925, Forecast Accuracy: -5.70%\nProductID: PVSL0122021125, Forecast Accuracy: -3.65%\nProductID: PVSL0123030925, Forecast Accuracy: 2.73%\nProductID: PVSL0125021125, Forecast Accuracy: 17.23%\nProductID: PVSL0125061125, Forecast Accuracy: -40.55%\nProductID: PVSL0127011025, Forecast Accuracy: 6.38%\nProductID: PVSL0128021225, Forecast Accuracy: -21.31%\nProductID: PVSL0129021025, Forecast Accuracy: -19.05%\nProductID: PVSL0130041025, Forecast Accuracy: 29.95%\nProductID: PVSL0131051025, Forecast Accuracy: 16.65%\nProductID: PVSL0133011025, Forecast Accuracy: -23.74%\nProductID: PVSL0135020925, Forecast Accuracy: 23.39%\nProductID: PVSL0137011225, Forecast Accuracy: 2.02%\nProductID: PVSL0138041225, Forecast Accuracy: -1.76%\nProductID: PVSL0139050126, Forecast Accuracy: -21.63%\nProductID: PVSL0140060925, Forecast Accuracy: 7.51%\nProductID: PVSL0142020126, Forecast Accuracy: 1.42%\nProductID: PVSL0201031025, Forecast Accuracy: -11.32%\nProductID: PVSL0202051125, Forecast Accuracy: 29.57%\nProductID: PVSL0905051225, Forecast Accuracy: 12.97%\nProductID: PVSL0906020126, Forecast Accuracy: 3.44%\nProductID: PVSL0906040925, Forecast Accuracy: 2.81%\nProductID: PVSL0906041225, Forecast Accuracy: -18.37%\nProductID: PVSL0906061125, Forecast Accuracy: -31.03%\nProductID: PVSL0907021125, Forecast Accuracy: -9.30%\nProductID: PVSL0907031225, Forecast Accuracy: -7.28%\nProductID: PVSL0907051225, Forecast Accuracy: -23.81%\nProductID: PVSL0908020126, Forecast Accuracy: -25.08%\nProductID: PVSL0908040126, Forecast Accuracy: 2.22%\nProductID: PVSL0908051025, Forecast Accuracy: -24.46%\nProductID: PVSL0908061025, Forecast Accuracy: 3.04%\nProductID: PVSL0909021125, Forecast Accuracy: 13.51%\nProductID: PVSL0909040925, Forecast Accuracy: -9.86%\nProductID: PVSL0909051025, Forecast Accuracy: -3.94%\nProductID: PVSL0910020925, Forecast Accuracy: -24.71%\nProductID: PVSL0910031225, Forecast Accuracy: -3.67%\nProductID: PVSL0910051225, Forecast Accuracy: -26.68%\nProductID: PVSL0104010126, Forecast Accuracy: 17.04%\nProductID: PVSL0108011025, Forecast Accuracy: -26.49%\nProductID: PVSL0110050126, Forecast Accuracy: -4.15%\nProductID: PVSL0115021125, Forecast Accuracy: -48.71%\nProductID: PVSL0117010126, Forecast Accuracy: 4.69%\nProductID: PVSL0122041125, Forecast Accuracy: -10.66%\nProductID: PVSL0124030925, Forecast Accuracy: -1.76%\nProductID: PVSL0129041025, Forecast Accuracy: -27.58%\nProductID: PVSL0134031125, Forecast Accuracy: -15.22%\nProductID: PVSL0138021225, Forecast Accuracy: 13.99%\nProductID: PVSL0140020925, Forecast Accuracy: -5.76%\nProductID: PVSL0202011125, Forecast Accuracy: 5.56%\nProductID: PVSL0204011225, Forecast Accuracy: -0.40%\nProductID: PVSL0207011125, Forecast Accuracy: 0.08%\nProductID: PVSL0209021225, Forecast Accuracy: -7.62%\nProductID: PVSL0213011225, Forecast Accuracy: 11.06%\nProductID: PVSL0216040126, Forecast Accuracy: -31.07%\nProductID: PVSL0220031025, Forecast Accuracy: -20.01%\nProductID: PVSL0301040925, Forecast Accuracy: 28.16%\nProductID: PVSL0305010925, Forecast Accuracy: -48.41%\nProductID: PVSL0310010126, Forecast Accuracy: -8.75%\nProductID: PVSL0313031025, Forecast Accuracy: -24.86%\nProductID: PVSL0316020925, Forecast Accuracy: 7.77%\nProductID: PVSL0319061225, Forecast Accuracy: 8.05%\nProductID: PVSL0402061225, Forecast Accuracy: -42.01%\nProductID: PVSL0405030126, Forecast Accuracy: -37.77%\nProductID: PVSL0410031125, Forecast Accuracy: -7.01%\nProductID: PVSL0412011025, Forecast Accuracy: -19.69%\nProductID: PVSL0413031225, Forecast Accuracy: -8.96%\nProductID: PVSL0420030925, Forecast Accuracy: -7.68%\nProductID: PVSL0502030925, Forecast Accuracy: 11.95%\nProductID: PVSL0503061225, Forecast Accuracy: 13.36%\nProductID: PVSL0506021225, Forecast Accuracy: -30.05%\nProductID: PVSL0509021125, Forecast Accuracy: 27.47%\nProductID: PVSL0511040925, Forecast Accuracy: 18.25%\nProductID: PVSL0515041225, Forecast Accuracy: 13.19%\nProductID: PVSL0518011025, Forecast Accuracy: 0.61%\nProductID: PVSL0519021125, Forecast Accuracy: -4.81%\nProductID: PVSL0602031125, Forecast Accuracy: 11.41%\nProductID: PVSL0604051025, Forecast Accuracy: -5.91%\nProductID: PVSL0605061125, Forecast Accuracy: 3.55%\nProductID: PVSL0609011125, Forecast Accuracy: 1.66%\nProductID: PVSL0701020925, Forecast Accuracy: 13.87%\nProductID: PVSL0702041225, Forecast Accuracy: -20.47%\nProductID: PVSL0705040925, Forecast Accuracy: 8.52%\nProductID: PVSL0706031225, Forecast Accuracy: 6.09%\nProductID: PVSL0708051225, Forecast Accuracy: -5.15%\nProductID: PVSL0804021025, Forecast Accuracy: 2.12%\nProductID: PVSL0808041125, Forecast Accuracy: -9.00%\nProductID: PVSL0901031225, Forecast Accuracy: -30.45%\nProductID: PVSL0903020925, Forecast Accuracy: 4.79%\nProductID: PVSL0904011225, Forecast Accuracy: 5.87%\nProductID: PVSL0904040126, Forecast Accuracy: 8.65%\nProductID: PVSL0905050925, Forecast Accuracy: -62.79%\nProductID: PVSL0906051125, Forecast Accuracy: -7.05%\nProductID: PVSL0907011125, Forecast Accuracy: 9.32%\nProductID: PVSL0908010925, Forecast Accuracy: -17.27%\nProductID: PVSL0909011025, Forecast Accuracy: 10.75%\nProductID: PVSL0910021025, Forecast Accuracy: -21.24%\nProductID: PVSL0109020126, Forecast Accuracy: 2.53%\nProductID: PVSL0117040126, Forecast Accuracy: -17.95%\nProductID: PVSL0126031025, Forecast Accuracy: -10.89%\nProductID: PVSL0130011025, Forecast Accuracy: -19.75%\nProductID: PVSL0136020925, Forecast Accuracy: -16.93%\nProductID: PVSL0142010126, Forecast Accuracy: -44.62%\nProductID: PVSL0210051125, Forecast Accuracy: -16.42%\nProductID: PVSL0218041125, Forecast Accuracy: -3.73%\nProductID: PVSL0308060925, Forecast Accuracy: -10.09%\nProductID: PVSL0314020925, Forecast Accuracy: 1.91%\nProductID: PVSL0318050126, Forecast Accuracy: -13.61%\nProductID: PVSL0409060925, Forecast Accuracy: 3.55%\nProductID: PVSL0419040126, Forecast Accuracy: 5.08%\nProductID: PVSL0502060925, Forecast Accuracy: 30.38%\nProductID: PVSL0515031225, Forecast Accuracy: -44.45%\nProductID: PVSL0520061125, Forecast Accuracy: -24.00%\nProductID: PVSL0703011025, Forecast Accuracy: -7.67%\nProductID: PVSL0708021225, Forecast Accuracy: -4.86%\nProductID: PVSL0802030925, Forecast Accuracy: 13.29%\nProductID: PVSL0810010126, Forecast Accuracy: 8.52%\nProductID: PVSL0901041125, Forecast Accuracy: -17.49%\nProductID: PVSL0903011025, Forecast Accuracy: -7.70%\nProductID: PVSL0905040925, Forecast Accuracy: 21.20%\nProductID: PVSL0908041125, Forecast Accuracy: -25.41%\nProductID: PVSL0120020126, Forecast Accuracy: -18.59%\nProductID: PVSL0136030925, Forecast Accuracy: 13.92%\nProductID: PVSL0209011225, Forecast Accuracy: -19.74%\nProductID: PVSL0301060925, Forecast Accuracy: -19.23%\nProductID: PVSL0401010925, Forecast Accuracy: -16.30%\nProductID: PVSL0414041125, Forecast Accuracy: 24.56%\nProductID: PVSL0602061125, Forecast Accuracy: -8.99%\nProductID: PVSL0701040925, Forecast Accuracy: -25.04%\nProductID: PVSL0803041025, Forecast Accuracy: -21.98%\nProductID: PVSL0906040126, Forecast Accuracy: 11.44%\nProductID: PVSL0213031225, Forecast Accuracy: 28.60%\nProductID: PVSL0413061225, Forecast Accuracy: -32.45%\nProductID: PVSL0901051025, Forecast Accuracy: 13.26%\nProductID: PVSL0510041025, Forecast Accuracy: -15.24%\nProductID: PVSL0601061125, Forecast Accuracy: -13.99%\nProductID: PVSL0101021125, Forecast Accuracy: 5.26%\nProductID: PVSL0102010126, Forecast Accuracy: 16.60%\nProductID: PVSL0102050126, Forecast Accuracy: -18.29%\nProductID: PVSL0104020126, Forecast Accuracy: -3.60%\nProductID: PVSL0105031025, Forecast Accuracy: 22.04%\nProductID: PVSL0106030126, Forecast Accuracy: 16.00%\nProductID: PVSL0107021025, Forecast Accuracy: 4.20%\nProductID: PVSL0108021025, Forecast Accuracy: -56.68%\nProductID: PVSL0109030126, Forecast Accuracy: 6.82%\nProductID: PVSL0110040126, Forecast Accuracy: -2.63%\nProductID: PVSL0111051025, Forecast Accuracy: 9.41%\nProductID: PVSL0112040925, Forecast Accuracy: 14.78%\nProductID: PVSL0113061125, Forecast Accuracy: -12.12%\nProductID: PVSL0114061125, Forecast Accuracy: 8.18%\nProductID: PVSL0115051125, Forecast Accuracy: -7.33%\nProductID: PVSL0117030126, Forecast Accuracy: 4.85%\nProductID: PVSL0118050126, Forecast Accuracy: 3.82%\nProductID: PVSL0119040925, Forecast Accuracy: -18.86%\nProductID: PVSL0121010925, Forecast Accuracy: 3.03%\nProductID: PVSL0122011125, Forecast Accuracy: -10.26%\nProductID: PVSL0123010925, Forecast Accuracy: -18.82%\nProductID: PVSL0124040925, Forecast Accuracy: -27.35%\nProductID: PVSL0125041125, Forecast Accuracy: -5.61%\nProductID: PVSL0126021025, Forecast Accuracy: -5.04%\nProductID: PVSL0127031025, Forecast Accuracy: -11.02%\nProductID: PVSL0128031225, Forecast Accuracy: 21.93%\nProductID: PVSL0129031025, Forecast Accuracy: -8.45%\nProductID: PVSL0131021025, Forecast Accuracy: -14.22%\nProductID: PVSL0132010126, Forecast Accuracy: -20.18%\nProductID: PVSL0133021025, Forecast Accuracy: -11.92%\nProductID: PVSL0135030925, Forecast Accuracy: -9.77%\nProductID: PVSL0137031225, Forecast Accuracy: -24.21%\nProductID: PVSL0138051225, Forecast Accuracy: -18.66%\nProductID: PVSL0140030925, Forecast Accuracy: -17.95%\nProductID: PVSL0141011125, Forecast Accuracy: -3.81%\nProductID: PVSL0142030126, Forecast Accuracy: -33.88%\nProductID: PVSL0201041025, Forecast Accuracy: -9.02%\nProductID: PVSL0202061125, Forecast Accuracy: 21.67%\n\nProductID: PVSL0905031025, Forecast Accuracy: -30.40%\nProductID: PVSL0905051125, Forecast Accuracy: -5.31%\nProductID: PVSL0906010925, Forecast Accuracy: -0.02%\nProductID: PVSL0906031225, Forecast Accuracy: -9.47%\nProductID: PVSL0906050126, Forecast Accuracy: 4.46%\nProductID: PVSL0906061225, Forecast Accuracy: -2.83%\nProductID: PVSL0907031125, Forecast Accuracy: -18.25%\nProductID: PVSL0907050925, Forecast Accuracy: -12.95%\nProductID: PVSL0907061025, Forecast Accuracy: -24.45%\nProductID: PVSL0908021025, Forecast Accuracy: -29.12%\nProductID: PVSL0908030925, Forecast Accuracy: -18.67%\nProductID: PVSL0908051125, Forecast Accuracy: -35.57%\nProductID: PVSL0909020925, Forecast Accuracy: -1.92%\nProductID: PVSL0909031225, Forecast Accuracy: -13.34%\nProductID: PVSL0909051225, Forecast Accuracy: 3.66%\nProductID: PVSL0910021225, Forecast Accuracy: -6.95%\nProductID: PVSL0910041025, Forecast Accuracy: -44.48%\nProductID: PVSL0910060126, Forecast Accuracy: -0.64%\nProductID: PVSL0107011025, Forecast Accuracy: 0.36%\nProductID: PVSL0110030126, Forecast Accuracy: -28.09%\nProductID: PVSL0114031125, Forecast Accuracy: -66.22%\nProductID: PVSL0116051125, Forecast Accuracy: -19.64%\nProductID: PVSL0121030925, Forecast Accuracy: 12.73%\nProductID: PVSL0124020925, Forecast Accuracy: -11.71%\nProductID: PVSL0127051025, Forecast Accuracy: -15.88%\nProductID: PVSL0133041025, Forecast Accuracy: -0.90%\nProductID: PVSL0137021225, Forecast Accuracy: 8.12%\nProductID: PVSL0139030126, Forecast Accuracy: -4.02%\nProductID: PVSL0201051025, Forecast Accuracy: -20.09%\nProductID: PVSL0203040126, Forecast Accuracy: -17.34%\nProductID: PVSL0206051225, Forecast Accuracy: -18.77%\nProductID: PVSL0208011025, Forecast Accuracy: -6.29%\nProductID: PVSL0212010126, Forecast Accuracy: -42.20%\nProductID: PVSL0216030126, Forecast Accuracy: -14.70%\nProductID: PVSL0220011025, Forecast Accuracy: -9.46%\nProductID: PVSL0301030925, Forecast Accuracy: -4.38%\nProductID: PVSL0304031025, Forecast Accuracy: -28.16%\nProductID: PVSL0307061225, Forecast Accuracy: -20.50%\nProductID: PVSL0312060126, Forecast Accuracy: 12.89%\nProductID: PVSL0315041025, Forecast Accuracy: 21.40%\nProductID: PVSL0319051225, Forecast Accuracy: -11.49%\nProductID: PVSL0320061225, Forecast Accuracy: -0.97%\nProductID: PVSL0404031225, Forecast Accuracy: -11.95%\nProductID: PVSL0408030925, Forecast Accuracy: -3.69%\nProductID: PVSL0411041225, Forecast Accuracy: 4.06%\nProductID: PVSL0413021225, Forecast Accuracy: 0.83%\nProductID: PVSL0418021125, Forecast Accuracy: -24.95%\nProductID: PVSL0501030925, Forecast Accuracy: -8.30%\nProductID: PVSL0503051225, Forecast Accuracy: 2.74%\nProductID: PVSL0505041125, Forecast Accuracy: -33.92%\nProductID: PVSL0508040925, Forecast Accuracy: -20.72%\nProductID: PVSL0510051025, Forecast Accuracy: -1.88%\nProductID: PVSL0513061225, Forecast Accuracy: -30.38%\nProductID: PVSL0517051225, Forecast Accuracy: -12.90%\nProductID: PVSL0518061025, Forecast Accuracy: -15.18%\nProductID: PVSL0520041125, Forecast Accuracy: -9.93%\nProductID: PVSL0604021025, Forecast Accuracy: -23.55%\nProductID: PVSL0605051125, Forecast Accuracy: -8.63%\nProductID: PVSL0608051025, Forecast Accuracy: -6.50%\nProductID: PVSL0610060925, Forecast Accuracy: -13.41%\nProductID: PVSL0702021225, Forecast Accuracy: 6.29%\nProductID: PVSL0705020925, Forecast Accuracy: -11.20%\nProductID: PVSL0706021225, Forecast Accuracy: 2.72%\nProductID: PVSL0707031125, Forecast Accuracy: 1.35%\nProductID: PVSL0801030126, Forecast Accuracy: 7.78%\nProductID: PVSL0806061025, Forecast Accuracy: -18.31%\nProductID: PVSL0810050126, Forecast Accuracy: 9.99%\nProductID: PVSL0902021125, Forecast Accuracy: 12.42%\nProductID: PVSL0903040925, Forecast Accuracy: 5.81%\nProductID: PVSL0905011025, Forecast Accuracy: -60.62%\nProductID: PVSL0906010126, Forecast Accuracy: -7.32%\nProductID: PVSL0906031125, Forecast Accuracy: -2.45%\nProductID: PVSL0906060925, Forecast Accuracy: 8.43%\nProductID: PVSL0907041025, Forecast Accuracy: 4.00%\nProductID: PVSL0908050126, Forecast Accuracy: -8.90%\nProductID: PVSL0909011225, Forecast Accuracy: -22.11%\nProductID: PVSL0910011225, Forecast Accuracy: -7.90%\nProductID: PVSL0104040126, Forecast Accuracy: 4.58%\nProductID: PVSL0113041125, Forecast Accuracy: -6.42%\nProductID: PVSL0123020925, Forecast Accuracy: -20.06%\nProductID: PVSL0129011025, Forecast Accuracy: -23.46%\nProductID: PVSL0134021125, Forecast Accuracy: -12.26%\nProductID: PVSL0141051125, Forecast Accuracy: -33.58%\nProductID: PVSL0209051225, Forecast Accuracy: 0.80%\nProductID: PVSL0217021225, Forecast Accuracy: -3.82%\nProductID: PVSL0307051225, Forecast Accuracy: 9.97%\nProductID: PVSL0313051025, Forecast Accuracy: -29.06%\nProductID: PVSL0317031125, Forecast Accuracy: 9.71%\nProductID: PVSL0405040126, Forecast Accuracy: 4.84%\nProductID: PVSL0418011125, Forecast Accuracy: -7.08%\nProductID: PVSL0502010925, Forecast Accuracy: -17.13%\nProductID: PVSL0515021225, Forecast Accuracy: 19.75%\nProductID: PVSL0520051125, Forecast Accuracy: -39.03%\nProductID: PVSL0606031225, Forecast Accuracy: -1.86%\nProductID: PVSL0707051125, Forecast Accuracy: -17.71%\nProductID: PVSL0710061225, Forecast Accuracy: -1.90%\nProductID: PVSL0809010925, Forecast Accuracy: -20.58%\nProductID: PVSL0901040925, Forecast Accuracy: -16.08%\nProductID: PVSL0902031225, Forecast Accuracy: -34.27%\nProductID: PVSL0905021025, Forecast Accuracy: -2.05%\nProductID: PVSL0908011225, Forecast Accuracy: -1.03%\nProductID: PVSL0118010126, Forecast Accuracy: -11.16%\nProductID: PVSL0132040126, Forecast Accuracy: -20.70%\nProductID: PVSL0208051025, Forecast Accuracy: -20.06%\nProductID: PVSL0301020925, Forecast Accuracy: -19.33%\nProductID: PVSL0314030925, Forecast Accuracy: 11.83%\nProductID: PVSL0501020925, Forecast Accuracy: -23.74%\nProductID: PVSL0603061025, Forecast Accuracy: -13.66%\nProductID: PVSL0802020925, Forecast Accuracy: -14.44%\nProductID: PVSL0901041025, Forecast Accuracy: -13.92%\nProductID: PVSL0132050126, Forecast Accuracy: 3.30%\nProductID: PVSL0213051225, Forecast Accuracy: 5.34%\nProductID: PVSL0514030126, Forecast Accuracy: -12.79%\nProductID: PVSL0902041125, Forecast Accuracy: -3.41%\nProductID: PVSL0309041125, Forecast Accuracy: -23.64%\nProductID: PVSL0410051125, Forecast Accuracy: -29.88%\nProductID: PVSL0101011125, Forecast Accuracy: 17.18%\nProductID: PVSL0101051125, Forecast Accuracy: 6.92%\nProductID: PVSL0102040126, Forecast Accuracy: 20.19%\nProductID: PVSL0103051025, Forecast Accuracy: -6.95%\nProductID: PVSL0105021025, Forecast Accuracy: 15.23%\nProductID: PVSL0106020126, Forecast Accuracy: -2.67%\nProductID: PVSL0107031025, Forecast Accuracy: 9.31%\nProductID: PVSL0108031025, Forecast Accuracy: -15.05%\nProductID: PVSL0109040126, Forecast Accuracy: -45.53%\nProductID: PVSL0111021025, Forecast Accuracy: -7.61%\nProductID: PVSL0112010925, Forecast Accuracy: 14.81%\nProductID: PVSL0112050925, Forecast Accuracy: -9.19%\nProductID: PVSL0113051125, Forecast Accuracy: 2.68%\nProductID: PVSL0114051125, Forecast Accuracy: -19.88%\nProductID: PVSL0115041125, Forecast Accuracy: 9.39%\nProductID: PVSL0116041125, Forecast Accuracy: -18.47%\nProductID: PVSL0118040126, Forecast Accuracy: -8.58%\nProductID: PVSL0119030925, Forecast Accuracy: -47.77%\nProductID: PVSL0120040126, Forecast Accuracy: -8.96%\nProductID: PVSL0121050925, Forecast Accuracy: -12.03%\nProductID: PVSL0122051125, Forecast Accuracy: -8.64%\nProductID: PVSL0125011125, Forecast Accuracy: 7.66%\nProductID: PVSL0125051125, Forecast Accuracy: -0.24%\nProductID: PVSL0126041025, Forecast Accuracy: -9.58%\nProductID: PVSL0127041025, Forecast Accuracy: -3.52%\nProductID: PVSL0128041225, Forecast Accuracy: -30.03%\nProductID: PVSL0130021025, Forecast Accuracy: 0.49%\nProductID: PVSL0131031025, Forecast Accuracy: -12.64%\nProductID: PVSL0132020126, Forecast Accuracy: -14.16%\nProductID: PVSL0133031025, Forecast Accuracy: -11.73%\nProductID: PVSL0135040925, Forecast Accuracy: -2.06%\nProductID: PVSL0138011225, Forecast Accuracy: 1.88%\nProductID: PVSL0138061225, Forecast Accuracy: 11.19%\nProductID: PVSL0140040925, Forecast Accuracy: 11.19%\nProductID: PVSL0141041125, Forecast Accuracy: 14.14%\nProductID: PVSL0142040126, Forecast Accuracy: -14.55%\nProductID: PVSL0202031125, Forecast Accuracy: -8.25%\nProductID: PVSL0203010126, Forecast Accuracy: -7.37%ProductID: PVSL0906020925, Forecast Accuracy: -29.20%\nProductID: PVSL0906041125, Forecast Accuracy: -23.84%\nProductID: PVSL0906051225, Forecast Accuracy: 9.27%\nProductID: PVSL0907021025, Forecast Accuracy: -0.42%\nProductID: PVSL0907021225, Forecast Accuracy: -17.15%\nProductID: PVSL0907040925, Forecast Accuracy: 13.29%\nProductID: PVSL0908010126, Forecast Accuracy: -27.33%\nProductID: PVSL0908021225, Forecast Accuracy: -30.05%\nProductID: PVSL0908031025, Forecast Accuracy: -2.44%\nProductID: PVSL0908041025, Forecast Accuracy: -66.12%\nProductID: PVSL0908061225, Forecast Accuracy: 4.55%\nProductID: PVSL0909031025, Forecast Accuracy: -9.69%\nProductID: PVSL0909041025, Forecast Accuracy: -14.48%\nProductID: PVSL0910011025, Forecast Accuracy: -22.29%\nProductID: PVSL0910040925, Forecast Accuracy: -11.67%\nProductID: PVSL0910041225, Forecast Accuracy: 30.97%\nProductID: PVSL0910060925, Forecast Accuracy: -43.86%\nProductID: PVSL0105041025, Forecast Accuracy: 11.13%\nProductID: PVSL0110020126, Forecast Accuracy: -22.58%\nProductID: PVSL0114011125, Forecast Accuracy: -17.34%\nProductID: PVSL0116011125, Forecast Accuracy: -1.30%\nProductID: PVSL0120010126, Forecast Accuracy: 19.79%\nProductID: PVSL0124010925, Forecast Accuracy: -7.19%\nProductID: PVSL0127021025, Forecast Accuracy: -49.50%\nProductID: PVSL0130051025, Forecast Accuracy: 11.08%\nProductID: PVSL0136010925, Forecast Accuracy: 8.97%\nProductID: PVSL0139020126, Forecast Accuracy: 7.75%\nProductID: PVSL0201021025, Forecast Accuracy: 1.25%\nProductID: PVSL0203030126, Forecast Accuracy: -7.84%\nProductID: PVSL0204031225, Forecast Accuracy: -29.20%\nProductID: PVSL0207051125, Forecast Accuracy: -11.52%\nProductID: PVSL0209041225, Forecast Accuracy: -14.60%\nProductID: PVSL0213061225, Forecast Accuracy: -5.30%\nProductID: PVSL0217041225, Forecast Accuracy: 10.86%\nProductID: PVSL0301010925, Forecast Accuracy: -7.76%\nProductID: PVSL0302061125, Forecast Accuracy: 1.60%\nProductID: PVSL0306041025, Forecast Accuracy: 1.56%\nProductID: PVSL0311031025, Forecast Accuracy: 2.45%\nProductID: PVSL0313061025, Forecast Accuracy: 24.29%\nProductID: PVSL0317021125, Forecast Accuracy: -10.06%\nProductID: PVSL0320011225, Forecast Accuracy: -15.24%\nProductID: PVSL0403030126, Forecast Accuracy: 8.07%\nProductID: PVSL0406061125, Forecast Accuracy: -0.60%\nProductID: PVSL0411011225, Forecast Accuracy: 24.34%\nProductID: PVSL0412021025, Forecast Accuracy: 0.68%\nProductID: PVSL0414011125, Forecast Accuracy: -24.59%\nProductID: PVSL0420060925, Forecast Accuracy: -18.83%\nProductID: PVSL0502050925, Forecast Accuracy: -0.48%\nProductID: PVSL0504021125, Forecast Accuracy: -4.16%\nProductID: PVSL0506041225, Forecast Accuracy: -6.84%\nProductID: PVSL0509031125, Forecast Accuracy: 1.66%\nProductID: PVSL0513011225, Forecast Accuracy: -10.59%\nProductID: PVSL0516041025, Forecast Accuracy: -18.66%\nProductID: PVSL0518021025, Forecast Accuracy: 0.74%\nProductID: PVSL0519031125, Forecast Accuracy: -28.73%\nProductID: PVSL0603011025, Forecast Accuracy: -18.91%\nProductID: PVSL0604061025, Forecast Accuracy: -11.86%\nProductID: PVSL0607041225, Forecast Accuracy: -19.90%\nProductID: PVSL0609021125, Forecast Accuracy: -37.55%\nProductID: PVSL0701050925, Forecast Accuracy: 14.35%\nProductID: PVSL0703021025, Forecast Accuracy: -13.62%\nProductID: PVSL0705050925, Forecast Accuracy: -51.61%\nProductID: PVSL0706061225, Forecast Accuracy: 2.43%\nProductID: PVSL0709011125, Forecast Accuracy: -0.24%\nProductID: PVSL0805011025, Forecast Accuracy: -37.21%\nProductID: PVSL0810020126, Forecast Accuracy: 4.65%\nProductID: PVSL0901060126, Forecast Accuracy: -6.83%\nProductID: PVSL0903010925, Forecast Accuracy: -1.03%\nProductID: PVSL0904011025, Forecast Accuracy: -5.30%\nProductID: PVSL0905030126, Forecast Accuracy: -4.32%\nProductID: PVSL0906011125, Forecast Accuracy: -9.04%\nProductID: PVSL0907010126, Forecast Accuracy: -32.82%\nProductID: PVSL0907041225, Forecast Accuracy: -24.44%\nProductID: PVSL0908050925, Forecast Accuracy: -50.08%\nProductID: PVSL0909061025, Forecast Accuracy: -22.95%\nProductID: PVSL0910050126, Forecast Accuracy: 8.31%\nProductID: PVSL0113011125, Forecast Accuracy: -21.23%\nProductID: PVSL0120050126, Forecast Accuracy: -19.53%\nProductID: PVSL0128011225, Forecast Accuracy: 6.07%\nProductID: PVSL0134011125, Forecast Accuracy: -19.95%\nProductID: PVSL0141031125, Forecast Accuracy: -3.50%\nProductID: PVSL0206011225, Forecast Accuracy: 2.28%\nProductID: PVSL0214021125, Forecast Accuracy: 8.65%\nProductID: PVSL0306051025, Forecast Accuracy: -9.85%\nProductID: PVSL0313021025, Forecast Accuracy: -18.93%\nProductID: PVSL0315061025, Forecast Accuracy: 9.38%\nProductID: PVSL0401020925, Forecast Accuracy: -5.78%\nProductID: PVSL0412031025, Forecast Accuracy: -6.86%\nProductID: PVSL0420050925, Forecast Accuracy: -5.88%\nProductID: PVSL0508060925, Forecast Accuracy: 3.28%\nProductID: PVSL0520021125, Forecast Accuracy: -22.11%\nProductID: PVSL0606011225, Forecast Accuracy: 7.08%\nProductID: PVSL0704031125, Forecast Accuracy: 6.76%\nProductID: PVSL0709051125, Forecast Accuracy: -30.88%\nProductID: PVSL0806041025, Forecast Accuracy: -4.19%\nProductID: PVSL0901010925, Forecast Accuracy: -11.86%\nProductID: PVSL0902020925, Forecast Accuracy: -18.82%\nProductID: PVSL0904041225, Forecast Accuracy: -2.39%\nProductID: PVSL0907060925, Forecast Accuracy: 4.58%\nProductID: PVSL0111011025, Forecast Accuracy: -3.04%\nProductID: PVSL0126071025, Forecast Accuracy: 1.28%\nProductID: PVSL0206021225, Forecast Accuracy: -21.97%\nProductID: PVSL0307031225, Forecast Accuracy: 10.09%\nProductID: PVSL0412041025, Forecast Accuracy: -22.09%\nProductID: PVSL0415020126, Forecast Accuracy: -8.04%\nProductID: PVSL0603041025, Forecast Accuracy: -9.83%\nProductID: PVSL0704011125, Forecast Accuracy: -19.11%\nProductID: PVSL0810040126, Forecast Accuracy: -3.20%\nProductID: PVSL0201011025, Forecast Accuracy: -2.02%\nProductID: PVSL0311021025, Forecast Accuracy: -9.80%\nProductID: PVSL0901021025, Forecast Accuracy: -20.77%\nProductID: PVSL0209031225, Forecast Accuracy: -31.61%\nProductID: PVSL0902040126, Forecast Accuracy: -42.26%\nProductID: PVSL0909050126, Forecast Accuracy: -12.03%\nProductID: PVSL0101041125, Forecast Accuracy: 16.66%\nProductID: PVSL0102030126, Forecast Accuracy: 37.08%\nProductID: PVSL0103021025, Forecast Accuracy: 0.90%\nProductID: PVSL0103041025, Forecast Accuracy: -4.54%\nProductID: PVSL0105011025, Forecast Accuracy: 19.91%\nProductID: PVSL0106010126, Forecast Accuracy: 7.26%\nProductID: PVSL0106040126, Forecast Accuracy: 14.83%\nProductID: PVSL0107041025, Forecast Accuracy: -2.94%\nProductID: PVSL0108051025, Forecast Accuracy: -18.45%\nProductID: PVSL0109050126, Forecast Accuracy: 0.40%\nProductID: PVSL0111041025, Forecast Accuracy: -10.84%\nProductID: PVSL0112030925, Forecast Accuracy: 0.03%\nProductID: PVSL0113031125, Forecast Accuracy: 25.16%\nProductID: PVSL0114041125, Forecast Accuracy: 6.28%\nProductID: PVSL0115031125, Forecast Accuracy: -26.71%\nProductID: PVSL0116031125, Forecast Accuracy: 3.28%\nProductID: PVSL0118020126, Forecast Accuracy: 3.38%\nProductID: PVSL0119020925, Forecast Accuracy: -13.74%\nProductID: PVSL0120030126, Forecast Accuracy: -3.22%\nProductID: PVSL0121040925, Forecast Accuracy: -14.24%\nProductID: PVSL0122031125, Forecast Accuracy: 12.53%\nProductID: PVSL0123040925, Forecast Accuracy: -6.09%\nProductID: PVSL0125031125, Forecast Accuracy: -3.43%\nProductID: PVSL0126011025, Forecast Accuracy: 7.82%\nProductID: PVSL0126051025, Forecast Accuracy: -5.37%\nProductID: PVSL0127071025, Forecast Accuracy: 1.39%\nProductID: PVSL0128051225, Forecast Accuracy: -17.18%\nProductID: PVSL0130031025, Forecast Accuracy: -2.98%\nProductID: PVSL0131041025, Forecast Accuracy: -16.80%\nProductID: PVSL0132030126, Forecast Accuracy: 0.79%\nProductID: PVSL0135010925, Forecast Accuracy: 14.64%\nProductID: PVSL0136040925, Forecast Accuracy: 3.90%\nProductID: PVSL0138031225, Forecast Accuracy: -5.96%\nProductID: PVSL0139040126, Forecast Accuracy: -18.43%\nProductID: PVSL0140050925, Forecast Accuracy: -12.84%\nProductID: PVSL0141061125, Forecast Accuracy: -31.94%\nProductID: PVSL0142060126, Forecast Accuracy: -5.36%\nProductID: PVSL0202041125, Forecast Accuracy: 8.72%\nProductID: PVSL0203020126, Forecast Accuracy: -7.13%\nProductID: PVSL0204051225, Forecast Accuracy: -7.08%\nProductID: PVSL0205031125, Forecast Accuracy: 7.23%","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_36/2972944833.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;31m# Run the forecasting in parallel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[0mproduct_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msummary_daily_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"ProductID\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 200\u001b[0;31m \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mParallel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdelayed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess_product_all\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msummary_daily_df\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mpid\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mproduct_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    201\u001b[0m \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mr\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresults\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mr\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   2070\u001b[0m         \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2071\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2072\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturn_generator\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2073\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2074\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_get_outputs\u001b[0;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[1;32m   1680\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1681\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1682\u001b[0;31m                 \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_retrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1683\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1684\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mGeneratorExit\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_retrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1798\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mTASK_PENDING\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1799\u001b[0m                 ):\n\u001b[0;32m-> 1800\u001b[0;31m                     \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1801\u001b[0m                     \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1802\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":45},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.preprocessing import MinMaxScaler, LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom joblib import Parallel, delayed\nimport ast\n\n# Use a GPU if available\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# ----------------------------\n# 1. Load Data\n# ----------------------------\n# Load the bills and products datasets\nbills_df = pd.read_csv(\"/kaggle/input/sales-prediction/bills (1).csv\")\nproducts_df = pd.read_csv(\"/kaggle/input/sales-prediction/products.csv\")\n\n# ----------------------------\n# 2. Preprocess Bills Data\n# ----------------------------\n# Clean column names by stripping whitespace\nbills_df.columns = bills_df.columns.str.strip()\n\n# Convert Product_IDs and Quantities from string representations to lists\nbills_df[\"Product_IDs\"] = bills_df[\"Product_IDs\"].apply(ast.literal_eval)\nbills_df[\"Quantities\"] = bills_df[\"Quantities\"].apply(ast.literal_eval)\n\n\n# Explode the DataFrame to have one row per product-quantity pair\nbills_df = bills_df.explode([\"Product_IDs\", \"Quantities\"])\nbills_df.rename(columns={\"Product_IDs\": \"ProductID\", \"Quantities\": \"Quantity_Sold\"}, inplace=True)\n\n\nbills_df[\"ProductID\"] = bills_df[\"ProductID\"].astype(str).str.strip()\nbills_df[\"Quantity_Sold\"] = bills_df[\"Quantity_Sold\"].astype(int)\n\nbills_df[\"Date\"] = pd.to_datetime(bills_df[\"Date_Time\"]).dt.date\n\n# ----------------------------\n# 3. Preprocess Products Data\n# ----------------------------\n# Clean up column names and ProductID\nproducts_df.columns = products_df.columns.str.strip()\nproducts_df.rename(columns={'Product Code': 'ProductID'}, inplace=True)\nproducts_df[\"ProductID\"] = products_df[\"ProductID\"].astype(str).str.strip()\n\n# --- Find ProductIDs in bills_df that are not in products_df ---\nunmatched_ids = set(bills_df[\"ProductID\"]) - set(products_df[\"ProductID\"])\nif unmatched_ids:\n    print(f\"Found {len(unmatched_ids)} ProductIDs in the bills data that are not in the product data.\")\n# ----------------------------------------------------------------\n\n# ----------------------------\n# 4. Merge DataFrames\n# ----------------------------\n# Merge the bills and products dataframes\nmerged_df = bills_df.merge(products_df[['ProductID', 'Brand Name']], on='ProductID', how='left')\nmerged_df['Brand Name'] = merged_df['Brand Name'].fillna('Unknown')\n\n# ----------------------------\n# 5. Create Daily Sales Summary and Feature Engineering\n# ----------------------------\nsummary_daily_df = (\n    merged_df.groupby([\"Date\", \"ProductID\", \"Brand Name\"])[\"Quantity_Sold\"]\n    .sum()\n    .reset_index()\n    .rename(columns={\"Quantity_Sold\": \"Total_Quantity_Sold\"})\n)\n# --- Feature Engineering ---\nsummary_daily_df['Date'] = pd.to_datetime(summary_daily_df['Date'])\nsummary_daily_df['dayofweek'] = summary_daily_df['Date'].dt.dayofweek\nsummary_daily_df['dayofyear'] = summary_daily_df['Date'].dt.dayofyear\nsummary_daily_df['month'] = summary_daily_df['Date'].dt.month\nsummary_daily_df['weekofyear'] = summary_daily_df['Date'].dt.isocalendar().week.astype(int)\n\n\n# ----------------------------\n# 6. PyTorch Dataset for Time Series\n# ----------------------------\nclass TimeSeriesDataset(Dataset):\n    def __init__(self, features, targets, seq_len):\n        self.features = features\n        self.targets = targets\n        self.seq_len = seq_len\n\n    def __len__(self):\n        return len(self.features) - self.seq_len\n\n    def __getitem__(self, idx):\n        return (\n            torch.tensor(self.features[idx:idx+self.seq_len], dtype=torch.float32),\n            torch.tensor(self.targets[idx+self.seq_len], dtype=torch.float32)\n        )\n\n# ----------------------------\n# 7. LSTM Model\n# ----------------------------\nclass LSTMModel(nn.Module):\n    def __init__(self, input_dim, hidden_dim=128, num_layers=3):\n        super().__init__()\n        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True, dropout=0.3)\n        self.fc = nn.Linear(hidden_dim, 1)\n\n    def forward(self, x):\n        lstm_out, _ = self.lstm(x)\n        output = self.fc(lstm_out[:, -1, :])\n        return output\n\n# ----------------------------\n# 8. Training, Forecasting, and Accuracy Check\n# ----------------------------\ndef train_and_forecast_with_accuracy(series_df, future=30, seq_len=30, epochs=150, batch_size=64):\n    if len(series_df) < seq_len + 15: # Ensure enough data\n        return np.zeros(future).tolist(), 0.0\n\n    features = series_df[['Total_Quantity_Sold', 'dayofweek', 'dayofyear', 'month', 'weekofyear']].values\n    targets = series_df['Total_Quantity_Sold'].values\n\n    # Train-test split\n    train_features, test_features, train_targets, test_targets = train_test_split(\n        features, targets, test_size=0.2, shuffle=False\n    )\n\n    scaler = MinMaxScaler()\n    train_features_scaled = scaler.fit_transform(train_features)\n    \n    train_dataset = TimeSeriesDataset(train_features_scaled, train_targets, seq_len)\n    if len(train_dataset) == 0:\n        return np.zeros(future).tolist(), 0.0\n\n    train_loader = DataLoader(train_dataset, batch_size=min(batch_size, len(train_dataset)), shuffle=True)\n    \n    model = LSTMModel(input_dim=train_features.shape[1]).to(device)\n    loss_fn = nn.MSELoss()\n    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n\n    # Training\n    for epoch in range(epochs):\n        for xb, yb in train_loader:\n            xb, yb = xb.to(device), yb.to(device)\n            pred = model(xb).squeeze(-1)\n            loss = loss_fn(pred, yb)\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n\n    # Accuracy check\n    model.eval()\n    test_inputs = scaler.transform(features[-seq_len - len(test_features):-len(test_features)])\n    test_actuals = test_targets\n    \n    test_preds = []\n    with torch.no_grad():\n        current_seq = torch.tensor(test_inputs, dtype=torch.float32).unsqueeze(0).to(device)\n        for _ in range(len(test_actuals)):\n            out = model(current_seq).item()\n            test_preds.append(out)\n            # Create the next input sequence\n            last_features = current_seq[:, -1, 1:].cpu().numpy() # Get the time features\n            new_row = np.hstack(([out], last_features[0])).reshape(1, -1)\n            new_row_scaled = scaler.transform(new_row)\n            new_seq_val = torch.tensor(new_row_scaled, dtype=torch.float32).unsqueeze(0).to(device)\n            current_seq = torch.cat([current_seq[:, 1:, :], new_seq_val], dim=1)\n\n    # Inverse transform only the sales predictions\n    sales_scaler = MinMaxScaler()\n    sales_scaler.min_, sales_scaler.scale_ = scaler.min_[0], scaler.scale_[0]\n    test_preds = sales_scaler.inverse_transform(np.array(test_preds).reshape(-1, 1)).flatten()\n    \n    mape = np.mean(np.abs((test_actuals - test_preds) / np.maximum(test_actuals, 1))) * 100\n    accuracy = 100 - mape\n\n    # Forecasting\n    model.eval()\n    input_seq_features = scaler.transform(features[-seq_len:])\n    future_preds = []\n    with torch.no_grad():\n        current_seq = torch.tensor(input_seq_features, dtype=torch.float32).unsqueeze(0).to(device)\n        for _ in range(future):\n            out = model(current_seq).item()\n            future_preds.append(out)\n            last_features = current_seq[:, -1, 1:].cpu().numpy() # Get the time features\n            new_row = np.hstack(([out], last_features[0])).reshape(1, -1)\n            new_row_scaled = scaler.transform(new_row)\n            new_seq_val = torch.tensor(new_row_scaled, dtype=torch.float32).unsqueeze(0).to(device)\n            current_seq = torch.cat([current_seq[:, 1:, :], new_seq_val], dim=1)\n            \n    future_preds = sales_scaler.inverse_transform(np.array(future_preds).reshape(-1, 1)).flatten()\n    future_preds = np.maximum(future_preds, 0)\n    \n    return np.round(future_preds).tolist(), accuracy\n\n# ----------------------------\n# 9. Generate Forecasts per Product\n# ----------------------------\ndef process_product_all(product_id, df, seq_len=30):\n    group = df[df[\"ProductID\"] == product_id].sort_values(\"Date\")\n    \n    if len(group) < 2:\n        return None\n\n    daily_forecast, accuracy = train_and_forecast_with_accuracy(group, future=30, seq_len=seq_len)\n    weekly_forecast, _ = train_and_forecast_with_accuracy(group, future=7, seq_len=seq_len)\n    monthly_forecast, _ = train_and_forecast_with_accuracy(group, future=30, seq_len=seq_len)\n    \n    print(f\"ProductID: {product_id}, Forecast Accuracy: {accuracy:.2f}%\")\n    \n    return {\n        \"ProductID\": product_id,\n        \"Brand Name\": group[\"Brand Name\"].iloc[0],\n        \"Daily_Forecast\": daily_forecast,\n        \"Weekly_Forecast\": weekly_forecast,\n        \"Monthly_Forecast\": monthly_forecast,\n        \"Accuracy (%)\": accuracy\n    }\n\n# Run the forecasting in parallel\nproduct_ids = summary_daily_df[\"ProductID\"].unique()\nresults = Parallel(n_jobs=-1)(delayed(process_product_all)(pid, summary_daily_df) for pid in product_ids)\nresults = [r for r in results if r is not None]\n\n# Save the product-level forecasts\nforecast_df = pd.DataFrame(results)\nforecast_df.to_csv(\"product_forecasts_with_accuracy.csv\", index=False)\nprint(\"✅ Product-level forecasts saved to product_forecasts_with_accuracy.csv\")\n\n# ----------------------------\n# 10. Aggregate Forecasts by Brand\n# ----------------------------\ndef aggregate_brand_forecast(forecast_df, col_name, horizon_name):\n    expanded = []\n    for _, row in forecast_df.iterrows():\n        brand_col = row['Brand Name'] if pd.notnull(row['Brand Name']) else 'Unknown'\n        for i, val in enumerate(row[col_name]):\n            expanded.append({\"Brand\": brand_col, horizon_name: i + 1, \"Forecast_Qty\": val})\n\n    expanded_df = pd.DataFrame(expanded)\n    brand_forecast = expanded_df.groupby([\"Brand\", horizon_name], as_index=False)[\"Forecast_Qty\"].sum()\n    brand_forecast[\"Forecast_Qty\"] = brand_forecast[\"Forecast_Qty\"].round()\n    return brand_forecast\n\n# Generate and save brand-level forecasts\nbrand_daily_forecast = aggregate_brand_forecast(forecast_df, \"Daily_Forecast\", \"Day\")\nbrand_weekly_forecast = aggregate_brand_forecast(forecast_df, \"Weekly_Forecast\", \"Week\")\nbrand_monthly_forecast = aggregate_brand_forecast(forecast_df, \"Month_Forecast\", \"Month\")\n\nbrand_daily_forecast.to_csv(\"brand_daily_forecast.csv\", index=False)\nbrand_weekly_forecast.to_csv(\"brand_weekly_forecast.csv\", index=False)\nbrand_monthly_forecast.to_csv(\"brand_monthly_forecast.csv\", index=False)\nprint(\"✅ Brand-level forecasts saved to brand_daily_forecast.csv, brand_weekly_forecast.csv, and brand_monthly_forecast.csv\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-13T08:23:17.892288Z","iopub.execute_input":"2025-09-13T08:23:17.892824Z","iopub.status.idle":"2025-09-13T10:05:38.189851Z","shell.execute_reply.started":"2025-09-13T08:23:17.892801Z","shell.execute_reply":"2025-09-13T10:05:38.188692Z"}},"outputs":[{"name":"stdout","text":"Found 1078 ProductIDs in the bills data that are not in the product data.\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_36/2601619952.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    220\u001b[0m \u001b[0;31m# Run the forecasting in parallel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m \u001b[0mproduct_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msummary_daily_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"ProductID\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 222\u001b[0;31m \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mParallel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdelayed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess_product_all\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msummary_daily_df\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mpid\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mproduct_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    223\u001b[0m \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mr\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresults\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mr\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   2070\u001b[0m         \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2071\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2072\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturn_generator\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2073\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2074\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_get_outputs\u001b[0;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[1;32m   1680\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1681\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1682\u001b[0;31m                 \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_retrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1683\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1684\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mGeneratorExit\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_retrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1798\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mTASK_PENDING\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1799\u001b[0m                 ):\n\u001b[0;32m-> 1800\u001b[0;31m                     \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1801\u001b[0m                     \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1802\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":46},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport lightgbm as lgb\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom joblib import Parallel, delayed\nimport ast\n\n# ----------------------------\n# 1. Load Data\n# ----------------------------\n# Load the bills and products datasets\nbills_df = pd.read_csv(\"/kaggle/input/sales-prediction/bills (1).csv\")\nproducts_df = pd.read_csv(\"/kaggle/input/sales-prediction/products.csv\")\n\n# ----------------------------\n# 2. Preprocess Data\n# ----------------------------\n# Clean column names by stripping whitespace\nbills_df.columns = bills_df.columns.str.strip()\nproducts_df.columns = products_df.columns.str.strip()\n\n# Clean and process bills data\nbills_df[\"Product_IDs\"] = bills_df[\"Product_IDs\"].apply(ast.literal_eval)\nbills_df[\"Quantities\"] = bills_df[\"Quantities\"].apply(ast.literal_eval)\nbills_df = bills_df.explode([\"Product_IDs\", \"Quantities\"])\nbills_df.rename(columns={\"Product_IDs\": \"ProductID\", \"Quantities\": \"Quantity_Sold\"}, inplace=True)\nbills_df[\"ProductID\"] = bills_df[\"ProductID\"].astype(str).str.strip()\nbills_df[\"Quantity_Sold\"] = bills_df[\"Quantity_Sold\"].astype(int)\nbills_df[\"Date\"] = pd.to_datetime(bills_df[\"Date_Time\"]).dt.date\n\n# Clean and process products data\nproducts_df.rename(columns={'Product Code': 'ProductID'}, inplace=True)\nproducts_df[\"ProductID\"] = products_df[\"ProductID\"].astype(str).str.strip()\n\n# ----------------------------\n# 3. Merge DataFrames\n# ----------------------------\n# Merge and handle products that are in bills but not in the product list\nmerged_df = bills_df.merge(products_df[['ProductID', 'Brand Name']], on='ProductID', how='left')\nmerged_df['Brand Name'] = merged_df['Brand Name'].fillna('Unknown')\n\n# ----------------------------\n# 4. Create Daily Sales Summary\n# ----------------------------\ndaily_sales = (\n    merged_df.groupby([\"Date\", \"ProductID\", \"Brand Name\"])[\"Quantity_Sold\"]\n    .sum()\n    .reset_index()\n    .rename(columns={\"Quantity_Sold\": \"Total_Quantity_Sold\"})\n)\ndaily_sales['Date'] = pd.to_datetime(daily_sales['Date'])\n\n# ----------------------------\n# 5. Feature Engineering Function\n# ----------------------------\ndef create_features(df):\n    \"\"\"Create time series features based on the date index.\"\"\"\n    df = df.copy()\n    df['dayofweek'] = df['Date'].dt.dayofweek\n    df['dayofyear'] = df['Date'].dt.dayofyear\n    df['month'] = df['Date'].dt.month\n    df['weekofyear'] = df['Date'].dt.isocalendar().week.astype(int)\n    \n    # Lag and Rolling Features\n    for lag in [1, 2, 3, 7]:\n        df[f'lag_{lag}'] = df['Total_Quantity_Sold'].shift(lag)\n    for window in [7, 14]:\n        df[f'rolling_mean_{window}'] = df['Total_Quantity_Sold'].shift(1).rolling(window).mean()\n        \n    return df.fillna(-1) # Fill NaNs created by lags/rolling windows\n\n# ----------------------------\n# 6. Training and Forecasting Function\n# ----------------------------\ndef train_and_forecast_lgbm(product_df, future_days=30):\n    if len(product_df) < 30: # Need enough data to create features and train\n        return np.zeros(future_days).tolist(), 0.0\n\n    # Create a full date range to handle days with no sales\n    full_date_range = pd.date_range(start=product_df['Date'].min(), end=product_df['Date'].max())\n    product_df = product_df.set_index('Date').reindex(full_date_range).fillna(0).reset_index()\n    product_df.rename(columns={'index': 'Date'}, inplace=True)\n\n    # Create features\n    data = create_features(product_df)\n    \n    # Split data for training and accuracy check\n    train_data, test_data = train_test_split(data, test_size=0.2, shuffle=False)\n    \n    features = ['dayofweek', 'dayofyear', 'month', 'weekofyear', \n                'lag_1', 'lag_2', 'lag_3', 'lag_7', 'rolling_mean_7', 'rolling_mean_14']\n    target = 'Total_Quantity_Sold'\n\n    X_train, y_train = train_data[features], train_data[target]\n    X_test, y_test = test_data[features], test_data[target]\n\n    # Train the LightGBM model\n    model = lgb.LGBMRegressor(\n        objective='regression_l1',\n        n_estimators=1000,\n        learning_rate=0.05,\n        num_leaves=31,\n        max_depth=-1,\n        min_child_samples=20,\n        subsample=0.8,\n        colsample_bytree=0.8,\n        random_state=42,\n        n_jobs=-1\n    )\n    model.fit(X_train, y_train, eval_set=[(X_test, y_test)], \n              callbacks=[lgb.early_stopping(100, verbose=False)])\n\n    # Accuracy check\n    preds = model.predict(X_test)\n    mape = np.mean(np.abs((y_test - preds) / np.maximum(y_test, 1))) * 100\n    accuracy = 100 - mape\n\n    # Forecasting\n    future_preds = []\n    last_known_data = data.tail(14).copy() # Use last 14 days to generate features\n\n    for _ in range(future_days):\n        last_date = last_known_data['Date'].iloc[-1]\n        next_date = last_date + pd.Timedelta(days=1)\n        \n        # Create a new row for the next day's prediction\n        next_day_features_df = pd.DataFrame([{'Date': next_date, 'Total_Quantity_Sold': 0}])\n        \n        # Create features for the next day\n        combined_data = pd.concat([last_known_data, next_day_features_df], ignore_index=True)\n        combined_features = create_features(combined_data)\n        \n        next_day_features = combined_features[features].tail(1)\n        \n        # Predict\n        prediction = model.predict(next_day_features)[0]\n        prediction = max(0, prediction) # Ensure non-negative sales\n        \n        # Add the prediction back for the next loop\n        new_row = pd.DataFrame([{'Date': next_date, 'Total_Quantity_Sold': prediction}])\n        last_known_data = pd.concat([last_known_data, new_row], ignore_index=True)\n        future_preds.append(prediction)\n\n    return np.round(future_preds).tolist(), accuracy\n\n# ----------------------------\n# 7. Generate Forecasts per Product\n# ----------------------------\ndef process_product(product_id, df):\n    group = df[df[\"ProductID\"] == product_id].sort_values(\"Date\")\n    \n    if len(group) < 2:\n        return None\n\n    daily_forecast, accuracy = train_and_forecast_lgbm(group.copy(), future_days=30)\n    weekly_forecast = daily_forecast[:7]\n    monthly_forecast = daily_forecast\n    \n    print(f\"ProductID: {product_id}, Brand: {group['Brand Name'].iloc[0]}, Forecast Accuracy: {accuracy:.2f}%\")\n    \n    return {\n        \"ProductID\": product_id,\n        \"Brand Name\": group[\"Brand Name\"].iloc[0],\n        \"Daily_Forecast\": daily_forecast,\n        \"Weekly_Forecast\": weekly_forecast,\n        \"Monthly_Forecast\": monthly_forecast,\n        \"Accuracy (%)\": accuracy\n    }\n\n# Run forecasting in parallel\nproduct_ids = daily_sales[\"ProductID\"].unique()\nresults = Parallel(n_jobs=-1)(delayed(process_product)(pid, daily_sales) for pid in product_ids)\nresults = [r for r in results if r is not None]\n\n# Save forecasts\nforecast_df = pd.DataFrame(results)\nforecast_df.to_csv(\"product_forecasts_with_accuracy.csv\", index=False)\nprint(\"\\n✅ Product-level forecasts saved to product_forecasts_with_accuracy.csv\")\n\n# ----------------------------\n# 8. Aggregate Forecasts by Brand\n# ----------------------------\ndef aggregate_brand_forecast(forecast_df, col_name, horizon_name):\n    expanded = []\n    for _, row in forecast_df.iterrows():\n        brand_col = row['Brand Name'] if pd.notnull(row['Brand Name']) else 'Unknown'\n        for i, val in enumerate(row[col_name]):\n            expanded.append({\"Brand\": brand_col, horizon_name: i + 1, \"Forecast_Qty\": val})\n\n    expanded_df = pd.DataFrame(expanded)\n    brand_forecast = expanded_df.groupby([\"Brand\", horizon_name], as_index=False)[\"Forecast_Qty\"].sum()\n    brand_forecast[\"Forecast_Qty\"] = brand_forecast[\"Forecast_Qty\"].round()\n    return brand_forecast\n\n# Generate and save brand-level forecasts\nbrand_daily_forecast = aggregate_brand_forecast(forecast_df, \"Daily_Forecast\", \"Day\")\nbrand_weekly_forecast = aggregate_brand_forecast(forecast_df, \"Weekly_Forecast\", \"Week\")\nbrand_monthly_forecast = aggregate_brand_forecast(forecast_df, \"Monthly_Forecast\", \"Month\")\n\nbrand_daily_forecast.to_csv(\"brand_daily_forecast.csv\", index=False)\nbrand_weekly_forecast.to_csv(\"brand_weekly_forecast.csv\", index=False)\nbrand_monthly_forecast.to_csv(\"brand_monthly_forecast.csv\", index=False)\nprint(\"✅ Brand-level forecasts saved.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-13T10:05:45.571151Z","iopub.execute_input":"2025-09-13T10:05:45.571440Z","execution_failed":"2025-09-13T16:10:42.816Z"}},"outputs":[{"name":"stdout","text":"[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.056199 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 853\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 23.000000\nProductID: PVSL0101021125, Brand: Unknown, Forecast Accuracy: -10.46%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.070185 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 857\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 23.000000\nProductID: PVSL0102020126, Brand: Unknown, Forecast Accuracy: -33.63%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.059183 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 608\n[LightGBM] [Info] Number of data points in the train set: 896, number of used features: 10\n[LightGBM] [Info] Start training from score 2.000000\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\nProductID: PVSL0103051025, Brand: Unknown, Forecast Accuracy: -77.24%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.047615 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 579\n[LightGBM] [Info] Number of data points in the train set: 898, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0105021025, Brand: Unknown, Forecast Accuracy: -59.83%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.062365 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 592\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\nProductID: PVSL0106020126, Brand: Unknown, Forecast Accuracy: -65.76%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.059070 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 598\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 2.000000\nProductID: PVSL0106040126, Brand: Unknown, Forecast Accuracy: -32.16%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.043044 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 596\n[LightGBM] [Info] Number of data points in the train set: 899, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0107031025, Brand: Unknown, Forecast Accuracy: -68.39%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.068322 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 600\n[LightGBM] [Info] Number of data points in the train set: 899, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0108021025, Brand: Unknown, Forecast Accuracy: -78.60%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.089155 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 591\n[LightGBM] [Info] Number of data points in the train set: 898, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0109050126, Brand: Unknown, Forecast Accuracy: -59.76%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.047250 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 588\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0111031025, Brand: Unknown, Forecast Accuracy: -71.98%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.057867 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 601\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0112020925, Brand: Unknown, Forecast Accuracy: -64.81%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.051682 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 589\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\nProductID: PVSL0112030925, Brand: Unknown, Forecast Accuracy: -56.78%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.059278 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 610\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 2.000000\nProductID: PVSL0113051125, Brand: Unknown, Forecast Accuracy: -24.95%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.086184 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 578\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0114051125, Brand: Unknown, Forecast Accuracy: -60.45%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.047295 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 592\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 2.000000\nProductID: PVSL0115041125, Brand: Unknown, Forecast Accuracy: -12.23%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.065230 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 590\n[LightGBM] [Info] Number of data points in the train set: 899, number of used features: 10\n[LightGBM] [Info] Start training from score 2.000000\nProductID: PVSL0116021125, Brand: Unknown, Forecast Accuracy: -28.25%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.059268 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 592\n[LightGBM] [Info] Number of data points in the train set: 899, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0118020126, Brand: Unknown, Forecast Accuracy: -58.61%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.067131 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 580\n[LightGBM] [Info] Number of data points in the train set: 899, number of used features: 10\n[LightGBM] [Info] Start training from score 2.000000\nProductID: PVSL0119010925, Brand: Unknown, Forecast Accuracy: -38.78%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.065325 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 592\n[LightGBM] [Info] Number of data points in the train set: 899, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0119040925, Brand: Unknown, Forecast Accuracy: -59.93%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.071756 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 598[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.059132 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 857\n[LightGBM] [Info] Number of data points in the train set: 899, number of used features: 10\n[LightGBM] [Info] Start training from score 23.000000\nProductID: PVSL0101041125, Brand: Unknown, Forecast Accuracy: -28.90%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.057996 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 862\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 23.000000\nProductID: PVSL0102040126, Brand: Unknown, Forecast Accuracy: 0.09%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.046741 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 581\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0103011025, Brand: Unknown, Forecast Accuracy: -81.91%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.061179 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 585\n[LightGBM] [Info] Number of data points in the train set: 899, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0103031025, Brand: Unknown, Forecast Accuracy: -66.32%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.067364 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 575\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 2.000000\nProductID: PVSL0105011025, Brand: Unknown, Forecast Accuracy: -41.12%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.067226 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 599\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 2.000000\nProductID: PVSL0106010126, Brand: Unknown, Forecast Accuracy: -34.99%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.063063 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 598\n[LightGBM] [Info] Number of data points in the train set: 898, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0106050126, Brand: Unknown, Forecast Accuracy: -71.32%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.013290 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 600\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\nProductID: PVSL0108031025, Brand: Unknown, Forecast Accuracy: -57.02%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.047320 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 599\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 2.000000\nProductID: PVSL0109040126, Brand: Unknown, Forecast Accuracy: -27.82%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.076002 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 602\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0110040126, Brand: Unknown, Forecast Accuracy: -64.11%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.083377 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 596\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 2.500000\nProductID: PVSL0112010925, Brand: Unknown, Forecast Accuracy: -63.92%\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.015407 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 593\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 2.000000\nProductID: PVSL0112050925, Brand: Unknown, Forecast Accuracy: -33.32%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.062194 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 594\n[LightGBM] [Info] Number of data points in the train set: 898, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0112060925, Brand: Unknown, Forecast Accuracy: -71.50%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.059321 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 591\n[LightGBM] [Info] Number of data points in the train set: 899, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0114021125, Brand: Unknown, Forecast Accuracy: -54.77%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.074146 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 613\n[LightGBM] [Info] Number of data points in the train set: 899, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0115011125, Brand: Unknown, Forecast Accuracy: -76.60%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.060221 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 607\n[LightGBM] [Info] Number of data points in the train set: 899, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0115051125, Brand: Unknown, Forecast Accuracy: -55.10%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.059228 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 588\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0117030126, Brand: Unknown, Forecast Accuracy: -79.44%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.058173 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 599\n[LightGBM] [Info] Number of data points in the train set: 899, number of used features: 10\n[LightGBM] [Info] Start training from score 2.000000\nProductID: PVSL0118040126, Brand: Unknown, Forecast Accuracy: -33.10%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.009436 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 588\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0119030925, Brand: Unknown, Forecast Accuracy: -65.52%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.062166 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 611\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.025321 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 842\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 24.000000\nProductID: PVSL0101011125, Brand: Unknown, Forecast Accuracy: 16.93%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.059328 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 23.000000\nProductID: PVSL0102010126, Brand: Unknown, Forecast Accuracy: -1.77%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.067329 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 870\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 23.000000\nProductID: PVSL0102050126, Brand: Unknown, Forecast Accuracy: -33.30%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.065369 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 592\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 2.000000\nProductID: PVSL0103041025, Brand: Unknown, Forecast Accuracy: -19.91%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.059254 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 587\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0104050126, Brand: Unknown, Forecast Accuracy: -67.13%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.061382 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 590\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 2.000000\nProductID: PVSL0105031025, Brand: Unknown, Forecast Accuracy: -54.16%\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.050865 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 577\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 2.000000\nProductID: PVSL0107021025, Brand: Unknown, Forecast Accuracy: -38.26%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.040270 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 592\n[LightGBM] [Info] Number of data points in the train set: 896, number of used features: 10\n[LightGBM] [Info] Start training from score 2.000000\nProductID: PVSL0107051025, Brand: Unknown, Forecast Accuracy: -33.61%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.059221 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 580\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 2.000000\nProductID: PVSL0109010126, Brand: Unknown, Forecast Accuracy: -28.96%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.061192 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 587\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0109030126, Brand: Unknown, Forecast Accuracy: -73.16%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.059307 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 593\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0111021025, Brand: Unknown, Forecast Accuracy: -62.56%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.057084 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 598\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 2.000000\nProductID: PVSL0111051025, Brand: Unknown, Forecast Accuracy: -39.50%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.059130 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 581\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 2.000000\nProductID: PVSL0113031125, Brand: Unknown, Forecast Accuracy: -27.08%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.071262 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 590\n[LightGBM] [Info] Number of data points in the train set: 899, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\nProductID: PVSL0114061125, Brand: Unknown, Forecast Accuracy: -77.58%\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.045863 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 603\n[LightGBM] [Info] Number of data points in the train set: 899, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0116031125, Brand: Unknown, Forecast Accuracy: -52.10%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.058678 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 581\n[LightGBM] [Info] Number of data points in the train set: 899, number of used features: 10\n[LightGBM] [Info] Start training from score 2.000000\nProductID: PVSL0118050126, Brand: Unknown, Forecast Accuracy: -26.86%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.047337 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 603\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0119050925, Brand: Unknown, Forecast Accuracy: -66.53%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.047132 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 581\n[LightGBM] [Info] Number of data points in the train set: 897, number of used features: 10\n[LightGBM] [Info] Start training from score 2.000000\nProductID: PVSL0120030126, Brand: Unknown, Forecast Accuracy: -18.48%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.057250 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 596\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 2.000000\nProductID: PVSL0121040925, Brand: Unknown, Forecast Accuracy: -23.16%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.046154 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 580\n[LightGBM] [Info] Number of data points in the train set: 899, number of used features: 10[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.068057 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 843\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 23.000000\nProductID: PVSL0101031125, Brand: Unknown, Forecast Accuracy: 7.37%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.051194 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 871\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 22.000000\nProductID: PVSL0101051125, Brand: Unknown, Forecast Accuracy: -26.73%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.071064 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 862\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 23.000000\nProductID: PVSL0102030126, Brand: Unknown, Forecast Accuracy: 20.11%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.056175 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 587\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0103021025, Brand: Unknown, Forecast Accuracy: -56.76%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.059264 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 586\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0104020126, Brand: Unknown, Forecast Accuracy: -50.75%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.066213 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 593\n[LightGBM] [Info] Number of data points in the train set: 899, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0105051025, Brand: Unknown, Forecast Accuracy: -66.56%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.059216 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 612\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0106030126, Brand: Unknown, Forecast Accuracy: -65.16%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.053302 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 610\n[LightGBM] [Info] Number of data points in the train set: 896, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0107041025, Brand: Unknown, Forecast Accuracy: -69.35%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.067264 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 598\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0108051025, Brand: Unknown, Forecast Accuracy: -65.47%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.055223 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 578\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0110010126, Brand: Unknown, Forecast Accuracy: -74.99%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.078189 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 606\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0111041025, Brand: Unknown, Forecast Accuracy: -70.34%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.065132 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 578\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0112040925, Brand: Unknown, Forecast Accuracy: -68.49%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.071348 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 610\n[LightGBM] [Info] Number of data points in the train set: 899, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0113061125, Brand: Unknown, Forecast Accuracy: -70.11%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.056323 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 584\n[LightGBM] [Info] Number of data points in the train set: 899, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0114041125, Brand: Unknown, Forecast Accuracy: -66.59%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.074140 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 578\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 2.000000\nProductID: PVSL0115031125, Brand: Unknown, Forecast Accuracy: -27.39%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.062236 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 601\n[LightGBM] [Info] Number of data points in the train set: 896, number of used features: 10\n[LightGBM] [Info] Start training from score 4.000000\nProductID: PVSL0116041125, Brand: Unknown, Forecast Accuracy: -107.36%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.059340 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 606\n[LightGBM] [Info] Number of data points in the train set: 899, number of used features: 10\n[LightGBM] [Info] Start training from score 2.000000\nProductID: PVSL0117050126, Brand: Unknown, Forecast Accuracy: -53.27%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.071211 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 589\n[LightGBM] [Info] Number of data points in the train set: 899, number of used features: 10\n[LightGBM] [Info] Start training from score 2.000000\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\nProductID: PVSL0119020925, Brand: Unknown, Forecast Accuracy: -49.77%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.081271 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 602\n[LightGBM] [Info] Number of data points in the train set: 899, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0121050925, Brand: Unknown, Forecast Accuracy: -63.65%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.071203 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 583\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0122021125, Brand: Unknown, Forecast Accuracy: -67.13%\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0121010925, Brand: Unknown, Forecast Accuracy: -68.86%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.050285 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 590\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 2.000000\nProductID: PVSL0121020925, Brand: Unknown, Forecast Accuracy: -34.20%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.047312 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 582\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 3.500000\nProductID: PVSL0122051125, Brand: Unknown, Forecast Accuracy: -66.20%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.071312 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 602\n[LightGBM] [Info] Number of data points in the train set: 899, number of used features: 10\n[LightGBM] [Info] Start training from score 2.000000\nProductID: PVSL0124040925, Brand: Unknown, Forecast Accuracy: -27.16%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.044295 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 594\n[LightGBM] [Info] Number of data points in the train set: 899, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0125051125, Brand: Unknown, Forecast Accuracy: -63.50%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.071354 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 608\n[LightGBM] [Info] Number of data points in the train set: 899, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0127011025, Brand: Unknown, Forecast Accuracy: -54.46%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.045281 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 589\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 2.000000\nProductID: PVSL0127071025, Brand: Unknown, Forecast Accuracy: -47.72%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.056241 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 591\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0129031025, Brand: Unknown, Forecast Accuracy: -63.96%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.046239 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 598\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0130041025, Brand: Unknown, Forecast Accuracy: -61.89%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.089233 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 579\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0131031025, Brand: Unknown, Forecast Accuracy: -74.73%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.062293 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 593\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\nProductID: PVSL0131051025, Brand: Unknown, Forecast Accuracy: -56.63%\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.049904 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 580\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0133021025, Brand: Unknown, Forecast Accuracy: -78.63%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.080250 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 596\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0133031025, Brand: Unknown, Forecast Accuracy: -58.77%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.048290 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 586\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0135040925, Brand: Unknown, Forecast Accuracy: -67.30%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.059335 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 585\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0136040925, Brand: Unknown, Forecast Accuracy: -69.53%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.057254 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 570\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 2.000000\nProductID: PVSL0139050126, Brand: Unknown, Forecast Accuracy: -37.39%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.039140 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 589\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 2.000000\nProductID: PVSL0141011125, Brand: Unknown, Forecast Accuracy: -19.37%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.051192 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 617\n[LightGBM] [Info] Number of data points in the train set: 899, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0142040126, Brand: Unknown, Forecast Accuracy: -52.89%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.046233 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 596\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 2.000000\nProductID: PVSL0201041025, Brand: Unknown, Forecast Accuracy: -25.97%\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001016 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 609\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0202051125, Brand: Unknown, Forecast Accuracy: -73.67%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.059199 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.061101 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 592\n[LightGBM] [Info] Number of data points in the train set: 899, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0123030925, Brand: Unknown, Forecast Accuracy: -62.62%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.059357 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 611\n[LightGBM] [Info] Number of data points in the train set: 899, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\nProductID: PVSL0125011125, Brand: Unknown, Forecast Accuracy: -67.13%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.071328 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 611\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0125041125, Brand: Unknown, Forecast Accuracy: -56.39%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.064245 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 570\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0126011025, Brand: Unknown, Forecast Accuracy: -59.28%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.079693 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 586\n[LightGBM] [Info] Number of data points in the train set: 899, number of used features: 10\n[LightGBM] [Info] Start training from score 2.000000\nProductID: PVSL0126041025, Brand: Unknown, Forecast Accuracy: -44.30%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.058154 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 600\n[LightGBM] [Info] Number of data points in the train set: 899, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0128031225, Brand: Unknown, Forecast Accuracy: -74.52%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.059338 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 580\n[LightGBM] [Info] Number of data points in the train set: 899, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0128051225, Brand: Unknown, Forecast Accuracy: -57.22%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.071151 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 617\n[LightGBM] [Info] Number of data points in the train set: 899, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0130021025, Brand: Unknown, Forecast Accuracy: -70.97%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.055545 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 599\n[LightGBM] [Info] Number of data points in the train set: 899, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0132010126, Brand: Unknown, Forecast Accuracy: -65.51%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.046323 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 597\n[LightGBM] [Info] Number of data points in the train set: 899, number of used features: 10\n[LightGBM] [Info] Start training from score 2.000000\nProductID: PVSL0133011025, Brand: Unknown, Forecast Accuracy: -31.82%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.059351 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 581\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 2.000000\nProductID: PVSL0135010925, Brand: Unknown, Forecast Accuracy: -30.97%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.072207 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 610\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0137031225, Brand: Unknown, Forecast Accuracy: -79.68%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.059192 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 604\n[LightGBM] [Info] Number of data points in the train set: 896, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0138031225, Brand: Unknown, Forecast Accuracy: -68.56%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.060178 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 600\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 2.000000\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\nProductID: PVSL0138051225, Brand: Unknown, Forecast Accuracy: -15.64%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.077187 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 590\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0140030925, Brand: Unknown, Forecast Accuracy: -60.86%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.080165 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 591\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0140050925, Brand: Unknown, Forecast Accuracy: -59.77%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.082249 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 585\n[LightGBM] [Info] Number of data points in the train set: 899, number of used features: 10\n[LightGBM] [Info] Start training from score 2.000000\nProductID: PVSL0142020126, Brand: Unknown, Forecast Accuracy: -43.36%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.058242 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 602\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0202031125, Brand: Unknown, Forecast Accuracy: -73.46%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.062379 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 596\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0203010126, Brand: Unknown, Forecast Accuracy: -64.69%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.047382 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 602\n[LightGBM] [Info] Number of data points in the train set: 899, number of used features: 10\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 2.000000\nProductID: PVSL0120040126, Brand: Unknown, Forecast Accuracy: -48.12%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.068170 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 595\n[LightGBM] [Info] Number of data points in the train set: 899, number of used features: 10\n[LightGBM] [Info] Start training from score 2.000000\nProductID: PVSL0122011125, Brand: Unknown, Forecast Accuracy: -38.13%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.064272 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 596\n[LightGBM] [Info] Number of data points in the train set: 899, number of used features: 10\n[LightGBM] [Info] Start training from score 2.000000\nProductID: PVSL0123010925, Brand: Unknown, Forecast Accuracy: -49.27%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.009373 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 606\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0125021125, Brand: Unknown, Forecast Accuracy: -71.02%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.074133 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 603\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\nProductID: PVSL0125061125, Brand: Unknown, Forecast Accuracy: -52.70%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.059258 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 595\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0127031025, Brand: Unknown, Forecast Accuracy: -69.84%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.045260 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 585\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0127041025, Brand: Unknown, Forecast Accuracy: -71.33%\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.048905 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 606\n[LightGBM] [Info] Number of data points in the train set: 899, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0128041225, Brand: Unknown, Forecast Accuracy: -65.96%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.074241 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 590\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0130031025, Brand: Unknown, Forecast Accuracy: -58.02%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.050254 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 583\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 2.000000\nProductID: PVSL0131021025, Brand: Unknown, Forecast Accuracy: -26.63%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.053218 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 596\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 2.000000\nProductID: PVSL0132020126, Brand: Unknown, Forecast Accuracy: -33.98%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.071056 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 577\n[LightGBM] [Info] Number of data points in the train set: 898, number of used features: 10\n[LightGBM] [Info] Start training from score 2.000000\nProductID: PVSL0135030925, Brand: Unknown, Forecast Accuracy: -19.97%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.047384 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 594\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0137011225, Brand: Unknown, Forecast Accuracy: -51.15%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.045654 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 593\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0138061225, Brand: Unknown, Forecast Accuracy: -53.89%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.071144 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 588\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 2.000000\nProductID: PVSL0140040925, Brand: Unknown, Forecast Accuracy: -33.90%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.057183 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 593\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0140060925, Brand: Unknown, Forecast Accuracy: -73.79%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.059182 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 595\n[LightGBM] [Info] Number of data points in the train set: 899, number of used features: 10\n[LightGBM] [Info] Start training from score 2.000000\nProductID: PVSL0142060126, Brand: Unknown, Forecast Accuracy: -43.30%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.071379 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 601\n[LightGBM] [Info] Number of data points in the train set: 899, number of used features: 10\n[LightGBM] [Info] Start training from score 2.000000\nProductID: PVSL0201031025, Brand: Unknown, Forecast Accuracy: -18.92%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.038828 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 578\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 2.000000\nProductID: PVSL0203020126, Brand: Unknown, Forecast Accuracy: -25.37%\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.020981 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 596\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0204051225, Brand: Unknown, Forecast Accuracy: -87.02%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.047347 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Start training from score 2.000000\nProductID: PVSL0122031125, Brand: Unknown, Forecast Accuracy: -24.48%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.048091 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 586\n[LightGBM] [Info] Number of data points in the train set: 899, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0123040925, Brand: Unknown, Forecast Accuracy: -74.03%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.062114 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 596\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 2.000000\nProductID: PVSL0125031125, Brand: Unknown, Forecast Accuracy: -34.48%\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.044989 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 588\n[LightGBM] [Info] Number of data points in the train set: 897, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0126021025, Brand: Unknown, Forecast Accuracy: -68.41%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.059238 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 589\n[LightGBM] [Info] Number of data points in the train set: 896, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0126051025, Brand: Unknown, Forecast Accuracy: -57.72%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.071195 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 606\n[LightGBM] [Info] Number of data points in the train set: 898, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0128021225, Brand: Unknown, Forecast Accuracy: -58.19%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.068105 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 603\n[LightGBM] [Info] Number of data points in the train set: 897, number of used features: 10\n[LightGBM] [Info] Start training from score 4.000000\nProductID: PVSL0129021025, Brand: Unknown, Forecast Accuracy: -99.04%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.059266 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 594\n[LightGBM] [Info] Number of data points in the train set: 899, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0131041025, Brand: Unknown, Forecast Accuracy: -65.91%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.063524 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 617\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0132030126, Brand: Unknown, Forecast Accuracy: -54.00%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.057094 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 597\n[LightGBM] [Info] Number of data points in the train set: 898, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\nProductID: PVSL0135020925, Brand: Unknown, Forecast Accuracy: -69.94%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.059305 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 598\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 2.000000\nProductID: PVSL0138011225, Brand: Unknown, Forecast Accuracy: -42.97%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.059336 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 599\n[LightGBM] [Info] Number of data points in the train set: 898, number of used features: 10\n[LightGBM] [Info] Start training from score 2.000000\nProductID: PVSL0138041225, Brand: Unknown, Forecast Accuracy: -33.98%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.047063 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 591\n[LightGBM] [Info] Number of data points in the train set: 899, number of used features: 10\n[LightGBM] [Info] Start training from score 2.000000\nProductID: PVSL0139040126, Brand: Unknown, Forecast Accuracy: -72.90%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.049235 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 585\n[LightGBM] [Info] Number of data points in the train set: 899, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0141041125, Brand: Unknown, Forecast Accuracy: -51.31%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.047076 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 594\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0141061125, Brand: Unknown, Forecast Accuracy: -66.84%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.057361 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 590\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0142030126, Brand: Unknown, Forecast Accuracy: -77.67%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.062235 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 604\n[LightGBM] [Info] Number of data points in the train set: 897, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\nProductID: PVSL0202041125, Brand: Unknown, Forecast Accuracy: -42.04%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.047367 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 600\n[LightGBM] [Info] Number of data points in the train set: 899, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0202061125, Brand: Unknown, Forecast Accuracy: -66.05%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.047327 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 596\n[LightGBM] [Info] Number of data points in the train set: 898, number of used features: 10\n[LightGBM] [Info] Start training from score 2.000000\nProductID: PVSL0204041225, Brand: Unknown, Forecast Accuracy: -33.47%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.045840 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 595\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 2.000000\nProductID: PVSL0205021125, Brand: Unknown, Forecast Accuracy: -28.12%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.044298 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 605\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0205031125, Brand: Unknown, Forecast Accuracy: -84.00%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.058228 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 584\n[LightGBM] [Info] Number of data points in the train set: 899, number of used features: 10\n[LightGBM] [Info] Start training from score 2.000000\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\nProductID: PVSL0206031225, Brand: Unknown, Forecast Accuracy: -27.23%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.056349 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 598\n[LightGBM] [Info] Number of data points in the train set: 898, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0206061225, Brand: Unknown, Forecast Accuracy: -56.91%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.058268 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 611\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0207061125, Brand: Unknown, Forecast Accuracy: -80.02%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.055370 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 606\n[LightGBM] [Info] Number of data points in the train set: 898, number of used features: 10\n[LightGBM] [Info] Start training from score 2.000000\nProductID: PVSL0210011125, Brand: Unknown, Forecast Accuracy: -41.97%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.058157 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 580\n[LightGBM] [Info] Number of data points in the train set: 899, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0210041125, Brand: Unknown, Forecast Accuracy: -66.88%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.068245 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 608\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\nProductID: PVSL0211041025, Brand: Unknown, Forecast Accuracy: -52.76%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.050164 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 584\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0211061025, Brand: Unknown, Forecast Accuracy: -66.72%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.047054 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 589\n[LightGBM] [Info] Number of data points in the train set: 896, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\nProductID: PVSL0213021225, Brand: Unknown, Forecast Accuracy: -55.59%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.062372 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 599\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 2.000000\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\nProductID: PVSL0214031125, Brand: Unknown, Forecast Accuracy: -50.57%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.093965 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 597\n[LightGBM] [Info] Number of data points in the train set: 899, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0215030126, Brand: Unknown, Forecast Accuracy: -55.33%\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.047981 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 594\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0215040126, Brand: Unknown, Forecast Accuracy: -58.18%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.050256 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 613\n[LightGBM] [Info] Number of data points in the train set: 897, number of used features: 10\n[LightGBM] [Info] Start training from score 2.000000\nProductID: PVSL0215060126, Brand: Unknown, Forecast Accuracy: -45.39%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.076032 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 587\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0216050126, Brand: Unknown, Forecast Accuracy: -62.01%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.059373 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 596\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 2.000000\nProductID: PVSL0218011125, Brand: Unknown, Forecast Accuracy: -39.55%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.059235 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 600\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0218051125, Brand: Unknown, Forecast Accuracy: -65.01%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.067189 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 589\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0219041225, Brand: Unknown, Forecast Accuracy: -72.44%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.059158 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 581\n[LightGBM] [Info] Number of data points in the train set: 899, number of used features: 10\n[LightGBM] [Info] Start training from score 2.000000\nProductID: PVSL0220061025, Brand: Unknown, Forecast Accuracy: -18.47%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.059376 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 599\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0302031125, Brand: Unknown, Forecast Accuracy: -54.94%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.059294 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 599\n[LightGBM] [Info] Number of data points in the train set: 898, number of used features: 10\n[LightGBM] [Info] Start training from score 4.000000\nProductID: PVSL0203060126, Brand: Unknown, Forecast Accuracy: -107.76%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001509 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 588\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0204061225, Brand: Unknown, Forecast Accuracy: -68.30%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.047338 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 616\n[LightGBM] [Info] Number of data points in the train set: 898, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0205051125, Brand: Unknown, Forecast Accuracy: -87.27%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.080138 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 604\n[LightGBM] [Info] Number of data points in the train set: 899, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0205061125, Brand: Unknown, Forecast Accuracy: -78.89%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.060286 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 602\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0207031125, Brand: Unknown, Forecast Accuracy: -74.25%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.072060 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 597\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 2.000000\nProductID: PVSL0208041025, Brand: Unknown, Forecast Accuracy: -41.39%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.065396 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 600\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0210021125, Brand: Unknown, Forecast Accuracy: -65.22%\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.054888 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 596\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0210061125, Brand: Unknown, Forecast Accuracy: -57.41%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.059379 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 600\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0212040126, Brand: Unknown, Forecast Accuracy: -73.13%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.060208 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 596\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0213041225, Brand: Unknown, Forecast Accuracy: -62.95%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.073270 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 577\n[LightGBM] [Info] Number of data points in the train set: 896, number of used features: 10\n[LightGBM] [Info] Start training from score 2.000000\nProductID: PVSL0214061125, Brand: Unknown, Forecast Accuracy: -47.60%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.050285 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 592\n[LightGBM] [Info] Number of data points in the train set: 899, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0216020126, Brand: Unknown, Forecast Accuracy: -64.21%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.056341 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 582\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 2.000000\nProductID: PVSL0217051225, Brand: Unknown, Forecast Accuracy: -48.56%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.071210 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 592\n[LightGBM] [Info] Number of data points in the train set: 899, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0219011225, Brand: Unknown, Forecast Accuracy: -64.64%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.044692 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 604\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0219021225, Brand: Unknown, Forecast Accuracy: -50.84%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.058397 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 598\n[LightGBM] [Info] Number of data points in the train set: 899, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0219051225, Brand: Unknown, Forecast Accuracy: -52.96%\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.047993 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 608\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\nProductID: PVSL0302011125, Brand: Unknown, Forecast Accuracy: -65.96%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.064404 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 598\n[LightGBM] [Info] Number of data points in the train set: 898, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0303011125, Brand: Unknown, Forecast Accuracy: -51.70%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.068400 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 578\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 2.000000\nProductID: PVSL0303041125, Brand: Unknown, Forecast Accuracy: -28.34%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.059202 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 595\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0304011025, Brand: Unknown, Forecast Accuracy: -60.62%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.058411 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 616\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0203050126, Brand: Unknown, Forecast Accuracy: -65.59%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.067252 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 596\n[LightGBM] [Info] Number of data points in the train set: 898, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0205011125, Brand: Unknown, Forecast Accuracy: -56.04%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.055242 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 579\n[LightGBM] [Info] Number of data points in the train set: 899, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0205041125, Brand: Unknown, Forecast Accuracy: -39.01%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.053941 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 613\n[LightGBM] [Info] Number of data points in the train set: 899, number of used features: 10\n[LightGBM] [Info] Start training from score 2.000000\nProductID: PVSL0208021025, Brand: Unknown, Forecast Accuracy: -22.61%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.054750 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 601\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0208061025, Brand: Unknown, Forecast Accuracy: -77.77%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.089229 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 594\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0211021025, Brand: Unknown, Forecast Accuracy: -66.86%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.073149 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 599\n[LightGBM] [Info] Number of data points in the train set: 895, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0211051025, Brand: Unknown, Forecast Accuracy: -62.23%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.059372 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 595\n[LightGBM] [Info] Number of data points in the train set: 898, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0212030126, Brand: Unknown, Forecast Accuracy: -67.26%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.056643 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 590\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0212060126, Brand: Unknown, Forecast Accuracy: -48.48%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.070227 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 606\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\nProductID: PVSL0214051125, Brand: Unknown, Forecast Accuracy: -67.95%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.045147 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 599\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 2.000000\nProductID: PVSL0215050126, Brand: Unknown, Forecast Accuracy: -56.39%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.011389 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 602\n[LightGBM] [Info] Number of data points in the train set: 899, number of used features: 10\n[LightGBM] [Info] Start training from score 2.000000\nProductID: PVSL0216060126, Brand: Unknown, Forecast Accuracy: -52.67%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.070337 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 591\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0217061225, Brand: Unknown, Forecast Accuracy: -63.75%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.082316 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 593\n[LightGBM] [Info] Number of data points in the train set: 899, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0218021125, Brand: Unknown, Forecast Accuracy: -44.57%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.058241 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 591\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 2.000000\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\nProductID: PVSL0218031125, Brand: Unknown, Forecast Accuracy: -45.03%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.065168 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 578\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 2.500000\nProductID: PVSL0220021025, Brand: Unknown, Forecast Accuracy: -45.92%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.071244 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 602\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 2.000000\nProductID: PVSL0302041125, Brand: Unknown, Forecast Accuracy: -27.87%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.062366 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 599\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\nProductID: PVSL0303051125, Brand: Unknown, Forecast Accuracy: -59.20%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.047328 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 591\n[LightGBM] [Info] Number of data points in the train set: 898, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\nProductID: PVSL0304051025, Brand: Unknown, Forecast Accuracy: -62.84%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.066236 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 608\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Total Bins 605\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0206041225, Brand: Unknown, Forecast Accuracy: -61.05%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.065360 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 594\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 2.000000\nProductID: PVSL0207041125, Brand: Unknown, Forecast Accuracy: -27.32%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.053867 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 583\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 2.000000\nProductID: PVSL0208031025, Brand: Unknown, Forecast Accuracy: -53.80%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.079251 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 590\n[LightGBM] [Info] Number of data points in the train set: 899, number of used features: 10\n[LightGBM] [Info] Start training from score 2.000000\nProductID: PVSL0209061225, Brand: Unknown, Forecast Accuracy: -41.22%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.053314 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 596\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 2.000000\nProductID: PVSL0210031125, Brand: Unknown, Forecast Accuracy: -40.83%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.058214 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 596\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0212020126, Brand: Unknown, Forecast Accuracy: -66.35%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.051240 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 604\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 2.000000\nProductID: PVSL0212050126, Brand: Unknown, Forecast Accuracy: -40.71%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.056162 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 588\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0214011125, Brand: Unknown, Forecast Accuracy: -72.30%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.060247 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 588\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 2.000000\nProductID: PVSL0215010126, Brand: Unknown, Forecast Accuracy: -33.32%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.092189 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 595\n[LightGBM] [Info] Number of data points in the train set: 899, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0215020126, Brand: Unknown, Forecast Accuracy: -74.42%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.057296 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 603\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0216010126, Brand: Unknown, Forecast Accuracy: -70.86%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.059250 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 616\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0217031225, Brand: Unknown, Forecast Accuracy: -40.64%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.071210 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 591\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0218061125, Brand: Unknown, Forecast Accuracy: -68.83%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.059364 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 597\n[LightGBM] [Info] Number of data points in the train set: 897, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\nProductID: PVSL0219031225, Brand: Unknown, Forecast Accuracy: -50.78%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.053380 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 590\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 2.000000\nProductID: PVSL0219061225, Brand: Unknown, Forecast Accuracy: -22.40%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.066942 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 591\n[LightGBM] [Info] Number of data points in the train set: 899, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0302021125, Brand: Unknown, Forecast Accuracy: -66.16%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.065334 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 608\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0302051125, Brand: Unknown, Forecast Accuracy: -66.55%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.075368 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 608\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0303061125, Brand: Unknown, Forecast Accuracy: -64.12%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.047383 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 588\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 2.000000\nProductID: PVSL0304041025, Brand: Unknown, Forecast Accuracy: -19.82%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.080451 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 590\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0305040925, Brand: Unknown, Forecast Accuracy: -50.36%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.047384 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0303021125, Brand: Unknown, Forecast Accuracy: -62.53%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.063218 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 601\n[LightGBM] [Info] Number of data points in the train set: 899, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0304021025, Brand: Unknown, Forecast Accuracy: -66.13%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.062246 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 598\n[LightGBM] [Info] Number of data points in the train set: 899, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0305030925, Brand: Unknown, Forecast Accuracy: -87.92%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.071310 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 605\n[LightGBM] [Info] Number of data points in the train set: 898, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0305060925, Brand: Unknown, Forecast Accuracy: -54.87%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.048389 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 583\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 2.000000\nProductID: PVSL0307021225, Brand: Unknown, Forecast Accuracy: -63.78%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.047246 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 590\n[LightGBM] [Info] Number of data points in the train set: 899, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\nProductID: PVSL0308020925, Brand: Unknown, Forecast Accuracy: -54.72%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.078248 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 594\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0309011125, Brand: Unknown, Forecast Accuracy: -81.27%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.062368 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 604\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 2.000000\nProductID: PVSL0309051125, Brand: Unknown, Forecast Accuracy: -27.34%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.061263 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 598\n[LightGBM] [Info] Number of data points in the train set: 899, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0310020126, Brand: Unknown, Forecast Accuracy: -67.61%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.059244 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 610\n[LightGBM] [Info] Number of data points in the train set: 899, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0311061025, Brand: Unknown, Forecast Accuracy: -66.96%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.060367 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 599\n[LightGBM] [Info] Number of data points in the train set: 899, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0312030126, Brand: Unknown, Forecast Accuracy: -59.05%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.058397 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 584\n[LightGBM] [Info] Number of data points in the train set: 898, number of used features: 10\n[LightGBM] [Info] Start training from score 2.000000\nProductID: PVSL0314010925, Brand: Unknown, Forecast Accuracy: -64.55%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.047863 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 587\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0315031025, Brand: Unknown, Forecast Accuracy: -70.33%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.094255 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 585\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0316040925, Brand: Unknown, Forecast Accuracy: -57.05%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.051227 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 572\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 2.000000\nProductID: PVSL0316060925, Brand: Unknown, Forecast Accuracy: -26.79%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.046272 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 596\n[LightGBM] [Info] Number of data points in the train set: 899, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0318010126, Brand: Unknown, Forecast Accuracy: -59.72%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.062248 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 591\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 2.000000\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\nProductID: PVSL0318030126, Brand: Unknown, Forecast Accuracy: -45.29%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.059660 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 609\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0320031225, Brand: Unknown, Forecast Accuracy: -56.53%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.051873 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 584\n[LightGBM] [Info] Number of data points in the train set: 899, number of used features: 10\n[LightGBM] [Info] Start training from score 2.000000\nProductID: PVSL0401040925, Brand: Unknown, Forecast Accuracy: -46.58%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.059362 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 586\n[LightGBM] [Info] Number of data points in the train set: 899, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0402021225, Brand: Unknown, Forecast Accuracy: -53.94%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.077178 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 594\n[LightGBM] [Info] Number of data points in the train set: 898, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0305020925, Brand: Unknown, Forecast Accuracy: -63.70%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.072223 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 603\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0306011025, Brand: Unknown, Forecast Accuracy: -86.27%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.058246 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 581\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 2.000000\nProductID: PVSL0307041225, Brand: Unknown, Forecast Accuracy: -14.22%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.071173 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 602\n[LightGBM] [Info] Number of data points in the train set: 897, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0308030925, Brand: Unknown, Forecast Accuracy: -53.00%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.056174 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 598\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0309031125, Brand: Unknown, Forecast Accuracy: -63.21%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.050366 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 622\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0309061125, Brand: Unknown, Forecast Accuracy: -63.78%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.070902 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 608\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0310030126, Brand: Unknown, Forecast Accuracy: -50.72%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.048288 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 610\n[LightGBM] [Info] Number of data points in the train set: 899, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0311011025, Brand: Unknown, Forecast Accuracy: -73.02%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.063326 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 588\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0313041025, Brand: Unknown, Forecast Accuracy: -71.48%\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.037648 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 602\n[LightGBM] [Info] Number of data points in the train set: 896, number of used features: 10\n[LightGBM] [Info] Start training from score 2.000000\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\nProductID: PVSL0314060925, Brand: Unknown, Forecast Accuracy: -29.26%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.078248 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 593\n[LightGBM] [Info] Number of data points in the train set: 897, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\nProductID: PVSL0316010925, Brand: Unknown, Forecast Accuracy: -74.58%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.071240 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 589\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 2.000000\nProductID: PVSL0317011125, Brand: Unknown, Forecast Accuracy: -24.20%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.059316 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 596\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\nProductID: PVSL0317061125, Brand: Unknown, Forecast Accuracy: -71.15%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.075369 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 596\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0318060126, Brand: Unknown, Forecast Accuracy: -56.04%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.046859 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 616\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0319041225, Brand: Unknown, Forecast Accuracy: -60.64%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.070265 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 578\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0401050925, Brand: Unknown, Forecast Accuracy: -75.75%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.056352 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 605\n[LightGBM] [Info] Number of data points in the train set: 899, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0402031225, Brand: Unknown, Forecast Accuracy: -69.02%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.053228 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 583\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0403010126, Brand: Unknown, Forecast Accuracy: -71.38%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.071230 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 584\n[LightGBM] [Info] Number of data points in the train set: 898, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0404051225, Brand: Unknown, Forecast Accuracy: -71.43%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.047331 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 620\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 2.000000\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0305050925, Brand: Unknown, Forecast Accuracy: -83.18%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.059380 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 596\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0306031025, Brand: Unknown, Forecast Accuracy: -65.56%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.059322 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 596\n[LightGBM] [Info] Number of data points in the train set: 899, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0307011225, Brand: Unknown, Forecast Accuracy: -81.70%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.057365 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 597\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0308050925, Brand: Unknown, Forecast Accuracy: -68.26%\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.042988 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 599\n[LightGBM] [Info] Number of data points in the train set: 898, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0310040126, Brand: Unknown, Forecast Accuracy: -59.30%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.050343 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 571\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0311051025, Brand: Unknown, Forecast Accuracy: -69.92%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.065233 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 585\n[LightGBM] [Info] Number of data points in the train set: 898, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0312010126, Brand: Unknown, Forecast Accuracy: -57.82%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.062369 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 593\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0313011025, Brand: Unknown, Forecast Accuracy: -58.26%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.050330 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 588\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0314040925, Brand: Unknown, Forecast Accuracy: -61.83%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.071371 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 578\n[LightGBM] [Info] Number of data points in the train set: 899, number of used features: 10\n[LightGBM] [Info] Start training from score 2.000000\nProductID: PVSL0315011025, Brand: Unknown, Forecast Accuracy: -40.93%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.059337 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 592\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\nProductID: PVSL0316030925, Brand: Unknown, Forecast Accuracy: -53.34%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.053590 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 594\n[LightGBM] [Info] Number of data points in the train set: 897, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0317051125, Brand: Unknown, Forecast Accuracy: -80.01%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.049526 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 592\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0318040126, Brand: Unknown, Forecast Accuracy: -62.73%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.047280 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 600\n[LightGBM] [Info] Number of data points in the train set: 899, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0319021225, Brand: Unknown, Forecast Accuracy: -64.31%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.077236 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 572\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 2.500000\nProductID: PVSL0401030925, Brand: Unknown, Forecast Accuracy: -43.57%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.071222 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 587\n[LightGBM] [Info] Number of data points in the train set: 899, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0402011225, Brand: Unknown, Forecast Accuracy: -63.81%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.058252 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 592\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0402041225, Brand: Unknown, Forecast Accuracy: -74.89%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.060258 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 609\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0404021225, Brand: Unknown, Forecast Accuracy: -64.14%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.058397 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 587\n[LightGBM] [Info] Number of data points in the train set: 899, number of used features: 10\n[LightGBM] [Info] Start training from score 2.000000\nProductID: PVSL0405060126, Brand: Unknown, Forecast Accuracy: -39.89%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.070240 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 584\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0406041125, Brand: Unknown, Forecast Accuracy: -50.90%\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009052 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 581\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0306061025, Brand: Unknown, Forecast Accuracy: -50.28%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.059346 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 596\n[LightGBM] [Info] Number of data points in the train set: 899, number of used features: 10\n[LightGBM] [Info] Start training from score 2.000000\nProductID: PVSL0308010925, Brand: Unknown, Forecast Accuracy: -25.92%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.049207 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 583\n[LightGBM] [Info] Number of data points in the train set: 898, number of used features: 10\n[LightGBM] [Info] Start training from score 2.500000\nProductID: PVSL0308040925, Brand: Unknown, Forecast Accuracy: -43.60%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.094258 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 590\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0309021125, Brand: Unknown, Forecast Accuracy: -64.47%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.061539 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 600\n[LightGBM] [Info] Number of data points in the train set: 899, number of used features: 10\n[LightGBM] [Info] Start training from score 2.000000\nProductID: PVSL0310060126, Brand: Unknown, Forecast Accuracy: -33.90%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.060390 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 607\n[LightGBM] [Info] Number of data points in the train set: 899, number of used features: 10\n[LightGBM] [Info] Start training from score 2.000000\nProductID: PVSL0312020126, Brand: Unknown, Forecast Accuracy: -67.84%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.039692 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 587\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 2.000000\nProductID: PVSL0312050126, Brand: Unknown, Forecast Accuracy: -41.60%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.071203 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 605\n[LightGBM] [Info] Number of data points in the train set: 899, number of used features: 10\n[LightGBM] [Info] Start training from score 2.000000\nProductID: PVSL0315051025, Brand: Unknown, Forecast Accuracy: -46.05%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.071236 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 588\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 2.000000\nProductID: PVSL0316050925, Brand: Unknown, Forecast Accuracy: -35.53%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.059193 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 590\n[LightGBM] [Info] Number of data points in the train set: 899, number of used features: 10\n[LightGBM] [Info] Start training from score 2.000000\nProductID: PVSL0317041125, Brand: Unknown, Forecast Accuracy: -31.29%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.062369 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 606\n[LightGBM] [Info] Number of data points in the train set: 899, number of used features: 10\n[LightGBM] [Info] Start training from score 2.000000\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\nProductID: PVSL0318020126, Brand: Unknown, Forecast Accuracy: -23.43%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.059225 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 609\n[LightGBM] [Info] Number of data points in the train set: 899, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0319011225, Brand: Unknown, Forecast Accuracy: -66.21%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.058231 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 596\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0320041225, Brand: Unknown, Forecast Accuracy: -59.39%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.079761 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 596\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 2.000000\nProductID: PVSL0401060925, Brand: Unknown, Forecast Accuracy: -21.62%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.066626 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 597\n[LightGBM] [Info] Number of data points in the train set: 898, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0402051225, Brand: Unknown, Forecast Accuracy: -76.57%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.071238 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 593\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0403020126, Brand: Unknown, Forecast Accuracy: -67.17%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.048410 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 595\n[LightGBM] [Info] Number of data points in the train set: 895, number of used features: 10\n[LightGBM] [Info] Start training from score 2.000000\nProductID: PVSL0403050126, Brand: Unknown, Forecast Accuracy: -32.52%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.053920 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 575\n[LightGBM] [Info] Number of data points in the train set: 899, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0404061225, Brand: Unknown, Forecast Accuracy: -72.47%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.046246 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 587\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0405050126, Brand: Unknown, Forecast Accuracy: -74.00%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.073211 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 592\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 2.000000\nProductID: PVSL0406021125, Brand: Unknown, Forecast Accuracy: -38.78%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.059140 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 604\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0403040126, Brand: Unknown, Forecast Accuracy: -61.06%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.059369 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 600\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0403060126, Brand: Unknown, Forecast Accuracy: -65.85%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.059253 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 583\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0404041225, Brand: Unknown, Forecast Accuracy: -65.48%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.078373 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 601\n[LightGBM] [Info] Number of data points in the train set: 899, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0405010126, Brand: Unknown, Forecast Accuracy: -64.27%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.067239 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 599\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0406011125, Brand: Unknown, Forecast Accuracy: -73.71%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.059250 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 610\n[LightGBM] [Info] Number of data points in the train set: 899, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0407020126, Brand: Unknown, Forecast Accuracy: -70.75%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.071245 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 600\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\nProductID: PVSL0408040925, Brand: Unknown, Forecast Accuracy: -56.62%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.054371 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 591\n[LightGBM] [Info] Number of data points in the train set: 899, number of used features: 10\n[LightGBM] [Info] Start training from score 2.000000\nProductID: PVSL0409030925, Brand: Unknown, Forecast Accuracy: -37.88%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.067232 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 601\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 2.000000\nProductID: PVSL0410011125, Brand: Unknown, Forecast Accuracy: -18.02%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.080353 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 586\n[LightGBM] [Info] Number of data points in the train set: 899, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0413011225, Brand: Unknown, Forecast Accuracy: -69.45%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.047365 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 596\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0414051125, Brand: Unknown, Forecast Accuracy: -71.13%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.071109 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 593\n[LightGBM] [Info] Number of data points in the train set: 899, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\nProductID: PVSL0414061125, Brand: Unknown, Forecast Accuracy: -65.43%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.047191 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 577\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 2.000000\nProductID: PVSL0415050126, Brand: Unknown, Forecast Accuracy: -27.48%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.059321 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 598\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 2.000000\nProductID: PVSL0416061025, Brand: Unknown, Forecast Accuracy: -50.09%\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013025 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 594\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 2.000000\nProductID: PVSL0417041225, Brand: Unknown, Forecast Accuracy: -34.34%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.059386 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 592\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0419010126, Brand: Unknown, Forecast Accuracy: -63.36%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.055161 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 586\n[LightGBM] [Info] Number of data points in the train set: 899, number of used features: 10\n[LightGBM] [Info] Start training from score 2.000000\nProductID: PVSL0419060126, Brand: Unknown, Forecast Accuracy: -58.72%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.055167 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 610\n[LightGBM] [Info] Number of data points in the train set: 899, number of used features: 10\n[LightGBM] [Info] Start training from score 4.000000\nProductID: PVSL0420010925, Brand: Unknown, Forecast Accuracy: -96.11%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.058328 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 602\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 2.000000\nProductID: PVSL0420040925, Brand: Unknown, Forecast Accuracy: -20.37%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.071361 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 590\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 2.000000\nProductID: PVSL0501050925, Brand: Unknown, Forecast Accuracy: -26.31%\nProductID: PVSL0405020126, Brand: Unknown, Forecast Accuracy: -48.97%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.056372 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 585\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0406031125, Brand: Unknown, Forecast Accuracy: -69.74%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.060256 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 574\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0407030126, Brand: Unknown, Forecast Accuracy: -57.74%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.048236 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 601\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 2.000000\nProductID: PVSL0407060126, Brand: Unknown, Forecast Accuracy: -31.77%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.062364 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 588\n[LightGBM] [Info] Number of data points in the train set: 899, number of used features: 10\n[LightGBM] [Info] Start training from score 2.000000\nProductID: PVSL0408050925, Brand: Unknown, Forecast Accuracy: -20.50%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.059176 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 583\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0408060925, Brand: Unknown, Forecast Accuracy: -54.13%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.068752 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 587\n[LightGBM] [Info] Number of data points in the train set: 898, number of used features: 10\n[LightGBM] [Info] Start training from score 2.000000\nProductID: PVSL0409050925, Brand: Unknown, Forecast Accuracy: -38.78%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.060126 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 587\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 2.000000\nProductID: PVSL0410041125, Brand: Unknown, Forecast Accuracy: -32.13%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.044414 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 584\n[LightGBM] [Info] Number of data points in the train set: 899, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0410061125, Brand: Unknown, Forecast Accuracy: -58.05%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.050375 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 599\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0411061225, Brand: Unknown, Forecast Accuracy: -66.08%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.047327 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 592\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0414031125, Brand: Unknown, Forecast Accuracy: -70.87%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000136 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 601\n[LightGBM] [Info] Number of data points in the train set: 899, number of used features: 10\n[LightGBM] [Info] Start training from score 4.000000\nProductID: PVSL0415030126, Brand: Unknown, Forecast Accuracy: -100.07%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.052363 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 584\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0416021025, Brand: Unknown, Forecast Accuracy: -81.90%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.059366 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 580\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 2.000000\nProductID: PVSL0417011225, Brand: Unknown, Forecast Accuracy: -30.34%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.059287 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 591\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 2.000000\nProductID: PVSL0417031225, Brand: Unknown, Forecast Accuracy: -54.31%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.059248 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 595\n[LightGBM] [Info] Number of data points in the train set: 899, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\nProductID: PVSL0418031125, Brand: Unknown, Forecast Accuracy: -69.35%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.058367 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 598\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\nProductID: PVSL0418051125, Brand: Unknown, Forecast Accuracy: -59.03%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.052271 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 585\n[LightGBM] [Info] Number of data points in the train set: 898, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0419030126, Brand: Unknown, Forecast Accuracy: -65.74%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.059240 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 608\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 2.500000\nProductID: PVSL0501040925, Brand: Unknown, Forecast Accuracy: -60.13%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.075408 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 590\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0503031225, Brand: Unknown, Forecast Accuracy: -73.92%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.061347 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 583\n[LightGBM] [Info] Number of data points in the train set: 898, number of used features: 10\n[LightGBM] [Info] Total Bins 595\n[LightGBM] [Info] Number of data points in the train set: 898, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0406051125, Brand: Unknown, Forecast Accuracy: -66.30%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.046200 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 600\n[LightGBM] [Info] Number of data points in the train set: 898, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0407010126, Brand: Unknown, Forecast Accuracy: -72.30%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.053249 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 588\n[LightGBM] [Info] Number of data points in the train set: 899, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0408010925, Brand: Unknown, Forecast Accuracy: -45.22%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.049028 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 610\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 4.000000\nProductID: PVSL0409010925, Brand: Unknown, Forecast Accuracy: -121.08%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.069635 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 580\n[LightGBM] [Info] Number of data points in the train set: 898, number of used features: 10\n[LightGBM] [Info] Start training from score 2.000000\nProductID: PVSL0409040925, Brand: Unknown, Forecast Accuracy: -29.95%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.056488 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 603\n[LightGBM] [Info] Number of data points in the train set: 899, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\nProductID: PVSL0410021125, Brand: Unknown, Forecast Accuracy: -60.84%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.070293 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 585\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0412051025, Brand: Unknown, Forecast Accuracy: -54.42%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.038217 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 601\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0414021125, Brand: Unknown, Forecast Accuracy: -64.77%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.057144 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 595\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0415010126, Brand: Unknown, Forecast Accuracy: -75.84%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.050209 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 588\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 2.000000\nProductID: PVSL0416011025, Brand: Unknown, Forecast Accuracy: -28.72%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.069650 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 586\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0416031025, Brand: Unknown, Forecast Accuracy: -63.18%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.059225 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 604\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0417021225, Brand: Unknown, Forecast Accuracy: -69.14%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.047372 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 584\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 2.000000\nProductID: PVSL0417061225, Brand: Unknown, Forecast Accuracy: -76.34%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.082413 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 603\n[LightGBM] [Info] Number of data points in the train set: 897, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0418041125, Brand: Unknown, Forecast Accuracy: -51.70%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.060171 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 594\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0419050126, Brand: Unknown, Forecast Accuracy: -72.04%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.074361 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 585\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0502020925, Brand: Unknown, Forecast Accuracy: -81.29%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.088145 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 592\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 3.500000\nProductID: PVSL0503041225, Brand: Unknown, Forecast Accuracy: -71.95%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.051368 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 594\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0504051125, Brand: Unknown, Forecast Accuracy: -64.91%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.065145 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 589\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 2.000000\nProductID: PVSL0505051125, Brand: Unknown, Forecast Accuracy: -31.98%\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.014001 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 588\n[LightGBM] [Info] Number of data points in the train set: 899, number of used features: 10\n[LightGBM] [Info] Start training from score 2.000000\nProductID: PVSL0507030925, Brand: Unknown, Forecast Accuracy: -38.86%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.038385 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 590\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0407040126, Brand: Unknown, Forecast Accuracy: -64.67%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.072194 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 604\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0407050126, Brand: Unknown, Forecast Accuracy: -57.63%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.049405 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 590\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 2.000000\nProductID: PVSL0409020925, Brand: Unknown, Forecast Accuracy: -45.13%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.063389 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 617\n[LightGBM] [Info] Number of data points in the train set: 899, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0411031225, Brand: Unknown, Forecast Accuracy: -64.42%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.047240 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 575\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 2.000000\nProductID: PVSL0413051225, Brand: Unknown, Forecast Accuracy: -31.77%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.053129 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 590\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\nProductID: PVSL0415040126, Brand: Unknown, Forecast Accuracy: -71.07%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.059251 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 591\n[LightGBM] [Info] Number of data points in the train set: 899, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0415060126, Brand: Unknown, Forecast Accuracy: -79.63%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.079180 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 612\n[LightGBM] [Info] Number of data points in the train set: 898, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0416041025, Brand: Unknown, Forecast Accuracy: -54.74%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.044348 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 598\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0417051225, Brand: Unknown, Forecast Accuracy: -66.05%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.059376 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 623\n[LightGBM] [Info] Number of data points in the train set: 899, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0418061125, Brand: Unknown, Forecast Accuracy: -53.59%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.086601 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 590\n[LightGBM] [Info] Number of data points in the train set: 897, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0419020126, Brand: Unknown, Forecast Accuracy: -67.15%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.096236 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 617\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0501060925, Brand: Unknown, Forecast Accuracy: -66.65%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.047237 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 587\n[LightGBM] [Info] Number of data points in the train set: 897, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\nProductID: PVSL0503021225, Brand: Unknown, Forecast Accuracy: -54.99%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.073327 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 592\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0504041125, Brand: Unknown, Forecast Accuracy: -62.95%\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.038990 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 595\n[LightGBM] [Info] Number of data points in the train set: 899, number of used features: 10\n[LightGBM] [Info] Start training from score 2.000000\nProductID: PVSL0505011125, Brand: Unknown, Forecast Accuracy: -34.90%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.059359 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 589\n[LightGBM] [Info] Number of data points in the train set: 898, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0506011225, Brand: Unknown, Forecast Accuracy: -67.08%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.059380 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 592\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0506051225, Brand: Unknown, Forecast Accuracy: -61.30%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.047311 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 587\n[LightGBM] [Info] Number of data points in the train set: 899, number of used features: 10\n[LightGBM] [Info] Start training from score 2.000000\nProductID: PVSL0507040925, Brand: Unknown, Forecast Accuracy: -25.71%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.059339 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 577\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0508050925, Brand: Unknown, Forecast Accuracy: -60.08%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.069199 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 603\n[LightGBM] [Info] Number of data points in the train set: 899, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0510021025, Brand: Unknown, Forecast Accuracy: -78.94%\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0504031125, Brand: Unknown, Forecast Accuracy: -57.24%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.075360 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 602\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0505031125, Brand: Unknown, Forecast Accuracy: -72.65%\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.020001 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 595\n[LightGBM] [Info] Number of data points in the train set: 899, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0506031225, Brand: Unknown, Forecast Accuracy: -66.00%\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.019557 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 615\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0506061225, Brand: Unknown, Forecast Accuracy: -50.40%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.062192 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 608\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 2.000000\nProductID: PVSL0508020925, Brand: Unknown, Forecast Accuracy: -35.46%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.047238 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 586\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0509011125, Brand: Unknown, Forecast Accuracy: -61.67%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.071228 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 607\n[LightGBM] [Info] Number of data points in the train set: 899, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0509051125, Brand: Unknown, Forecast Accuracy: -68.12%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.065213 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 604\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 2.000000\nProductID: PVSL0511010925, Brand: Unknown, Forecast Accuracy: -21.91%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.059355 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 606\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 2.000000\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\nProductID: PVSL0511030925, Brand: Unknown, Forecast Accuracy: -45.48%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.059322 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 610\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 2.000000\nProductID: PVSL0512021125, Brand: Unknown, Forecast Accuracy: -85.37%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.068153 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 598\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 2.000000\nProductID: PVSL0512061125, Brand: Unknown, Forecast Accuracy: -21.00%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.060211 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 593\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0514010126, Brand: Unknown, Forecast Accuracy: -83.55%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.047244 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 599\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0516051025, Brand: Unknown, Forecast Accuracy: -63.18%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.070243 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 608\n[LightGBM] [Info] Number of data points in the train set: 899, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0517021225, Brand: Unknown, Forecast Accuracy: -59.37%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.057356 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 599\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0519011125, Brand: Unknown, Forecast Accuracy: -66.57%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.078495 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 581\n[LightGBM] [Info] Number of data points in the train set: 899, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0520011125, Brand: Unknown, Forecast Accuracy: -59.84%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.059230 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 593\n[LightGBM] [Info] Number of data points in the train set: 899, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0602051125, Brand: Unknown, Forecast Accuracy: -73.47%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.059362 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 595\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0604031025, Brand: Unknown, Forecast Accuracy: -61.46%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.046401 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 599\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0604041025, Brand: Unknown, Forecast Accuracy: -69.11%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.045485 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 587\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 2.000000\nProductID: PVSL0606041225, Brand: Unknown, Forecast Accuracy: -19.77%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.059303 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 595\n[LightGBM] [Info] Number of data points in the train set: 898, number of used features: 10\n[LightGBM] [Info] Start training from score 2.500000\nProductID: PVSL0507050925, Brand: Unknown, Forecast Accuracy: -58.04%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.058098 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 613\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 4.000000\nProductID: PVSL0508010925, Brand: Unknown, Forecast Accuracy: -105.53%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.059379 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 593\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0509041125, Brand: Unknown, Forecast Accuracy: -56.68%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.047347 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 622\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0510061025, Brand: Unknown, Forecast Accuracy: -57.39%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.067240 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 597\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0511060925, Brand: Unknown, Forecast Accuracy: -49.71%\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.050883 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 599\n[LightGBM] [Info] Number of data points in the train set: 897, number of used features: 10\n[LightGBM] [Info] Start training from score 2.000000\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\nProductID: PVSL0512031125, Brand: Unknown, Forecast Accuracy: -26.29%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.059361 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 594\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0512051125, Brand: Unknown, Forecast Accuracy: -79.31%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.059312 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 594\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 2.000000\nProductID: PVSL0513021225, Brand: Unknown, Forecast Accuracy: -34.14%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.056307 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 587\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0514020126, Brand: Unknown, Forecast Accuracy: -55.10%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.066247 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 594\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0514060126, Brand: Unknown, Forecast Accuracy: -49.64%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.071156 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 592\n[LightGBM] [Info] Number of data points in the train set: 899, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0516011025, Brand: Unknown, Forecast Accuracy: -63.37%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.058212 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 586\n[LightGBM] [Info] Number of data points in the train set: 898, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0516031025, Brand: Unknown, Forecast Accuracy: -60.81%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.068730 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 595\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 2.000000\nProductID: PVSL0516061025, Brand: Unknown, Forecast Accuracy: -21.70%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.047211 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 594\n[LightGBM] [Info] Number of data points in the train set: 899, number of used features: 10\n[LightGBM] [Info] Start training from score 2.000000\nProductID: PVSL0517041225, Brand: Unknown, Forecast Accuracy: -54.68%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.049362 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 572\n[LightGBM] [Info] Number of data points in the train set: 899, number of used features: 10\n[LightGBM] [Info] Start training from score 2.000000\nProductID: PVSL0518041025, Brand: Unknown, Forecast Accuracy: -28.09%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.059367 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 591\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0601011125, Brand: Unknown, Forecast Accuracy: -63.44%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.058392 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 588\n[LightGBM] [Info] Number of data points in the train set: 898, number of used features: 10\n[LightGBM] [Info] Start training from score 2.000000\nProductID: PVSL0601031125, Brand: Unknown, Forecast Accuracy: -15.72%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.056383 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 602\n[LightGBM] [Info] Number of data points in the train set: 899, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0602021125, Brand: Unknown, Forecast Accuracy: -64.08%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.060338 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 590\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 1.500000\nProductID: PVSL0603021025, Brand: Unknown, Forecast Accuracy: -41.03%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.066372 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 593\n[LightGBM] [Info] Number of data points in the train set: 899, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0605041125, Brand: Unknown, Forecast Accuracy: -59.61%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.074162 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.079328 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 604\n[LightGBM] [Info] Number of data points in the train set: 898, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0502040925, Brand: Unknown, Forecast Accuracy: -61.42%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.059352 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 591\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 2.000000\nProductID: PVSL0504011125, Brand: Unknown, Forecast Accuracy: -35.10%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.074171 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 599\n[LightGBM] [Info] Number of data points in the train set: 899, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0505021125, Brand: Unknown, Forecast Accuracy: -68.09%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.057301 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 596\n[LightGBM] [Info] Number of data points in the train set: 896, number of used features: 10\n[LightGBM] [Info] Start training from score 2.000000\nProductID: PVSL0505061125, Brand: Unknown, Forecast Accuracy: -32.20%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.059291 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 587\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0507010925, Brand: Unknown, Forecast Accuracy: -67.62%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.061238 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 594\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0507060925, Brand: Unknown, Forecast Accuracy: -51.92%\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.046911 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 583\n[LightGBM] [Info] Number of data points in the train set: 899, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0509061125, Brand: Unknown, Forecast Accuracy: -65.47%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.056227 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 584\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 2.000000\nProductID: PVSL0511020925, Brand: Unknown, Forecast Accuracy: -40.36%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.057947 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 615\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0514050126, Brand: Unknown, Forecast Accuracy: -56.22%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.071226 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 580\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0515061225, Brand: Unknown, Forecast Accuracy: -60.43%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.068244 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 588\n[LightGBM] [Info] Number of data points in the train set: 899, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\nProductID: PVSL0517031225, Brand: Unknown, Forecast Accuracy: -63.21%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.047372 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 592\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 2.000000\nProductID: PVSL0517061225, Brand: Unknown, Forecast Accuracy: -30.67%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.068209 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 585\n[LightGBM] [Info] Number of data points in the train set: 898, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0519051125, Brand: Unknown, Forecast Accuracy: -67.61%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.051357 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 602\n[LightGBM] [Info] Number of data points in the train set: 899, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0519061125, Brand: Unknown, Forecast Accuracy: -69.92%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.071115 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 596\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0601041125, Brand: Unknown, Forecast Accuracy: -58.38%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.059369 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 611\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0602011125, Brand: Unknown, Forecast Accuracy: -70.52%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.058355 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 584\n[LightGBM] [Info] Number of data points in the train set: 898, number of used features: 10\n[LightGBM] [Info] Start training from score 2.000000\nProductID: PVSL0604011025, Brand: Unknown, Forecast Accuracy: -24.06%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.067464 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 594\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\nProductID: PVSL0605031125, Brand: Unknown, Forecast Accuracy: -68.57%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.070124 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 602\n[LightGBM] [Info] Number of data points in the train set: 897, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\nProductID: PVSL0606021225, Brand: Unknown, Forecast Accuracy: -75.97%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.059379 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.079248 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 585\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0511050925, Brand: Unknown, Forecast Accuracy: -71.51%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.065223 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 593\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0512011125, Brand: Unknown, Forecast Accuracy: -81.42%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.046624 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 610\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0512041125, Brand: Unknown, Forecast Accuracy: -67.44%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.069174 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 610\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0513041225, Brand: Unknown, Forecast Accuracy: -68.35%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.051094 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 583\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 2.000000\nProductID: PVSL0513051225, Brand: Unknown, Forecast Accuracy: -50.19%\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.083941 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 583\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0514040126, Brand: Unknown, Forecast Accuracy: -67.11%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.071215 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 585\n[LightGBM] [Info] Number of data points in the train set: 899, number of used features: 10\n[LightGBM] [Info] Start training from score 2.000000\nProductID: PVSL0515011225, Brand: Unknown, Forecast Accuracy: -33.32%\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.046898 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 581\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 2.000000\nProductID: PVSL0515051225, Brand: Unknown, Forecast Accuracy: -23.60%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.071361 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 602\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0516021025, Brand: Unknown, Forecast Accuracy: -67.15%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.069367 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 589\n[LightGBM] [Info] Number of data points in the train set: 897, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\nProductID: PVSL0518031025, Brand: Unknown, Forecast Accuracy: -65.85%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.059393 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 590\n[LightGBM] [Info] Number of data points in the train set: 899, number of used features: 10\n[LightGBM] [Info] Start training from score 2.000000\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\nProductID: PVSL0601021125, Brand: Unknown, Forecast Accuracy: -37.75%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.078252 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 581\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0602041125, Brand: Unknown, Forecast Accuracy: -59.98%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.047379 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 574\n[LightGBM] [Info] Number of data points in the train set: 896, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\nProductID: PVSL0603031025, Brand: Unknown, Forecast Accuracy: -76.23%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.069179 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 593\n[LightGBM] [Info] Number of data points in the train set: 898, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\nProductID: PVSL0605021125, Brand: Unknown, Forecast Accuracy: -60.55%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.060248 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 599\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\nProductID: PVSL0606051225, Brand: Unknown, Forecast Accuracy: -61.03%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.059072 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 596\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0606061225, Brand: Unknown, Forecast Accuracy: -59.56%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.047321 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 591\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0607031225, Brand: Unknown, Forecast Accuracy: -65.99%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.058256 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 595\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 2.000000\nProductID: PVSL0607051225, Brand: Unknown, Forecast Accuracy: -25.94%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.055212 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 592\n[LightGBM] [Info] Number of data points in the train set: 893, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0608061025, Brand: Unknown, Forecast Accuracy: -65.38%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.049381 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 595\n[LightGBM] [Info] Number of data points in the train set: 898, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0609041125, Brand: Unknown, Forecast Accuracy: -74.73%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.059357 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 624\n[LightGBM] [Info] Number of data points in the train set: 896, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0610020925, Brand: Unknown, Forecast Accuracy: -73.88%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.080102 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 602\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0701010925, Brand: Unknown, Forecast Accuracy: -80.45%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.047377 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 600\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0702031225, Brand: Unknown, Forecast Accuracy: -70.19%\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.010005 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 601\n[LightGBM] [Info] Number of data points in the train set: 898, number of used features: 10\n[LightGBM] [Info] Start training from score 2.500000\nProductID: PVSL0702051225, Brand: Unknown, Forecast Accuracy: -46.31%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.057217 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 586\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0703061025, Brand: Unknown, Forecast Accuracy: -78.46%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.059373 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 581\n[LightGBM] [Info] Number of data points in the train set: 897, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0704021125, Brand: Unknown, Forecast Accuracy: -75.82%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.047360 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 598\n[LightGBM] [Info] Number of data points in the train set: 896, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0705030925, Brand: Unknown, Forecast Accuracy: -60.66%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.058848 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 594\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 4.000000\nProductID: PVSL0708031225, Brand: Unknown, Forecast Accuracy: -91.24%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.072335 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 591\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0709021125, Brand: Unknown, Forecast Accuracy: -68.12%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.047360 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 591\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0709041125, Brand: Unknown, Forecast Accuracy: -72.27%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.065247 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 614\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0710021225, Brand: Unknown, Forecast Accuracy: -65.42%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.047237 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 591\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\nProductID: PVSL0801050126, Brand: Unknown, Forecast Accuracy: -76.33%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.059359 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 564\n[LightGBM] [Info] Number of data points in the train set: 899, number of used features: 10\n[LightGBM] [Info] Start training from score 2.000000\nProductID: PVSL0802040925, Brand: Unknown, Forecast Accuracy: -43.47%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.056245 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 610\n[LightGBM] [Info] Number of data points in the train set: 899, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0802060925, Brand: Unknown, Forecast Accuracy: -68.26%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.047374 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 581\n[LightGBM] [Info] Number of data points in the train set: 899, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0804041025, Brand: Unknown, Forecast Accuracy: -71.00%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.068372 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 602\n[LightGBM] [Info] Number of data points in the train set: 897, number of used features: 10\n[LightGBM] [Info] Start training from score 2.000000\nProductID: PVSL0805041025, Brand: Unknown, Forecast Accuracy: -50.01%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.083137 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 598\n[LightGBM] [Info] Number of data points in the train set: 898, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0805051025, Brand: Unknown, Forecast Accuracy: -81.96%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.050350 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 591\n[LightGBM] [Info] Number of data points in the train set: 897, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0806011025, Brand: Unknown, Forecast Accuracy: -72.93%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.047374 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 581\n[LightGBM] [Info] Number of data points in the train set: 899, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0607061225, Brand: Unknown, Forecast Accuracy: -63.71%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.059153 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 598\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0608041025, Brand: Unknown, Forecast Accuracy: -58.53%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.071245 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 603\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 2.000000\nProductID: PVSL0609061125, Brand: Unknown, Forecast Accuracy: -55.03%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.068191 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 605\n[LightGBM] [Info] Number of data points in the train set: 899, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0610040925, Brand: Unknown, Forecast Accuracy: -67.72%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.067366 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 597\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\nProductID: PVSL0702061225, Brand: Unknown, Forecast Accuracy: -67.15%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.047355 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 599\n[LightGBM] [Info] Number of data points in the train set: 897, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0703051025, Brand: Unknown, Forecast Accuracy: -75.81%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.080248 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 590\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 2.000000\nProductID: PVSL0704061125, Brand: Unknown, Forecast Accuracy: -33.08%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.056373 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 582\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 2.000000\nProductID: PVSL0706041225, Brand: Unknown, Forecast Accuracy: -21.81%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.055190 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 588\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 4.000000\nProductID: PVSL0707061125, Brand: Unknown, Forecast Accuracy: -95.75%\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.045880 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 587\n[LightGBM] [Info] Number of data points in the train set: 898, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0708061225, Brand: Unknown, Forecast Accuracy: -66.65%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.047365 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 599\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 2.000000\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\nProductID: PVSL0709061125, Brand: Unknown, Forecast Accuracy: -32.33%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.059386 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 588\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 2.000000\nProductID: PVSL0710051225, Brand: Unknown, Forecast Accuracy: -34.58%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.063952 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 601\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0801020126, Brand: Unknown, Forecast Accuracy: -63.20%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.059390 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 581\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0802010925, Brand: Unknown, Forecast Accuracy: -67.23%\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.058888 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 598\n[LightGBM] [Info] Number of data points in the train set: 897, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0803031025, Brand: Unknown, Forecast Accuracy: -59.07%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.053386 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 598\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0804051025, Brand: Unknown, Forecast Accuracy: -73.49%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.061181 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 585\n[LightGBM] [Info] Number of data points in the train set: 896, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0805061025, Brand: Unknown, Forecast Accuracy: -85.31%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.071866 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 600\n[LightGBM] [Info] Number of data points in the train set: 898, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\nProductID: PVSL0807010126, Brand: Unknown, Forecast Accuracy: -62.57%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.056288 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 588\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0808021125, Brand: Unknown, Forecast Accuracy: -62.26%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.064235 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 582\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Total Bins 579\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\nProductID: PVSL0607011225, Brand: Unknown, Forecast Accuracy: -43.86%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.047205 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 604\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0608021025, Brand: Unknown, Forecast Accuracy: -60.95%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.069195 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 595\n[LightGBM] [Info] Number of data points in the train set: 898, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0610030925, Brand: Unknown, Forecast Accuracy: -70.66%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.048397 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 579\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 4.000000\nProductID: PVSL0701030925, Brand: Unknown, Forecast Accuracy: -113.85%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.047149 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 608\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0703041025, Brand: Unknown, Forecast Accuracy: -64.10%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.047356 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 594\n[LightGBM] [Info] Number of data points in the train set: 899, number of used features: 10\n[LightGBM] [Info] Start training from score 2.000000\nProductID: PVSL0705010925, Brand: Unknown, Forecast Accuracy: -24.20%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.062240 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 608\n[LightGBM] [Info] Number of data points in the train set: 899, number of used features: 10\n[LightGBM] [Info] Start training from score 4.000000\nProductID: PVSL0706051225, Brand: Unknown, Forecast Accuracy: -115.01%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.046387 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 602\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0707011125, Brand: Unknown, Forecast Accuracy: -37.59%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.062230 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 580\n[LightGBM] [Info] Number of data points in the train set: 896, number of used features: 10\n[LightGBM] [Info] Start training from score 2.000000\nProductID: PVSL0708011225, Brand: Unknown, Forecast Accuracy: -48.17%\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.036592 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 603\n[LightGBM] [Info] Number of data points in the train set: 899, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0710031225, Brand: Unknown, Forecast Accuracy: -68.09%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.056216 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 590\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0801040126, Brand: Unknown, Forecast Accuracy: -60.46%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.054380 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 597\n[LightGBM] [Info] Number of data points in the train set: 898, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0801060126, Brand: Unknown, Forecast Accuracy: -51.14%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.054254 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 604\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0803011025, Brand: Unknown, Forecast Accuracy: -71.87%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.047191 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 601\n[LightGBM] [Info] Number of data points in the train set: 899, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0803061025, Brand: Unknown, Forecast Accuracy: -72.82%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001404 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 601\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0805021025, Brand: Unknown, Forecast Accuracy: -68.01%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.059202 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 611\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0806021025, Brand: Unknown, Forecast Accuracy: -67.22%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.068251 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 607\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 2.000000\nProductID: PVSL0806051025, Brand: Unknown, Forecast Accuracy: -24.91%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.059152 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 614\n[LightGBM] [Info] Number of data points in the train set: 899, number of used features: 10\n[LightGBM] [Info] Start training from score 2.000000\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\nProductID: PVSL0807020126, Brand: Unknown, Forecast Accuracy: -28.64%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.059374 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 603\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 4.000000\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\nProductID: PVSL0808051125, Brand: Unknown, Forecast Accuracy: -106.77%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.049977 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 593\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 2.000000\n[LightGBM] [Info] Total Bins 612\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 2.500000\nProductID: PVSL0607021225, Brand: Unknown, Forecast Accuracy: -52.13%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.059356 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 595\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\nProductID: PVSL0608011025, Brand: Unknown, Forecast Accuracy: -63.86%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.074281 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 603\n[LightGBM] [Info] Number of data points in the train set: 899, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0609031125, Brand: Unknown, Forecast Accuracy: -60.89%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.059261 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 619\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 2.000000\nProductID: PVSL0610010925, Brand: Unknown, Forecast Accuracy: -53.68%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.045184 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 603\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0702011225, Brand: Unknown, Forecast Accuracy: -102.37%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.038234 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 585\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0705060925, Brand: Unknown, Forecast Accuracy: -70.48%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.052176 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 600\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0707041125, Brand: Unknown, Forecast Accuracy: -65.99%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.059399 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 605\n[LightGBM] [Info] Number of data points in the train set: 899, number of used features: 10\n[LightGBM] [Info] Start training from score 2.000000\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\nProductID: PVSL0708041225, Brand: Unknown, Forecast Accuracy: -29.75%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.065141 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 597\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0710011225, Brand: Unknown, Forecast Accuracy: -57.31%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.057192 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 580\n[LightGBM] [Info] Number of data points in the train set: 899, number of used features: 10\n[LightGBM] [Info] Start training from score 2.000000\nProductID: PVSL0801010126, Brand: Unknown, Forecast Accuracy: -53.38%\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.067861 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 579\n[LightGBM] [Info] Number of data points in the train set: 896, number of used features: 10\n[LightGBM] [Info] Start training from score 2.000000\nProductID: PVSL0802050925, Brand: Unknown, Forecast Accuracy: -32.17%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.048371 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 563\n[LightGBM] [Info] Number of data points in the train set: 897, number of used features: 10\n[LightGBM] [Info] Start training from score 2.000000\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\nProductID: PVSL0804031025, Brand: Unknown, Forecast Accuracy: -28.25%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.062247 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 598\n[LightGBM] [Info] Number of data points in the train set: 899, number of used features: 10\n[LightGBM] [Info] Start training from score 2.000000\nProductID: PVSL0804061025, Brand: Unknown, Forecast Accuracy: -76.58%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.060166 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 605\n[LightGBM] [Info] Number of data points in the train set: 899, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0806031025, Brand: Unknown, Forecast Accuracy: -81.43%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.059384 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 599\n[LightGBM] [Info] Number of data points in the train set: 898, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0807050126, Brand: Unknown, Forecast Accuracy: -59.70%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.055342 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 597\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0807060126, Brand: Unknown, Forecast Accuracy: -58.13%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.059319 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 603\n[LightGBM] [Info] Number of data points in the train set: 899, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0808031125, Brand: Unknown, Forecast Accuracy: -60.02%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.045142 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 590\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 2.000000\nProductID: PVSL0808061125, Brand: Unknown, Forecast Accuracy: -30.39%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.059216 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 737\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 13.000000\nProductID: PVSL0901011025, Brand: Unknown, Forecast Accuracy: -140.53%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.042348 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 601\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0809040925, Brand: Unknown, Forecast Accuracy: -27.70%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.047235 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 588\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0809060925, Brand: Unknown, Forecast Accuracy: -56.49%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.068354 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 682\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 8.000000\nProductID: PVSL0901030126, Brand: Unknown, Forecast Accuracy: -153.85%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.059397 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 677\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 8.000000\nProductID: PVSL0901060925, Brand: Unknown, Forecast Accuracy: -109.49%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.061296 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 590\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 2.000000\nProductID: PVSL0902021025, Brand: Unknown, Forecast Accuracy: -23.82%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.047212 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 593\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0902041025, Brand: Unknown, Forecast Accuracy: -54.52%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.060444 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 697\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 8.000000\nProductID: PVSL0902050126, Brand: Unknown, Forecast Accuracy: -137.15%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.059398 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 602\n[LightGBM] [Info] Number of data points in the train set: 899, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0902060126, Brand: Unknown, Forecast Accuracy: -72.77%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.047365 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 672\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 9.000000\nProductID: PVSL0903011225, Brand: Unknown, Forecast Accuracy: -131.40%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.046354 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 596\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0903041125, Brand: Unknown, Forecast Accuracy: -50.25%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.056198 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 685\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 9.000000\nProductID: PVSL0903050126, Brand: Unknown, Forecast Accuracy: -119.36%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.047373 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 687\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 9.000000\nProductID: PVSL0903060126, Brand: Unknown, Forecast Accuracy: -131.27%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.060291 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 609\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0903061025, Brand: Unknown, Forecast Accuracy: -55.45%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.075828 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 732\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 14.000000\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\nProductID: PVSL0904021225, Brand: Unknown, Forecast Accuracy: -91.64%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.056300 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 610\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0904031125, Brand: Unknown, Forecast Accuracy: -69.59%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.048356 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 670\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 9.000000\nProductID: PVSL0904050925, Brand: Unknown, Forecast Accuracy: -127.80%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.071207 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 680\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 8.000000\nProductID: PVSL0904060925, Brand: Unknown, Forecast Accuracy: -74.73%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.043301 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 583\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 2.000000\nProductID: PVSL0904061025, Brand: Unknown, Forecast Accuracy: -31.75%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.047340 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 600\n[LightGBM] [Info] Number of data points in the train set: 899, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0905010925, Brand: Unknown, Forecast Accuracy: -75.98%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.045086 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 689\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 9.000000\nProductID: PVSL0905041125, Brand: Unknown, Forecast Accuracy: -117.49%\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.011883 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 595\n[LightGBM] [Info] Number of data points in the train set: 898, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\nProductID: PVSL0809020925, Brand: Unknown, Forecast Accuracy: -71.66%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.047248 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 596\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0809050925, Brand: Unknown, Forecast Accuracy: -57.36%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.087193 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 687\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 8.000000\nProductID: PVSL0901020925, Brand: Unknown, Forecast Accuracy: -110.54%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.063103 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 604\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0901051225, Brand: Unknown, Forecast Accuracy: -63.68%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.059349 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 597\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0901061125, Brand: Unknown, Forecast Accuracy: -83.63%\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001921 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 604\n[LightGBM] [Info] Number of data points in the train set: 897, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0902021225, Brand: Unknown, Forecast Accuracy: -71.73%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.044225 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 704\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 8.000000\nProductID: PVSL0902060925, Brand: Unknown, Forecast Accuracy: -93.55%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.059363 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 600\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0903020126, Brand: Unknown, Forecast Accuracy: -57.89%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.055184 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 664\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 8.000000\nProductID: PVSL0903021225, Brand: Unknown, Forecast Accuracy: -109.97%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.070187 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 588\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\nProductID: PVSL0903040126, Brand: Unknown, Forecast Accuracy: -59.54%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.050350 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 590\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 2.500000\nProductID: PVSL0903051125, Brand: Unknown, Forecast Accuracy: -50.83%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.042325 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 591\n[LightGBM] [Info] Number of data points in the train set: 898, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0904011125, Brand: Unknown, Forecast Accuracy: -52.44%\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.031993 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 685\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 8.500000\nProductID: PVSL0904030925, Brand: Unknown, Forecast Accuracy: -112.07%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.071225 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 689\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 8.000000\nProductID: PVSL0904050126, Brand: Unknown, Forecast Accuracy: -81.07%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.064240 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 616\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0904060126, Brand: Unknown, Forecast Accuracy: -66.78%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.062179 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 680\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 8.000000\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\nProductID: PVSL0905020126, Brand: Unknown, Forecast Accuracy: -104.28%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.068134 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 747\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 13.000000\nProductID: PVSL0905031025, Brand: Unknown, Forecast Accuracy: -60.28%\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.050987 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 605\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0905051025, Brand: Unknown, Forecast Accuracy: -67.75%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.059524 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 701\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 9.000000\nProductID: PVSL0906020925, Brand: Unknown, Forecast Accuracy: -153.30%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.065666 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 591\n[LightGBM] [Info] Number of data points in the train set: 899, number of used features: 10\n[LightGBM] [Info] Total Bins 589\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0807040126, Brand: Unknown, Forecast Accuracy: -53.33%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.071199 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 610\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\nProductID: PVSL0808011125, Brand: Unknown, Forecast Accuracy: -63.25%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.075166 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 584\n[LightGBM] [Info] Number of data points in the train set: 899, number of used features: 10\n[LightGBM] [Info] Start training from score 2.000000\nProductID: PVSL0809030925, Brand: Unknown, Forecast Accuracy: -29.58%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.056334 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 594\n[LightGBM] [Info] Number of data points in the train set: 899, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0901011125, Brand: Unknown, Forecast Accuracy: -67.23%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.057236 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 700\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 9.000000\nProductID: PVSL0901021125, Brand: Unknown, Forecast Accuracy: -138.25%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.050358 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 687\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 8.000000\nProductID: PVSL0901041225, Brand: Unknown, Forecast Accuracy: -108.02%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.047613 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 695\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 8.000000\nProductID: PVSL0902011025, Brand: Unknown, Forecast Accuracy: -98.34%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.084351 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 765\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 12.000000\nProductID: PVSL0902031125, Brand: Unknown, Forecast Accuracy: -96.53%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.070247 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 611\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 2.000000\nProductID: PVSL0902050925, Brand: Unknown, Forecast Accuracy: -32.13%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.087891 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 711\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 9.000000\nProductID: PVSL0902051025, Brand: Unknown, Forecast Accuracy: -148.19%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.059711 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 584\n[LightGBM] [Info] Number of data points in the train set: 899, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0903021025, Brand: Unknown, Forecast Accuracy: -63.67%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.035137 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 653\n[LightGBM] [Info] Number of data points in the train set: 896, number of used features: 10\n[LightGBM] [Info] Start training from score 8.000000\nProductID: PVSL0903030126, Brand: Unknown, Forecast Accuracy: -122.22%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.057041 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 695\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 8.000000\nProductID: PVSL0903051225, Brand: Unknown, Forecast Accuracy: -109.05%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.060373 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 577\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0904020126, Brand: Unknown, Forecast Accuracy: -72.86%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.070251 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 764\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 13.000000\nProductID: PVSL0904041125, Brand: Unknown, Forecast Accuracy: -95.78%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.071227 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 599\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0904051125, Brand: Unknown, Forecast Accuracy: -69.48%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.053088 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 661\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 8.000000\nProductID: PVSL0905010126, Brand: Unknown, Forecast Accuracy: -103.67%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.070109 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 590\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0905021225, Brand: Unknown, Forecast Accuracy: -73.21%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.058280 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 687\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 9.000000\nProductID: PVSL0905061125, Brand: Unknown, Forecast Accuracy: -95.69%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.086119 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 668\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 8.000000\nProductID: PVSL0906020126, Brand: Unknown, Forecast Accuracy: -108.11%\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.057905 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\nProductID: PVSL0901031025, Brand: Unknown, Forecast Accuracy: -55.99%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.062211 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 740\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 13.000000\nProductID: PVSL0901050126, Brand: Unknown, Forecast Accuracy: -110.44%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.050349 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 605\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0902011125, Brand: Unknown, Forecast Accuracy: -49.44%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.047407 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 589\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0902030126, Brand: Unknown, Forecast Accuracy: -54.40%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.048205 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 691\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 8.500000\nProductID: PVSL0902041225, Brand: Unknown, Forecast Accuracy: -111.04%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.077223 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 683\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 8.000000\nProductID: PVSL0902061225, Brand: Unknown, Forecast Accuracy: -138.88%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.071232 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 676\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 9.000000\nProductID: PVSL0903030925, Brand: Unknown, Forecast Accuracy: -128.15%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.071152 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 572\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0903041225, Brand: Unknown, Forecast Accuracy: -58.67%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.107200 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 691\n[LightGBM] [Info] Number of data points in the train set: 897, number of used features: 10\n[LightGBM] [Info] Start training from score 8.000000\nProductID: PVSL0903060925, Brand: Unknown, Forecast Accuracy: -101.56%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.050324 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 599\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 4.000000\nProductID: PVSL0904010925, Brand: Unknown, Forecast Accuracy: -103.23%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.065340 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 664\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 9.000000\nProductID: PVSL0904031225, Brand: Unknown, Forecast Accuracy: -143.44%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.053285 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 584\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0905021125, Brand: Unknown, Forecast Accuracy: -63.37%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.063295 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 686\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 9.000000\nProductID: PVSL0905041225, Brand: Unknown, Forecast Accuracy: -132.82%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.071073 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 600\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0905051125, Brand: Unknown, Forecast Accuracy: -64.72%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.055371 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 689\n[LightGBM] [Info] Number of data points in the train set: 899, number of used features: 10\n[LightGBM] [Info] Start training from score 8.000000\nProductID: PVSL0905061225, Brand: Unknown, Forecast Accuracy: -74.79%\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.053948 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 685\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 8.000000\nProductID: PVSL0906031225, Brand: Unknown, Forecast Accuracy: -92.74%\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.048905 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 604\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0906041225, Brand: Unknown, Forecast Accuracy: -61.77%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.066147 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 573\n[LightGBM] [Info] Number of data points in the train set: 899, number of used features: 10\n[LightGBM] [Info] Start training from score 2.000000\nProductID: PVSL0906051225, Brand: Unknown, Forecast Accuracy: -27.59%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.063197 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 672\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 9.000000\nProductID: PVSL0907011025, Brand: Unknown, Forecast Accuracy: -142.05%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.071805 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 676\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 8.000000\nProductID: PVSL0907021225, Brand: Unknown, Forecast Accuracy: -102.39%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.071148 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 595\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0905051225, Brand: Unknown, Forecast Accuracy: -60.26%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.045196 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 676\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 8.000000\nProductID: PVSL0906010925, Brand: Unknown, Forecast Accuracy: -80.90%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.070790 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 604\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0906041025, Brand: Unknown, Forecast Accuracy: -60.62%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.072214 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 576\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0906061125, Brand: Unknown, Forecast Accuracy: -69.84%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.048212 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 584\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 4.000000\nProductID: PVSL0907021025, Brand: Unknown, Forecast Accuracy: -98.98%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.047205 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 684\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 8.000000\nProductID: PVSL0907031125, Brand: Unknown, Forecast Accuracy: -120.69%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.047269 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 583\n[LightGBM] [Info] Number of data points in the train set: 899, number of used features: 10\n[LightGBM] [Info] Start training from score 1.000000\nProductID: PVSL0907040126, Brand: Unknown, Forecast Accuracy: 9.13%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.062352 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 667\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 8.000000\nProductID: PVSL0907061025, Brand: Unknown, Forecast Accuracy: -100.47%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.104059 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 747\n[LightGBM] [Info] Number of data points in the train set: 899, number of used features: 10\n[LightGBM] [Info] Start training from score 13.000000\nProductID: PVSL0908020126, Brand: Unknown, Forecast Accuracy: -112.29%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.056255 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 595\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 2.000000\nProductID: PVSL0908021225, Brand: Unknown, Forecast Accuracy: -34.02%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.065172 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 613\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0908051025, Brand: Unknown, Forecast Accuracy: -67.42%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.047251 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 602\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 2.000000\nProductID: PVSL0908061025, Brand: Unknown, Forecast Accuracy: -51.13%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.051835 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 689\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 9.000000\nProductID: PVSL0909020925, Brand: Unknown, Forecast Accuracy: -129.27%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.083244 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 582\n[LightGBM] [Info] Number of data points in the train set: 899, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\nProductID: PVSL0909031125, Brand: Unknown, Forecast Accuracy: -70.45%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.058216 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 809\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 17.000000\nProductID: PVSL0909041025, Brand: Unknown, Forecast Accuracy: -73.17%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.076157 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 712\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 9.000000\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\nProductID: PVSL0910020925, Brand: Unknown, Forecast Accuracy: -144.54%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.067255 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 683\n[LightGBM] [Info] Number of data points in the train set: 899, number of used features: 10\n[LightGBM] [Info] Start training from score 8.000000\nProductID: PVSL0910021225, Brand: Unknown, Forecast Accuracy: -111.30%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.081159 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 683\n[LightGBM] [Info] Number of data points in the train set: 899, number of used features: 10\n[LightGBM] [Info] Start training from score 8.000000\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\nProductID: PVSL0910060126, Brand: Unknown, Forecast Accuracy: -72.37%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.059235 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 686\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 9.000000\nProductID: PVSL0910061125, Brand: Unknown, Forecast Accuracy: -112.70%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.059243 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 609\n[LightGBM] [Info] Number of data points in the train set: 898, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0108041025, Brand: Unknown, Forecast Accuracy: -64.95%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.047349 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 599\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0906040925, Brand: Unknown, Forecast Accuracy: -67.22%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.065068 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 583\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0906041125, Brand: Unknown, Forecast Accuracy: -84.69%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.055573 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 605\n[LightGBM] [Info] Number of data points in the train set: 899, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0906050925, Brand: Unknown, Forecast Accuracy: -61.57%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.071257 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 754\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 13.000000\nProductID: PVSL0906061225, Brand: Unknown, Forecast Accuracy: -84.94%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.057918 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 620\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 2.000000\nProductID: PVSL0907030126, Brand: Unknown, Forecast Accuracy: -26.30%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.061312 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 675\n[LightGBM] [Info] Number of data points in the train set: 899, number of used features: 10\n[LightGBM] [Info] Start training from score 8.000000\nProductID: PVSL0907040925, Brand: Unknown, Forecast Accuracy: -98.15%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.067144 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 575\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 2.000000\nProductID: PVSL0908011125, Brand: Unknown, Forecast Accuracy: -20.06%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.068162 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 581\n[LightGBM] [Info] Number of data points in the train set: 898, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0908021025, Brand: Unknown, Forecast Accuracy: -74.62%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.060220 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 587\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0908031025, Brand: Unknown, Forecast Accuracy: -60.91%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.060569 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 705\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 9.000000\nProductID: PVSL0908041025, Brand: Unknown, Forecast Accuracy: -154.78%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.059762 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 586\n[LightGBM] [Info] Number of data points in the train set: 899, number of used features: 10\n[LightGBM] [Info] Start training from score 2.000000\nProductID: PVSL0908060925, Brand: Unknown, Forecast Accuracy: -31.75%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.060517 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 681\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 8.000000\nProductID: PVSL0909011125, Brand: Unknown, Forecast Accuracy: -151.32%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.058593 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 747\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 14.000000\nProductID: PVSL0909021125, Brand: Unknown, Forecast Accuracy: -46.65%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.044399 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 684\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 9.000000\nProductID: PVSL0909031225, Brand: Unknown, Forecast Accuracy: -97.91%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.068123 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 601\n[LightGBM] [Info] Number of data points in the train set: 899, number of used features: 10\n[LightGBM] [Info] Start training from score 2.000000\nProductID: PVSL0909051225, Brand: Unknown, Forecast Accuracy: -29.86%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.047315 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 694\n[LightGBM] [Info] Number of data points in the train set: 897, number of used features: 10\n[LightGBM] [Info] Start training from score 8.000000\nProductID: PVSL0910030126, Brand: Unknown, Forecast Accuracy: -89.90%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.072357 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 578\n[LightGBM] [Info] Number of data points in the train set: 898, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0910040925, Brand: Unknown, Forecast Accuracy: -67.32%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.071162 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 675\n[LightGBM] [Info] Number of data points in the train set: 899, number of used features: 10\n[LightGBM] [Info] Start training from score 9.000000\nProductID: PVSL0910051225, Brand: Unknown, Forecast Accuracy: -189.40%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.046213 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 583\n[LightGBM] [Info] Number of data points in the train set: 897, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0105041025, Brand: Unknown, Forecast Accuracy: -56.89%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.059367 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 581\n[LightGBM] [Info] Number of data points in the train set: 899, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0108011025, Brand: Unknown, Forecast Accuracy: -63.89%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.080212 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 573\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0906021125, Brand: Unknown, Forecast Accuracy: -57.67%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.052033 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 603\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0906050126, Brand: Unknown, Forecast Accuracy: -54.28%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.072106 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 693\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 8.000000\nProductID: PVSL0907021125, Brand: Unknown, Forecast Accuracy: -90.90%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.057143 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 684\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 8.000000\nProductID: PVSL0907031225, Brand: Unknown, Forecast Accuracy: -113.50%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.070955 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 702\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 8.500000\nProductID: PVSL0907051025, Brand: Unknown, Forecast Accuracy: -156.80%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.059322 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 673\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 8.000000\nProductID: PVSL0908010126, Brand: Unknown, Forecast Accuracy: -92.93%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.065160 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 668\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 9.000000\nProductID: PVSL0908030925, Brand: Unknown, Forecast Accuracy: -126.89%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.062353 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 611\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 2.000000\nProductID: PVSL0908040925, Brand: Unknown, Forecast Accuracy: -26.98%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.063234 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 606\n[LightGBM] [Info] Number of data points in the train set: 898, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0908061225, Brand: Unknown, Forecast Accuracy: -68.35%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.049169 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 602\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0909040925, Brand: Unknown, Forecast Accuracy: -55.14%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.050339 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 594\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 2.000000\nProductID: PVSL0909051025, Brand: Unknown, Forecast Accuracy: -46.17%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.068230 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 683\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 9.000000\nProductID: PVSL0909061125, Brand: Unknown, Forecast Accuracy: -128.26%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.074251 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 672\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 8.000000\nProductID: PVSL0910031225, Brand: Unknown, Forecast Accuracy: -77.58%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.055362 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 750\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 13.000000\nProductID: PVSL0910041025, Brand: Unknown, Forecast Accuracy: -126.95%\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.031202 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 684\n[LightGBM] [Info] Number of data points in the train set: 899, number of used features: 10\n[LightGBM] [Info] Start training from score 8.000000\nProductID: PVSL0910051125, Brand: Unknown, Forecast Accuracy: -111.31%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.059175 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 590\n[LightGBM] [Info] Number of data points in the train set: 899, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0104010126, Brand: Unknown, Forecast Accuracy: -72.39%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.077247 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 597\n[LightGBM] [Info] Number of data points in the train set: 899, number of used features: 10\n[LightGBM] [Info] Start training from score 2.000000\nProductID: PVSL0110020126, Brand: Unknown, Forecast Accuracy: -34.42%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.065189 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 611\n[LightGBM] [Info] Number of data points in the train set: 897, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0113021125, Brand: Unknown, Forecast Accuracy: -66.64%\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.050909 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 606\n[LightGBM] [Info] Number of data points in the train set: 899, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0114031125, Brand: Unknown, Forecast Accuracy: -68.72%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.046191 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 597\n[LightGBM] [Info] Number of data points in the train set: 898, number of used features: 10\n[LightGBM] [Info] Start training from score 4.000000\nProductID: PVSL0116051125, Brand: Unknown, Forecast Accuracy: -82.22%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.047339 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n\n[LightGBM] [Info] Start training from score 3.000000\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\nProductID: PVSL0907050925, Brand: Unknown, Forecast Accuracy: -64.95%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.066359 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 687\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 9.000000\nProductID: PVSL0907051225, Brand: Unknown, Forecast Accuracy: -127.17%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.049063 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 671\n[LightGBM] [Info] Number of data points in the train set: 899, number of used features: 10\n[LightGBM] [Info] Start training from score 8.000000\nProductID: PVSL0908030126, Brand: Unknown, Forecast Accuracy: -101.07%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.070241 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 594\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0908040126, Brand: Unknown, Forecast Accuracy: -61.93%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.047377 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 703\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 9.000000\nProductID: PVSL0908051125, Brand: Unknown, Forecast Accuracy: -139.47%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.083677 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 684\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 8.000000\nProductID: PVSL0909031025, Brand: Unknown, Forecast Accuracy: -101.44%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.065149 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 691\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 8.000000\nProductID: PVSL0909050925, Brand: Unknown, Forecast Accuracy: -94.05%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.075180 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 765\n[LightGBM] [Info] Number of data points in the train set: 900, number of used features: 10\n[LightGBM] [Info] Start training from score 12.000000\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\nProductID: PVSL0910011025, Brand: Unknown, Forecast Accuracy: -99.31%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.057385 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 588\n[LightGBM] [Info] Number of data points in the train set: 899, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0910041225, Brand: Unknown, Forecast Accuracy: -48.41%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.054360 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 613\n[LightGBM] [Info] Number of data points in the train set: 895, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0910060925, Brand: Unknown, Forecast Accuracy: -60.45%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.062172 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 585\n[LightGBM] [Info] Number of data points in the train set: 897, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0107011025, Brand: Unknown, Forecast Accuracy: -69.60%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.051110 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 599\n[LightGBM] [Info] Number of data points in the train set: 898, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\nProductID: PVSL0110050126, Brand: Unknown, Forecast Accuracy: -65.98%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.041323 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 596\n[LightGBM] [Info] Number of data points in the train set: 899, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0115021125, Brand: Unknown, Forecast Accuracy: -60.05%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.083554 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 617\n[LightGBM] [Info] Number of data points in the train set: 899, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0117020126, Brand: Unknown, Forecast Accuracy: -68.82%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.053354 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 611\n[LightGBM] [Info] Number of data points in the train set: 899, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0122041125, Brand: Unknown, Forecast Accuracy: -63.48%\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.046901 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 577\n[LightGBM] [Info] Number of data points in the train set: 899, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0126061025, Brand: Unknown, Forecast Accuracy: -53.79%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.068246 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 580\n[LightGBM] [Info] Number of data points in the train set: 899, number of used features: 10\n[LightGBM] [Info] Start training from score 2.000000\nProductID: PVSL0130051025, Brand: Unknown, Forecast Accuracy: -39.10%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.054401 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 584\n[LightGBM] [Info] Number of data points in the train set: 899, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0138021225, Brand: Unknown, Forecast Accuracy: -43.45%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.059246 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 584\n[LightGBM] [Info] Number of data points in the train set: 899, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0139020126, Brand: Unknown, Forecast Accuracy: -69.73%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.069067 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 585\n[LightGBM] [Info] Number of data points in the train set: 899, number of used features: 10\n[LightGBM] [Info] Start training from score 2.000000\nProductID: PVSL0201021025, Brand: Unknown, Forecast Accuracy: -37.44%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.056052 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Number of data points in the train set: 899, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0114011125, Brand: Unknown, Forecast Accuracy: -62.01%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.070293 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 598\n[LightGBM] [Info] Number of data points in the train set: 898, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\nProductID: PVSL0116011125, Brand: Unknown, Forecast Accuracy: -52.87%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.047245 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 592\n[LightGBM] [Info] Number of data points in the train set: 899, number of used features: 10\n[LightGBM] [Info] Start training from score 2.000000\nProductID: PVSL0117010126, Brand: Unknown, Forecast Accuracy: -48.18%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.071250 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 589\n[LightGBM] [Info] Number of data points in the train set: 898, number of used features: 10\n[LightGBM] [Info] Start training from score 2.000000\nProductID: PVSL0121030925, Brand: Unknown, Forecast Accuracy: -30.92%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.046264 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 606\n[LightGBM] [Info] Number of data points in the train set: 897, number of used features: 10\n[LightGBM] [Info] Start training from score 2.000000\nProductID: PVSL0124030925, Brand: Unknown, Forecast Accuracy: -39.45%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.060372 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 589\n[LightGBM] [Info] Number of data points in the train set: 897, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\nProductID: PVSL0129041025, Brand: Unknown, Forecast Accuracy: -63.75%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.069300 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 596\n[LightGBM] [Info] Number of data points in the train set: 899, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0134041125, Brand: Unknown, Forecast Accuracy: -67.49%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.047379 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 595\n[LightGBM] [Info] Number of data points in the train set: 898, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0136010925, Brand: Unknown, Forecast Accuracy: -54.92%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.049973 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 586\n[LightGBM] [Info] Number of data points in the train set: 898, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\nProductID: PVSL0139030126, Brand: Unknown, Forecast Accuracy: -37.62%\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.053075 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 594\n[LightGBM] [Info] Number of data points in the train set: 895, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\nProductID: PVSL0201051025, Brand: Unknown, Forecast Accuracy: -69.31%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.064701 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 591\n[LightGBM] [Info] Number of data points in the train set: 898, number of used features: 10\n[LightGBM] [Info] Start training from score 2.000000\nProductID: PVSL0202021125, Brand: Unknown, Forecast Accuracy: -24.61%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.047053 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 599\n[LightGBM] [Info] Number of data points in the train set: 899, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0204021225, Brand: Unknown, Forecast Accuracy: -36.08%\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.051992 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 608\n[LightGBM] [Info] Number of data points in the train set: 899, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0207021125, Brand: Unknown, Forecast Accuracy: -56.76%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.068145 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 594\n[LightGBM] [Info] Number of data points in the train set: 899, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\nProductID: PVSL0211031025, Brand: Unknown, Forecast Accuracy: -73.76%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.056937 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 587\n[LightGBM] [Info] Number of data points in the train set: 899, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0213011225, Brand: Unknown, Forecast Accuracy: -66.29%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.068072 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 593\n[LightGBM] [Info] Number of data points in the train set: 899, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0217011225, Brand: Unknown, Forecast Accuracy: -64.43%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.059335 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 595\n[LightGBM] [Info] Number of data points in the train set: 899, number of used features: 10\n[LightGBM] [Info] Start training from score 2.000000\nProductID: PVSL0220041025, Brand: Unknown, Forecast Accuracy: -44.71%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.059118 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 598\n[LightGBM] [Info] Number of data points in the train set: 899, number of used features: 10\n[LightGBM] [Info] Start training from score 4.000000\nProductID: PVSL0301050925, Brand: Unknown, Forecast Accuracy: -104.69%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.061019 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 582\n[LightGBM] [Info] Number of data points in the train set: 899, number of used features: 10\n[LightGBM] [Info] Start training from score 2.000000\nProductID: PVSL0304031025, Brand: Unknown, Forecast Accuracy: -35.82%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.047357 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 594\n[LightGBM] [Info] Number of data points in the train set: 899, number of used features: 10\n[LightGBM] [Info] Start training from score 4.000000\nProductID: PVSL0110030126, Brand: Unknown, Forecast Accuracy: -94.41%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.082139 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 597\n[LightGBM] [Info] Number of data points in the train set: 898, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0115061125, Brand: Unknown, Forecast Accuracy: -61.30%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.062118 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 586\n[LightGBM] [Info] Number of data points in the train set: 898, number of used features: 10\n[LightGBM] [Info] Start training from score 2.000000\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\nProductID: PVSL0120010126, Brand: Unknown, Forecast Accuracy: -52.99%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.070356 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 588\n[LightGBM] [Info] Number of data points in the train set: 897, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0124010925, Brand: Unknown, Forecast Accuracy: -56.65%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.059332 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 613\n[LightGBM] [Info] Number of data points in the train set: 898, number of used features: 10\n[LightGBM] [Info] Start training from score 2.000000\nProductID: PVSL0127021025, Brand: Unknown, Forecast Accuracy: -31.79%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.068163 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 602\n[LightGBM] [Info] Number of data points in the train set: 897, number of used features: 10\n[LightGBM] [Info] Start training from score 2.000000\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\nProductID: PVSL0127051025, Brand: Unknown, Forecast Accuracy: -16.69%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.071173 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 586\n[LightGBM] [Info] Number of data points in the train set: 899, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0133041025, Brand: Unknown, Forecast Accuracy: -73.77%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.059370 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 599\n[LightGBM] [Info] Number of data points in the train set: 896, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\nProductID: PVSL0139010126, Brand: Unknown, Forecast Accuracy: -68.05%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.070151 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 598\n[LightGBM] [Info] Number of data points in the train set: 898, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0141021125, Brand: Unknown, Forecast Accuracy: -71.61%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.074207 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 599\n[LightGBM] [Info] Number of data points in the train set: 898, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0203040126, Brand: Unknown, Forecast Accuracy: -61.20%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.058292 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 592\n[LightGBM] [Info] Number of data points in the train set: 899, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0206051225, Brand: Unknown, Forecast Accuracy: -65.56%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.071214 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 598\n[LightGBM] [Info] Number of data points in the train set: 898, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0207051125, Brand: Unknown, Forecast Accuracy: -56.89%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.070925 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 612\n[LightGBM] [Info] Number of data points in the train set: 899, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0212010126, Brand: Unknown, Forecast Accuracy: -61.22%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.059338 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 617\n[LightGBM] [Info] Number of data points in the train set: 899, number of used features: 10\n[LightGBM] [Info] Start training from score 4.000000\nProductID: PVSL0213061225, Brand: Unknown, Forecast Accuracy: -92.47%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.073361 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 586\n[LightGBM] [Info] Number of data points in the train set: 898, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0220031025, Brand: Unknown, Forecast Accuracy: -82.50%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.047037 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 594\n[LightGBM] [Info] Number of data points in the train set: 899, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\nProductID: PVSL0301040925, Brand: Unknown, Forecast Accuracy: -71.92%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.045335 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 595\n[LightGBM] [Info] Number of data points in the train set: 897, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0305010925, Brand: Unknown, Forecast Accuracy: -62.43%\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.037992 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 598\n[LightGBM] [Info] Number of data points in the train set: 899, number of used features: 10\n[LightGBM] [Info] Start training from score 2.000000\nProductID: PVSL0311031025, Brand: Unknown, Forecast Accuracy: -21.51%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.068028 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 588\n[LightGBM] [Info] Number of data points in the train set: 898, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0312060126, Brand: Unknown, Forecast Accuracy: -54.56%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.057284 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 598\n[LightGBM] [Info] Number of data points in the train set: 899, number of used features: 10[LightGBM] [Info] Total Bins 585\n[LightGBM] [Info] Number of data points in the train set: 899, number of used features: 10\n[LightGBM] [Info] Start training from score 2.000000\nProductID: PVSL0123050925, Brand: Unknown, Forecast Accuracy: -27.94%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.052404 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 597\n[LightGBM] [Info] Number of data points in the train set: 896, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0124020925, Brand: Unknown, Forecast Accuracy: -65.05%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.090179 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 594\n[LightGBM] [Info] Number of data points in the train set: 899, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0129051025, Brand: Unknown, Forecast Accuracy: -78.89%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.047139 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 597\n[LightGBM] [Info] Number of data points in the train set: 897, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0134031125, Brand: Unknown, Forecast Accuracy: -57.72%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.061222 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 598\n[LightGBM] [Info] Number of data points in the train set: 899, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0137021225, Brand: Unknown, Forecast Accuracy: -67.11%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.070972 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 600\n[LightGBM] [Info] Number of data points in the train set: 899, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0140020925, Brand: Unknown, Forecast Accuracy: -51.73%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000148 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 595\n[LightGBM] [Info] Number of data points in the train set: 897, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0202011125, Brand: Unknown, Forecast Accuracy: -53.55%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.055095 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 576\n[LightGBM] [Info] Number of data points in the train set: 896, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0204011225, Brand: Unknown, Forecast Accuracy: -61.25%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.089081 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 584\n[LightGBM] [Info] Number of data points in the train set: 899, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0204031225, Brand: Unknown, Forecast Accuracy: -58.51%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.052214 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 586\n[LightGBM] [Info] Number of data points in the train set: 899, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\nProductID: PVSL0208011025, Brand: Unknown, Forecast Accuracy: -51.62%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.068098 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 582\n[LightGBM] [Info] Number of data points in the train set: 899, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0209041225, Brand: Unknown, Forecast Accuracy: -52.69%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.065177 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 611\n[LightGBM] [Info] Number of data points in the train set: 899, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0216030126, Brand: Unknown, Forecast Accuracy: -55.38%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.058378 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 579\n[LightGBM] [Info] Number of data points in the train set: 898, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0216040126, Brand: Unknown, Forecast Accuracy: -65.72%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.051335 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 580\n[LightGBM] [Info] Number of data points in the train set: 899, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0220011025, Brand: Unknown, Forecast Accuracy: -56.04%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.089320 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 603\n[LightGBM] [Info] Number of data points in the train set: 899, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0301030925, Brand: Unknown, Forecast Accuracy: -62.78%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.063940 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 592\n[LightGBM] [Info] Number of data points in the train set: 899, number of used features: 10\n[LightGBM] [Info] Start training from score 2.000000\nProductID: PVSL0302061125, Brand: Unknown, Forecast Accuracy: -22.31%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.078219 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 591\n[LightGBM] [Info] Number of data points in the train set: 898, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0307061225, Brand: Unknown, Forecast Accuracy: -55.05%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.067046 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 586\n[LightGBM] [Info] Number of data points in the train set: 898, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0313031025, Brand: Unknown, Forecast Accuracy: -57.73%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.071222 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 597\n[LightGBM] [Info] Number of data points in the train set: 899, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0316020925, Brand: Unknown, Forecast Accuracy: -60.69%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.055832 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 600\n[LightGBM] [Info] Number of data points in the train set: 898, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0317021125, Brand: Unknown, Forecast Accuracy: -59.08%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.062269 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 587\n[LightGBM] [Info] Number of data points in the train set: 898, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\nProductID: PVSL0203030126, Brand: Unknown, Forecast Accuracy: -77.82%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.048223 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 584\n[LightGBM] [Info] Number of data points in the train set: 899, number of used features: 10\n[LightGBM] [Info] Start training from score 2.000000\nProductID: PVSL0207011125, Brand: Unknown, Forecast Accuracy: -38.70%\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.063251 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 582\n[LightGBM] [Info] Number of data points in the train set: 899, number of used features: 10\n[LightGBM] [Info] Start training from score 2.000000\nProductID: PVSL0209021225, Brand: Unknown, Forecast Accuracy: -33.13%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.047346 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 596\n[LightGBM] [Info] Number of data points in the train set: 897, number of used features: 10\n[LightGBM] [Info] Start training from score 2.000000\nProductID: PVSL0214041125, Brand: Unknown, Forecast Accuracy: -24.74%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.073057 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 589\n[LightGBM] [Info] Number of data points in the train set: 898, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0217041225, Brand: Unknown, Forecast Accuracy: -74.45%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.061341 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 596\n[LightGBM] [Info] Number of data points in the train set: 899, number of used features: 10\n[LightGBM] [Info] Start training from score 2.000000\nProductID: PVSL0301010925, Brand: Unknown, Forecast Accuracy: -66.23%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.076941 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 622\n[LightGBM] [Info] Number of data points in the train set: 896, number of used features: 10\n[LightGBM] [Info] Start training from score 4.000000\nProductID: PVSL0306021025, Brand: Unknown, Forecast Accuracy: -95.51%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.067891 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 602\n[LightGBM] [Info] Number of data points in the train set: 899, number of used features: 10\n[LightGBM] [Info] Start training from score 2.000000\nProductID: PVSL0311041025, Brand: Unknown, Forecast Accuracy: -21.92%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.043402 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 603\n[LightGBM] [Info] Number of data points in the train set: 899, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0313061025, Brand: Unknown, Forecast Accuracy: -63.48%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.059417 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 591\n[LightGBM] [Info] Number of data points in the train set: 899, number of used features: 10\n[LightGBM] [Info] Start training from score 2.000000\nProductID: PVSL0319061225, Brand: Unknown, Forecast Accuracy: -27.48%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.076727 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 597\n[LightGBM] [Info] Number of data points in the train set: 898, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0320021225, Brand: Unknown, Forecast Accuracy: -58.41%\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.047978 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 587\n[LightGBM] [Info] Number of data points in the train set: 899, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\nProductID: PVSL0404011225, Brand: Unknown, Forecast Accuracy: -45.75%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.063367 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 605\n[LightGBM] [Info] Number of data points in the train set: 899, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0406061125, Brand: Unknown, Forecast Accuracy: -43.29%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001190 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 596\n[LightGBM] [Info] Number of data points in the train set: 899, number of used features: 10\n[LightGBM] [Info] Start training from score 2.000000\nProductID: PVSL0411011225, Brand: Unknown, Forecast Accuracy: -42.91%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.046938 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 603\n[LightGBM] [Info] Number of data points in the train set: 899, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0412011025, Brand: Unknown, Forecast Accuracy: -79.34%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.069324 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 588\n[LightGBM] [Info] Number of data points in the train set: 898, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0413031225, Brand: Unknown, Forecast Accuracy: -65.77%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.069130 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 583\n[LightGBM] [Info] Number of data points in the train set: 899, number of used features: 10\n[LightGBM] [Info] Start training from score 2.000000\nProductID: PVSL0420030925, Brand: Unknown, Forecast Accuracy: -62.89%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.068240 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 591\n[LightGBM] [Info] Number of data points in the train set: 898, number of used features: 10\n[LightGBM] [Info] Start training from score 2.000000\nProductID: PVSL0502050925, Brand: Unknown, Forecast Accuracy: -45.64%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000147 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 592\n[LightGBM] [Info] Number of data points in the train set: 899, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0509031125, Brand: Unknown, Forecast Accuracy: -77.84%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.057064 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 607\n[LightGBM] [Info] Number of data points in the train set: 898, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\n[LightGBM] [Info] Total Bins 599\n[LightGBM] [Info] Number of data points in the train set: 899, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0306041025, Brand: Unknown, Forecast Accuracy: -72.93%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.058515 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 587\n[LightGBM] [Info] Number of data points in the train set: 899, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0310010126, Brand: Unknown, Forecast Accuracy: -70.15%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.075701 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 581\n[LightGBM] [Info] Number of data points in the train set: 898, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0315041025, Brand: Unknown, Forecast Accuracy: -52.39%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.064158 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 591\n[LightGBM] [Info] Number of data points in the train set: 898, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0320011225, Brand: Unknown, Forecast Accuracy: -67.09%\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.045966 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 616\n[LightGBM] [Info] Number of data points in the train set: 899, number of used features: 10\n[LightGBM] [Info] Start training from score 2.000000\nProductID: PVSL0403030126, Brand: Unknown, Forecast Accuracy: -51.02%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.016090 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 584\n[LightGBM] [Info] Number of data points in the train set: 899, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0411021225, Brand: Unknown, Forecast Accuracy: -53.05%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.047110 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 595\n[LightGBM] [Info] Number of data points in the train set: 899, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0413021225, Brand: Unknown, Forecast Accuracy: -68.18%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.061079 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 609\n[LightGBM] [Info] Number of data points in the train set: 899, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0420060925, Brand: Unknown, Forecast Accuracy: -55.77%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.045112 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 603\n[LightGBM] [Info] Number of data points in the train set: 899, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0501030925, Brand: Unknown, Forecast Accuracy: -66.89%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.047044 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 588\n[LightGBM] [Info] Number of data points in the train set: 899, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0503051225, Brand: Unknown, Forecast Accuracy: -63.52%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.056028 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 596\n[LightGBM] [Info] Number of data points in the train set: 899, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0503061225, Brand: Unknown, Forecast Accuracy: -58.61%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.047161 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 601\n[LightGBM] [Info] Number of data points in the train set: 899, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\nProductID: PVSL0504021125, Brand: Unknown, Forecast Accuracy: -48.34%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001665 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 593\n[LightGBM] [Info] Number of data points in the train set: 897, number of used features: 10\n[LightGBM] [Info] Start training from score 2.000000\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\nProductID: PVSL0506021225, Brand: Unknown, Forecast Accuracy: -27.64%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.059782 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 600\n[LightGBM] [Info] Number of data points in the train set: 899, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0507020925, Brand: Unknown, Forecast Accuracy: -75.05%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.052829 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 569\n[LightGBM] [Info] Number of data points in the train set: 897, number of used features: 10\n[LightGBM] [Info] Start training from score 2.000000\nProductID: PVSL0509021125, Brand: Unknown, Forecast Accuracy: -23.27%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.009112 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 590\n[LightGBM] [Info] Number of data points in the train set: 899, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0513061225, Brand: Unknown, Forecast Accuracy: -76.23%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.059044 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 591\n[LightGBM] [Info] Number of data points in the train set: 898, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0517051225, Brand: Unknown, Forecast Accuracy: -68.22%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.048964 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 605\n[LightGBM] [Info] Number of data points in the train set: 898, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0518061025, Brand: Unknown, Forecast Accuracy: -70.33%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.070838 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 586\n[LightGBM] [Info] Number of data points in the train set: 896, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0519021125, Brand: Unknown, Forecast Accuracy: -64.45%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.059620 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 605\n[LightGBM] [Info] Number of data points in the train set: 899, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0602031125, Brand: Unknown, Forecast Accuracy: -57.76%\n[LightGBM] [Info] Start training from score 2.000000\nProductID: PVSL0314050925, Brand: Unknown, Forecast Accuracy: -20.50%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.071745 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 587\n[LightGBM] [Info] Number of data points in the train set: 899, number of used features: 10\n[LightGBM] [Info] Start training from score 2.000000\nProductID: PVSL0319031225, Brand: Unknown, Forecast Accuracy: -22.36%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.074034 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 607\n[LightGBM] [Info] Number of data points in the train set: 898, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0402061225, Brand: Unknown, Forecast Accuracy: -65.16%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.064043 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 601\n[LightGBM] [Info] Number of data points in the train set: 899, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0404031225, Brand: Unknown, Forecast Accuracy: -66.61%\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.036143 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 587\n[LightGBM] [Info] Number of data points in the train set: 898, number of used features: 10\n[LightGBM] [Info] Start training from score 2.000000\nProductID: PVSL0408020925, Brand: Unknown, Forecast Accuracy: -43.38%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.047071 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 621\n[LightGBM] [Info] Number of data points in the train set: 897, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0410031125, Brand: Unknown, Forecast Accuracy: -65.57%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.055113 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 597\n[LightGBM] [Info] Number of data points in the train set: 899, number of used features: 10\n[LightGBM] [Info] Start training from score 2.000000\nProductID: PVSL0412061025, Brand: Unknown, Forecast Accuracy: -35.26%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.062334 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 577\n[LightGBM] [Info] Number of data points in the train set: 898, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0418021125, Brand: Unknown, Forecast Accuracy: -67.44%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.056234 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 599\n[LightGBM] [Info] Number of data points in the train set: 899, number of used features: 10\n[LightGBM] [Info] Start training from score 2.000000\nProductID: PVSL0503011225, Brand: Unknown, Forecast Accuracy: -25.24%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.059353 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 587\n[LightGBM] [Info] Number of data points in the train set: 899, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0504061125, Brand: Unknown, Forecast Accuracy: -69.86%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.048088 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 594\n[LightGBM] [Info] Number of data points in the train set: 897, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0506041225, Brand: Unknown, Forecast Accuracy: -83.65%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.078127 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 590\n[LightGBM] [Info] Number of data points in the train set: 898, number of used features: 10\n[LightGBM] [Info] Start training from score 2.000000\nProductID: PVSL0510011025, Brand: Unknown, Forecast Accuracy: -25.82%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.058949 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 583\n[LightGBM] [Info] Number of data points in the train set: 899, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0510051025, Brand: Unknown, Forecast Accuracy: -76.00%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.040119 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 588\n[LightGBM] [Info] Number of data points in the train set: 896, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0515041225, Brand: Unknown, Forecast Accuracy: -64.44%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.068236 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 604\n[LightGBM] [Info] Number of data points in the train set: 899, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0518051025, Brand: Unknown, Forecast Accuracy: -72.48%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.046854 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 601\n[LightGBM] [Info] Number of data points in the train set: 898, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0519031125, Brand: Unknown, Forecast Accuracy: -60.49%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.068238 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 606\n[LightGBM] [Info] Number of data points in the train set: 897, number of used features: 10\n[LightGBM] [Info] Start training from score 2.000000\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\nProductID: PVSL0603051025, Brand: Unknown, Forecast Accuracy: -40.17%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.056169 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 595\n[LightGBM] [Info] Number of data points in the train set: 899, number of used features: 10\n[LightGBM] [Info] Start training from score 1.000000\nProductID: PVSL0605051125, Brand: Unknown, Forecast Accuracy: -25.49%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.055890 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 586\n[LightGBM] [Info] Number of data points in the train set: 899, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0609011125, Brand: Unknown, Forecast Accuracy: -70.55%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.072778 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 594\n[LightGBM] [Info] Number of data points in the train set: 899, number of used features: 10\n[LightGBM] [Info] Start training from score 2.000000\nProductID: PVSL0610050925, Brand: Unknown, Forecast Accuracy: -38.01%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.071176 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 598\n[LightGBM] [Info] Number of data points in the train set: 899, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0319051225, Brand: Unknown, Forecast Accuracy: -58.65%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.059165 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 581\n[LightGBM] [Info] Number of data points in the train set: 896, number of used features: 10\n[LightGBM] [Info] Start training from score 2.000000\nProductID: PVSL0320061225, Brand: Unknown, Forecast Accuracy: -34.64%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.048943 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 603\n[LightGBM] [Info] Number of data points in the train set: 899, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0405030126, Brand: Unknown, Forecast Accuracy: -64.88%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.052103 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 573\n[LightGBM] [Info] Number of data points in the train set: 899, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\nProductID: PVSL0408030925, Brand: Unknown, Forecast Accuracy: -82.36%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.059519 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 585\n[LightGBM] [Info] Number of data points in the train set: 899, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0411041225, Brand: Unknown, Forecast Accuracy: -46.97%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.065202 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 611\n[LightGBM] [Info] Number of data points in the train set: 899, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0412021025, Brand: Unknown, Forecast Accuracy: -59.73%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.049494 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 594\n[LightGBM] [Info] Number of data points in the train set: 898, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0414011125, Brand: Unknown, Forecast Accuracy: -56.30%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.059380 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 595\n[LightGBM] [Info] Number of data points in the train set: 899, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0416051025, Brand: Unknown, Forecast Accuracy: -55.00%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.073331 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 593\n[LightGBM] [Info] Number of data points in the train set: 899, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0501010925, Brand: Unknown, Forecast Accuracy: -64.71%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.071370 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 568\n[LightGBM] [Info] Number of data points in the train set: 899, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0502030925, Brand: Unknown, Forecast Accuracy: -60.25%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.091944 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 596\n[LightGBM] [Info] Number of data points in the train set: 899, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0505041125, Brand: Unknown, Forecast Accuracy: -69.87%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.059220 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 593\n[LightGBM] [Info] Number of data points in the train set: 898, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0508040925, Brand: Unknown, Forecast Accuracy: -87.14%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.088859 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 578\n[LightGBM] [Info] Number of data points in the train set: 896, number of used features: 10\n[LightGBM] [Info] Start training from score 2.000000\nProductID: PVSL0513011225, Brand: Unknown, Forecast Accuracy: -33.22%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.048260 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 600\n[LightGBM] [Info] Number of data points in the train set: 899, number of used features: 10\n[LightGBM] [Info] Start training from score 2.000000\nProductID: PVSL0513031225, Brand: Unknown, Forecast Accuracy: -24.29%\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.066880 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 596\n[LightGBM] [Info] Number of data points in the train set: 899, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0518021025, Brand: Unknown, Forecast Accuracy: -63.83%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.047689 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 605\n[LightGBM] [Info] Number of data points in the train set: 899, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0520041125, Brand: Unknown, Forecast Accuracy: -67.46%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.059219 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 597\n[LightGBM] [Info] Number of data points in the train set: 899, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0605011125, Brand: Unknown, Forecast Accuracy: -69.10%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.059241 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 602\n[LightGBM] [Info] Number of data points in the train set: 899, number of used features: 10\n[LightGBM] [Info] Start training from score 2.000000\nProductID: PVSL0607041225, Brand: Unknown, Forecast Accuracy: -23.22%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.058978 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 589\n[LightGBM] [Info] Number of data points in the train set: 899, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0608051025, Brand: Unknown, Forecast Accuracy: -53.42%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.061401 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 596\n[LightGBM] [Info] Number of data points in the train set: 897, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0610060925, Brand: Unknown, Forecast Accuracy: -68.74%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.060283 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\nProductID: PVSL0511040925, Brand: Unknown, Forecast Accuracy: -71.34%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.047113 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 592\n[LightGBM] [Info] Number of data points in the train set: 896, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0516041025, Brand: Unknown, Forecast Accuracy: -55.69%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.066110 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 595\n[LightGBM] [Info] Number of data points in the train set: 899, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0517011225, Brand: Unknown, Forecast Accuracy: -71.88%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.065357 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 597\n[LightGBM] [Info] Number of data points in the train set: 899, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0518011025, Brand: Unknown, Forecast Accuracy: -67.47%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.058920 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 592\n[LightGBM] [Info] Number of data points in the train set: 899, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0520031125, Brand: Unknown, Forecast Accuracy: -37.27%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.059373 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 586\n[LightGBM] [Info] Number of data points in the train set: 899, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0603011025, Brand: Unknown, Forecast Accuracy: -71.99%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.046355 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 582\n[LightGBM] [Info] Number of data points in the train set: 899, number of used features: 10\n[LightGBM] [Info] Start training from score 2.000000\nProductID: PVSL0604021025, Brand: Unknown, Forecast Accuracy: -30.88%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.048185 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 592\n[LightGBM] [Info] Number of data points in the train set: 899, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0604051025, Brand: Unknown, Forecast Accuracy: -53.99%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.076769 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 596\n[LightGBM] [Info] Number of data points in the train set: 898, number of used features: 10\n[LightGBM] [Info] Start training from score 2.000000\nProductID: PVSL0605061125, Brand: Unknown, Forecast Accuracy: -32.77%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.076993 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 595\n[LightGBM] [Info] Number of data points in the train set: 899, number of used features: 10\n[LightGBM] [Info] Start training from score 2.000000\nProductID: PVSL0609021125, Brand: Unknown, Forecast Accuracy: -31.20%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.072843 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 591\n[LightGBM] [Info] Number of data points in the train set: 898, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0701060925, Brand: Unknown, Forecast Accuracy: -61.70%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.071265 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 610\n[LightGBM] [Info] Number of data points in the train set: 898, number of used features: 10\n[LightGBM] [Info] Start training from score 2.000000\nProductID: PVSL0705040925, Brand: Unknown, Forecast Accuracy: -30.51%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.071996 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 581\n[LightGBM] [Info] Number of data points in the train set: 899, number of used features: 10\n[LightGBM] [Info] Start training from score 2.000000\nProductID: PVSL0706011225, Brand: Unknown, Forecast Accuracy: -30.09%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.065786 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 606\n[LightGBM] [Info] Number of data points in the train set: 899, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0707031125, Brand: Unknown, Forecast Accuracy: -66.72%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.058553 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 592\n[LightGBM] [Info] Number of data points in the train set: 899, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0801030126, Brand: Unknown, Forecast Accuracy: -60.79%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.047944 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 585\n[LightGBM] [Info] Number of data points in the train set: 898, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0804021025, Brand: Unknown, Forecast Accuracy: -61.11%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.045853 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 598\n[LightGBM] [Info] Number of data points in the train set: 896, number of used features: 10\n[LightGBM] [Info] Start training from score 2.000000\nProductID: PVSL0808041125, Brand: Unknown, Forecast Accuracy: -27.31%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.057184 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 667\n[LightGBM] [Info] Number of data points in the train set: 899, number of used features: 10\n[LightGBM] [Info] Start training from score 8.000000\nProductID: PVSL0901031225, Brand: Unknown, Forecast Accuracy: -117.49%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.068134 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 595\n[LightGBM] [Info] Number of data points in the train set: 897, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0903031125, Brand: Unknown, Forecast Accuracy: -70.33%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.083527 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 599\n[LightGBM] [Info] Number of data points in the train set: 899, number of used features: 10\n[LightGBM] [Info] Start training from score 2.000000\nProductID: PVSL0904040126, Brand: Unknown, Forecast Accuracy: -70.98%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.059261 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 595\n[LightGBM] [Info] Number of data points in the train set: 899, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0905011025, Brand: Unknown, Forecast Accuracy: -64.13%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.059374 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 578\n[LightGBM] [Info] Number of data points in the train set: 897, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0604061025, Brand: Unknown, Forecast Accuracy: -73.42%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.061235 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 605\n[LightGBM] [Info] Number of data points in the train set: 898, number of used features: 10\n[LightGBM] [Info] Start training from score 2.000000\nProductID: PVSL0608031025, Brand: Unknown, Forecast Accuracy: -64.97%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.081135 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 594\n[LightGBM] [Info] Number of data points in the train set: 899, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0701020925, Brand: Unknown, Forecast Accuracy: -57.73%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.055151 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 593\n[LightGBM] [Info] Number of data points in the train set: 899, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0703021025, Brand: Unknown, Forecast Accuracy: -60.93%\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.052357 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 607\n[LightGBM] [Info] Number of data points in the train set: 896, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\nProductID: PVSL0704041125, Brand: Unknown, Forecast Accuracy: -48.18%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.047191 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 593\n[LightGBM] [Info] Number of data points in the train set: 899, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\nProductID: PVSL0705050925, Brand: Unknown, Forecast Accuracy: -67.83%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.057883 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 607\n[LightGBM] [Info] Number of data points in the train set: 897, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0706031225, Brand: Unknown, Forecast Accuracy: -61.29%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.043921 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 604\n[LightGBM] [Info] Number of data points in the train set: 899, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0709011125, Brand: Unknown, Forecast Accuracy: -66.52%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.046875 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 595\n[LightGBM] [Info] Number of data points in the train set: 898, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0806061025, Brand: Unknown, Forecast Accuracy: -64.85%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.048156 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 590\n[LightGBM] [Info] Number of data points in the train set: 899, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\nProductID: PVSL0810020126, Brand: Unknown, Forecast Accuracy: -63.14%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.043351 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 601\n[LightGBM] [Info] Number of data points in the train set: 897, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0901061025, Brand: Unknown, Forecast Accuracy: -58.96%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.047380 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 597\n[LightGBM] [Info] Number of data points in the train set: 898, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0903020925, Brand: Unknown, Forecast Accuracy: -50.00%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.051169 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 676\n[LightGBM] [Info] Number of data points in the train set: 899, number of used features: 10\n[LightGBM] [Info] Start training from score 8.000000\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\nProductID: PVSL0904011025, Brand: Unknown, Forecast Accuracy: -70.53%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.049137 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 609\n[LightGBM] [Info] Number of data points in the train set: 897, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0904020925, Brand: Unknown, Forecast Accuracy: -68.26%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.047216 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 589\n[LightGBM] [Info] Number of data points in the train set: 896, number of used features: 10\n[LightGBM] [Info] Start training from score 2.000000\nProductID: PVSL0905061025, Brand: Unknown, Forecast Accuracy: -36.95%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.060211 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 673\n[LightGBM] [Info] Number of data points in the train set: 899, number of used features: 10\n[LightGBM] [Info] Start training from score 9.000000\nProductID: PVSL0906011125, Brand: Unknown, Forecast Accuracy: -121.44%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.059982 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 616\n[LightGBM] [Info] Number of data points in the train set: 899, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0907010925, Brand: Unknown, Forecast Accuracy: -59.88%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.069868 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 595\n[LightGBM] [Info] Number of data points in the train set: 899, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0908010925, Brand: Unknown, Forecast Accuracy: -65.18%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.041778 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 684\n[LightGBM] [Info] Number of data points in the train set: 899, number of used features: 10\n[LightGBM] [Info] Start training from score 8.000000\nProductID: PVSL0909011025, Brand: Unknown, Forecast Accuracy: -92.38%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.047972 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 589\n[LightGBM] [Info] Number of data points in the train set: 899, number of used features: 10\n[LightGBM] [Info] Start training from score 2.000000\nProductID: PVSL0701050925, Brand: Unknown, Forecast Accuracy: -44.76%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.051259 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 599\n[LightGBM] [Info] Number of data points in the train set: 895, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0702041225, Brand: Unknown, Forecast Accuracy: -75.21%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.063056 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 584\n[LightGBM] [Info] Number of data points in the train set: 899, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\nProductID: PVSL0705020925, Brand: Unknown, Forecast Accuracy: -59.14%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.067895 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 597\n[LightGBM] [Info] Number of data points in the train set: 898, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0706021225, Brand: Unknown, Forecast Accuracy: -67.32%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.057520 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 588\n[LightGBM] [Info] Number of data points in the train set: 899, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0707021125, Brand: Unknown, Forecast Accuracy: -60.17%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.047065 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 577\n[LightGBM] [Info] Number of data points in the train set: 898, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0708051225, Brand: Unknown, Forecast Accuracy: -64.28%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.071287 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 588\n[LightGBM] [Info] Number of data points in the train set: 896, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0805031025, Brand: Unknown, Forecast Accuracy: -54.86%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.056324 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 598\n[LightGBM] [Info] Number of data points in the train set: 898, number of used features: 10\n[LightGBM] [Info] Start training from score 2.000000\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\nProductID: PVSL0810050126, Brand: Unknown, Forecast Accuracy: -62.42%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.091219 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 674\n[LightGBM] [Info] Number of data points in the train set: 898, number of used features: 10\n[LightGBM] [Info] Start training from score 8.000000\nProductID: PVSL0902021125, Brand: Unknown, Forecast Accuracy: -65.96%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.058019 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 584\n[LightGBM] [Info] Number of data points in the train set: 899, number of used features: 10\n[LightGBM] [Info] Start training from score 2.000000\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\nProductID: PVSL0904011225, Brand: Unknown, Forecast Accuracy: -38.81%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.061905 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 702\n[LightGBM] [Info] Number of data points in the train set: 899, number of used features: 10\n[LightGBM] [Info] Start training from score 9.000000\nProductID: PVSL0905050925, Brand: Unknown, Forecast Accuracy: -163.16%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.011090 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 591\n[LightGBM] [Info] Number of data points in the train set: 899, number of used features: 10\n[LightGBM] [Info] Start training from score 2.000000\nProductID: PVSL0906031125, Brand: Unknown, Forecast Accuracy: -50.80%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.053379 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 588\n[LightGBM] [Info] Number of data points in the train set: 899, number of used features: 10\n[LightGBM] [Info] Start training from score 2.000000\nProductID: PVSL0907010126, Brand: Unknown, Forecast Accuracy: -32.46%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.073366 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 594\n[LightGBM] [Info] Number of data points in the train set: 898, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\nProductID: PVSL0907041025, Brand: Unknown, Forecast Accuracy: -57.32%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.047051 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 594\n[LightGBM] [Info] Number of data points in the train set: 898, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0908050126, Brand: Unknown, Forecast Accuracy: -57.83%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.067837 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 598\n[LightGBM] [Info] Number of data points in the train set: 898, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0909011225, Brand: Unknown, Forecast Accuracy: -66.29%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.046344 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 590\n[LightGBM] [Info] Number of data points in the train set: 897, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0910010126, Brand: Unknown, Forecast Accuracy: -74.12%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.073768 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 607\n[LightGBM] [Info] Number of data points in the train set: 898, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0109020126, Brand: Unknown, Forecast Accuracy: -62.28%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.049868 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 598\n[LightGBM] [Info] Number of data points in the train set: 898, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0117040126, Brand: Unknown, Forecast Accuracy: -68.30%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.072074 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 581\n[LightGBM] [Info] Number of data points in the train set: 898, number of used features: 10\n[LightGBM] [Info] Total Bins 605\n[LightGBM] [Info] Number of data points in the train set: 899, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0702021225, Brand: Unknown, Forecast Accuracy: -76.32%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.054977 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 581\n[LightGBM] [Info] Number of data points in the train set: 899, number of used features: 10\n[LightGBM] [Info] Start training from score 2.000000\nProductID: PVSL0706061225, Brand: Unknown, Forecast Accuracy: -29.85%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.056044 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 587\n[LightGBM] [Info] Number of data points in the train set: 897, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0710041225, Brand: Unknown, Forecast Accuracy: -76.74%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.046049 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 583\n[LightGBM] [Info] Number of data points in the train set: 898, number of used features: 10\n[LightGBM] [Info] Start training from score 2.000000\nProductID: PVSL0805011025, Brand: Unknown, Forecast Accuracy: -42.24%\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.048895 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 605\n[LightGBM] [Info] Number of data points in the train set: 899, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0810030126, Brand: Unknown, Forecast Accuracy: -67.66%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.047392 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 586\n[LightGBM] [Info] Number of data points in the train set: 899, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0901060126, Brand: Unknown, Forecast Accuracy: -66.69%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.046309 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 606\n[LightGBM] [Info] Number of data points in the train set: 899, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\nProductID: PVSL0903010925, Brand: Unknown, Forecast Accuracy: -70.93%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.058351 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 681\n[LightGBM] [Info] Number of data points in the train set: 899, number of used features: 10\n[LightGBM] [Info] Start training from score 8.000000\nProductID: PVSL0903040925, Brand: Unknown, Forecast Accuracy: -118.94%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.068385 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 597\n[LightGBM] [Info] Number of data points in the train set: 897, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0904061125, Brand: Unknown, Forecast Accuracy: -64.61%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.053846 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 588\n[LightGBM] [Info] Number of data points in the train set: 899, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0906010126, Brand: Unknown, Forecast Accuracy: -55.21%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.069167 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 681\n[LightGBM] [Info] Number of data points in the train set: 898, number of used features: 10\n[LightGBM] [Info] Start training from score 8.000000\nProductID: PVSL0906051125, Brand: Unknown, Forecast Accuracy: -123.40%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.061268 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 593\n[LightGBM] [Info] Number of data points in the train set: 899, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0907011125, Brand: Unknown, Forecast Accuracy: -56.13%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.077063 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 584\n[LightGBM] [Info] Number of data points in the train set: 899, number of used features: 10\n[LightGBM] [Info] Start training from score 2.000000\nProductID: PVSL0907041225, Brand: Unknown, Forecast Accuracy: -31.58%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.052960 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 705\n[LightGBM] [Info] Number of data points in the train set: 899, number of used features: 10\n[LightGBM] [Info] Start training from score 9.000000\nProductID: PVSL0908060126, Brand: Unknown, Forecast Accuracy: -149.09%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.059359 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 597\n[LightGBM] [Info] Number of data points in the train set: 899, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\nProductID: PVSL0910021025, Brand: Unknown, Forecast Accuracy: -76.72%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.068566 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 608\n[LightGBM] [Info] Number of data points in the train set: 898, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0104040126, Brand: Unknown, Forecast Accuracy: -66.02%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.048842 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 596\n[LightGBM] [Info] Number of data points in the train set: 895, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0113041125, Brand: Unknown, Forecast Accuracy: -62.69%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.081887 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 589\n[LightGBM] [Info] Number of data points in the train set: 896, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\nProductID: PVSL0118030126, Brand: Unknown, Forecast Accuracy: -63.23%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.059363 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 589\n[LightGBM] [Info] Number of data points in the train set: 896, number of used features: 10\n[LightGBM] [Info] Start training from score 2.000000\nProductID: PVSL0128011225, Brand: Unknown, Forecast Accuracy: -41.07%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.056857 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 604\n[LightGBM] [Info] Number of data points in the train set: 898, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.053093 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 688\n[LightGBM] [Info] Number of data points in the train set: 899, number of used features: 10\n[LightGBM] [Info] Start training from score 8.000000\nProductID: PVSL0905030126, Brand: Unknown, Forecast Accuracy: -62.62%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.048988 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 699\n[LightGBM] [Info] Number of data points in the train set: 899, number of used features: 10\n[LightGBM] [Info] Start training from score 9.000000\nProductID: PVSL0906030925, Brand: Unknown, Forecast Accuracy: -164.03%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.050113 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 582\n[LightGBM] [Info] Number of data points in the train set: 898, number of used features: 10\n[LightGBM] [Info] Start training from score 2.500000\nProductID: PVSL0906060925, Brand: Unknown, Forecast Accuracy: -37.45%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.063890 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 606\n[LightGBM] [Info] Number of data points in the train set: 898, number of used features: 10\n[LightGBM] [Info] Start training from score 2.000000\nProductID: PVSL0907061125, Brand: Unknown, Forecast Accuracy: -31.43%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.072164 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 598\n[LightGBM] [Info] Number of data points in the train set: 899, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\nProductID: PVSL0908050925, Brand: Unknown, Forecast Accuracy: -56.70%\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.056924 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 757\n[LightGBM] [Info] Number of data points in the train set: 899, number of used features: 10\n[LightGBM] [Info] Start training from score 13.000000\nProductID: PVSL0909061025, Brand: Unknown, Forecast Accuracy: -96.92%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.069116 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 576\n[LightGBM] [Info] Number of data points in the train set: 898, number of used features: 10\n[LightGBM] [Info] Start training from score 2.000000\nProductID: PVSL0910031025, Brand: Unknown, Forecast Accuracy: -42.57%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.059961 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 597\n[LightGBM] [Info] Number of data points in the train set: 897, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0111061025, Brand: Unknown, Forecast Accuracy: -72.46%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.056894 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 595\n[LightGBM] [Info] Number of data points in the train set: 897, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0120050126, Brand: Unknown, Forecast Accuracy: -61.14%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.045385 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 598\n[LightGBM] [Info] Number of data points in the train set: 898, number of used features: 10\n[LightGBM] [Info] Start training from score 4.000000\nProductID: PVSL0127061025, Brand: Unknown, Forecast Accuracy: -102.42%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.059362 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 604\n[LightGBM] [Info] Number of data points in the train set: 898, number of used features: 10\n[LightGBM] [Info] Start training from score 2.000000\nProductID: PVSL0130011025, Brand: Unknown, Forecast Accuracy: -46.61%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.065304 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 573\n[LightGBM] [Info] Number of data points in the train set: 898, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0136020925, Brand: Unknown, Forecast Accuracy: -58.94%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.059378 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 594\n[LightGBM] [Info] Number of data points in the train set: 898, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0142010126, Brand: Unknown, Forecast Accuracy: -71.48%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.061874 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 588\n[LightGBM] [Info] Number of data points in the train set: 897, number of used features: 10\n[LightGBM] [Info] Start training from score 2.000000\nProductID: PVSL0214021125, Brand: Unknown, Forecast Accuracy: -56.73%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.044773 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 596\n[LightGBM] [Info] Number of data points in the train set: 898, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0314020925, Brand: Unknown, Forecast Accuracy: -65.11%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.061195 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 582\n[LightGBM] [Info] Number of data points in the train set: 898, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0315061025, Brand: Unknown, Forecast Accuracy: -66.88%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.051368 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 596\n[LightGBM] [Info] Number of data points in the train set: 898, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0405040126, Brand: Unknown, Forecast Accuracy: -72.26%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.048230 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 570\n[LightGBM] [Info] Number of data points in the train set: 898, number of used features: 10\n[LightGBM] [Info] Start training from score 3.000000\nProductID: PVSL0409060925, Brand: Unknown, Forecast Accuracy: -71.08%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.062376 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 601\n[LightGBM] [Info] Number of data points in the train set: 898, number of used features: 10\n[LightGBM] [Info] Start training from score 2.500000\nProductID: PVSL0419040126, Brand: Unknown, Forecast Accuracy: -49.11%\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.071199 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 595\n[LightGBM] [Info] Number of data points in the train set: 898, number of used features: 10","output_type":"stream"}],"execution_count":null}]}